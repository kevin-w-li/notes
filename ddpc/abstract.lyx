#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Abstract
\end_layout

\begin_layout Standard
Supervised and unsupervised learning methods solve different types of problems.
 Recently, the dichotomy between the two becomes less clear as supervised
 learning techniques, such as neural networks, are introduced into unsupervised
 objectives for greater flexibility and scalability.
 This thesis explores more ways to bring supervised techniques to enhance
 and understand unsupervised learning agents in computational or biological
 contexts.
 
\end_layout

\begin_layout Standard
In the first part, I describe how to perform density estimation using a
 kernel augmented by a neural network.
 This kernel has location dependent shape and is thus more flexible than
 conventional kernels.
 The kernel parameters are trained by a meta-learning procedure that is
 also used in later parts of the thesis.
 
\end_layout

\begin_layout Standard
The second and third parts investigates a biologically-inspired representation
 of probability distributions, the distributed distributional codes (DDCs),
 that is related to finite-dimensional kernel mean embedding.
 I first find that, in neural networks trained to solve probabilistic inference
 problems with supervision, DDC arises naturally as the internal representation
 of uncertainty associated with the problem.
 I then propose several DDC-based message passing algorithms.
 These include, for state-space models, the operations of filtering, smoothing
 and, unusually, real-time postdiction.
 The latter is a biologically relevant computation that can explain a wide
 range of perceptual phenomena.
 The main idea behind these algorithm is to train a regressor online to
 predict features of the latent dynamics from observed sequences.
 The flexible representation and plausible learning to infer makes DDC an
 attractive proposal for how the brain may handle the ubiquitous uncertainties
 in the dynamical world.
\end_layout

\begin_layout Standard
Finally, I extend the type of regression in the DDC framework to form a
 maximum likelihood learning algorithm.
 It is applicable to a wide range of generative latent variable models,
 regardless of its graphical structure and latent variable type.
 The use of kernel ridge regression, a flexible supervised algorithm, to
 predict the gradient for learning lies at the heart of this approach.
 The type of kernel and its meta-learning procedure used in the first part
 of the thesis is critical for scaling to high-dimensional data.
 In many unsupervised learning tasks, this approach outperforms many existing
 ones that extend the classical expectation-maximisation algorithm with
 approximate inference.
\end_layout

\begin_layout Standard

\end_layout

\end_body
\end_document

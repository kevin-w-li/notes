#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\topmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\noindent
Amortised learning
\end_layout

\begin_layout Author
\noindent
Kevin W Li
\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\bphi}{\bm{\phi}}
{\bm{\phi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bmu}{\bm{\mu}}
{\bm{\mu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bSigma}{\bm{\Sigma}}
{\bm{\Sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bI}{\bm{I}}
{I}
\end_inset


\begin_inset FormulaMacro
\newcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\E}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\H}{\mathbb{H}}
{\mathbb{H}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bpsi}{\bm{\psi}}
{\bm{\psi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cW}{\mathcal{W}}
{\mathcal{W}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\m}{\mathbf{m}}
{\mathbf{m}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bxi}{\bm{\xi}}
{\bm{\xi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cH}{\mathcal{H}}
{\mathcal{H}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cF}{\mathcal{F}}
{\mathcal{F}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cG}{\mathcal{G}}
{\mathcal{G}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cC}{\mathcal{C}}
{\mathcal{C}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cU}{\mathcal{U}}
{\mathcal{U}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cN}{\mathcal{N}}
{\mathcal{N}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cV}{\mathcal{V}}
{\mathcal{V}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\GP}{\mathcal{GP}}
{\mathcal{GP}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cD}{\mathcal{D}}
{\mathcal{D}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
{\boldsymbol{k}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mK}{\mathbf{K}}
{\mathbf{K}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mI}{\mathbf{I}}
{\mathbf{I}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mSigma}{\boldsymbol{\Sigma}}
{\boldsymbol{\Sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vm}{\boldsymbol{m}}
{\boldsymbol{m}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
{\boldsymbol{b}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
{\boldsymbol{\theta}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\lotimes}{\Large{\otimes}}
{\Large{\otimes}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bnu}{\bm{\nu}}
{\bm{\nu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
{\textrm{arg min}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mA}{\mathbf{A}}
{\mathbf{A}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mB}{\mathbf{B}}
{\mathbf{B}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\valpha}{\bm{\alpha}}
{\bm{\alpha}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vphi}{\bm{\phi}}
{\bm{\phi}}
\end_inset


\end_layout

\begin_layout Section
Gaussian processes
\end_layout

\begin_layout Standard
Suppose we have data that are input and output paris 
\begin_inset Formula $\cD=\left\{ \left(x_{n,}y_{n}\right)\right\} _{n=1}^{N}$
\end_inset

 and we would like to perform regression to find a function 
\begin_inset Formula $f$
\end_inset

 such that 
\begin_inset Formula $f(x_{n})$
\end_inset

 is a good approximation of the label 
\begin_inset Formula $y_{n}$
\end_inset

.
 Gaussian processes regression is an attractive method for two reasons:
 first, it makes explicit the smoothness and regularity assumptions of 
\begin_inset Formula $f$
\end_inset

 in a explicit way through a kernel function, and second, it produces uncertaint
y and correlation estimates for predictions, since the posterior 
\begin_inset Formula $f$
\end_inset

 is uncertain and correlated across input points.
 Define by 
\begin_inset Formula $f\sim\GP(0,k_{\vtheta}(\cdot,\cdot))$
\end_inset

 a function drawn from a GP with zero mean function and covariance kernel
 
\begin_inset Formula $k_{\vtheta}$
\end_inset

 with parameters 
\begin_inset Formula $\vtheta$
\end_inset

, and denote the value of this function at 
\begin_inset Formula $x$
\end_inset

 as 
\begin_inset Formula $f^{x}$
\end_inset

, and the distribution induced by evaluating the GP at 
\begin_inset Formula $x$
\end_inset

 as 
\begin_inset Formula $F^{x}$
\end_inset

.
 Depending on the nature of 
\begin_inset Formula $y$
\end_inset

, a likelihood function is specified 
\begin_inset Formula $p_{\vtheta}(Y^{x}|F^{x})$
\end_inset

 to account for observation noise.
 Assuming conditional independence of the outputs given 
\begin_inset Formula $f$
\end_inset

, the generative model for GP regression then takes the following joint
 distribution
\begin_inset Formula 
\[
p_{\vtheta}(\left\{ F_{n}^{x}\right\} )p_{\vtheta}(\left\{ Y_{n}^{x}\right\} |\left\{ F_{n}^{x}\right\} )=P_{\vtheta}(\left\{ F^{x}\right\} )\prod_{n=1}^{N}p_{\vtheta}(Y_{n}^{x}|F_{n}^{x})
\]

\end_inset


\end_layout

\begin_layout Standard
For simplicity, suppose we are interested in predicting the output at a
 single input 
\begin_inset Formula $x^{*}$
\end_inset

 (as opposed to a set of new input points, referred to as joint prediction),
 then the goal of inference is to produce posterior 
\begin_inset Formula $p(y^{*}|\left\{ y^{x_{n}}\right\} )$
\end_inset

.
 
\end_layout

\begin_layout Standard
In the case that the likelihood is a Gaussian distribution 
\begin_inset Formula $p_{\vtheta}(y^{x}|f^{x})=\cN(y|f^{x},\sigma^{2})$
\end_inset

, then we have closed-form predictive posterior for a new test input 
\begin_inset Formula $x^{*}$
\end_inset

.
 However, for many other non-conjugate likelihood functions, no closed-form
 posteriors exists.
 In addition, inference usually involves evaluation of the inverse of the
 covariance matrix at all input locations, which has size 
\begin_inset Formula $N\times N$
\end_inset

; for large 
\begin_inset Formula $N$
\end_inset

, the computational intractability prevents scaling up GP to model large
 datasets.
 The latter computational cost may be reduced by augmenting the GP by a
 smaller number of 
\begin_inset Formula $M$
\end_inset

 inducing points 
\begin_inset Formula $\left\{ z_{m}\right\} _{m=1}^{M}$
\end_inset

 with pseudo-measurements 
\begin_inset Formula $\left\{ u_{m}\right\} _{m=1}^{M},u_{m}=f(z_{m})$
\end_inset

.
 This modifies the (single-data) joint distribution to 
\begin_inset Formula 
\[
p_{\vtheta}(U^{z})p_{\vtheta}(F^{x}|U^{z})p_{\vtheta}(Y^{x}|F^{x})
\]

\end_inset

From here, two general simplification methods exists to reduce the computational
 costs.
 
\end_layout

\begin_layout Enumerate
simplify factors in joint distribution model above, giving potentially inconsist
ent joint distributions between training and test data 
\end_layout

\begin_layout Enumerate
assuming a flexible parametric form of posterior and perform (stochastic)
 variational optimisation.
\end_layout

\begin_layout Standard
Methods of learning 
\begin_inset Formula $\vtheta$
\end_inset

 on simplified generative models can be directly adapted into the amortised
 learning framework, as it only requires specifying the joint distribution
 through the simplified generative factors.
 However, these simplifications result in degenerate prior GP processes,
 or render the model not a GP anymore.
 
\end_layout

\begin_layout Standard
The second method usually involves a clever factorisation of the variational
 posterior, which is reviewed now.
 For a single data and inducing input pair, the true posterior 
\begin_inset Formula $p_{\vtheta}(F^{x},U^{z}|\cD)=p_{\vtheta}(F^{x}|U^{z},\cD)p_{\vtheta}(U^{z}|\mathcal{\cD})$
\end_inset

.
 Now, assume a variational posterior 
\begin_inset Formula $q_{\phi}(F^{x},U^{z}|\cD):=p_{\vtheta}(F^{x}|U^{z})q_{\phi}(U^{z}|\mathcal{\cD})$
\end_inset

, that is, only assume a variational distribution 
\begin_inset Formula $q_{\phi}(U^{z}|\mathcal{\cD})$
\end_inset

 and construct the joint distribution by multiplying the conditional under
 the generative model 
\begin_inset Formula $p_{\vtheta}(F^{x}|U^{z})$
\end_inset

 that, nonetheless, does NOT depend on the dataset 
\begin_inset Formula $\cD$
\end_inset

.
 The variational objective is 
\begin_inset Formula 
\begin{align*}
\cF(\vtheta,\vphi) & =\E_{q_{\vphi}(F^{x},U^{z}|\mathcal{\cD})}\left[\log\frac{p_{\vtheta}(Y^{x}|F^{x})p_{\vtheta}(F^{x}|U^{z})p_{\vtheta}(U^{z})}{q_{\phi}(F^{x},U^{z}|\mathcal{\cD})}\right]\\
 & =\E_{q_{\phi}(U^{z}|\mathcal{\cD})}\left[\E_{p_{\vtheta}(F^{x}|U^{z})}\left[\log p_{\vtheta}(Y^{x}|F^{x})\right]\right]-KL\left[q_{\vphi}(U^{z}|D)\|p_{\vtheta}(U^{z})\right]
\end{align*}

\end_inset

We may interpret this free energy as corresponding to a generative model
 specified by a prior 
\begin_inset Formula $\log p_{\vtheta}(U^{z})$
\end_inset

 that follows the original GP and a likelihood function 
\begin_inset Formula $\E_{p_{\vtheta}(F^{x}|U^{z})}\left[\log p_{\vtheta}(Y^{x}|F^{x})\right]$
\end_inset

, a conditional expectation under the original GP.
 Drawing samples from or evaluating 
\begin_inset Formula $p_{\vtheta}(F^{x}|U^{z})$
\end_inset

 is tractable under the original GP, but computing 
\begin_inset Formula $\E_{p_{\vtheta}(F^{x}|U^{z})}\left[\log p_{\vtheta}(Y^{x}|F^{x})\right]$
\end_inset

 is analytically intractable for many likelihood functions.
 
\end_layout

\begin_layout Standard
To amortise learning 
\begin_inset Formula $\vtheta$
\end_inset

, we approximate the inner expectation by 1-sample Monte Carlo estimate.
 More specifically, we can substitute 
\begin_inset Formula $\log p_{\vtheta}(y|f^{(s)})\approx\E_{p_{\vtheta}(F^{x}|U^{z})}\left[\log p_{\vtheta}(Y^{x}|F^{x})\right]$
\end_inset

 where 
\begin_inset Formula $f^{(s)}\sim p_{\vtheta}(F^{x}|U^{z})$
\end_inset

.
 In effect, the log joint, which is the target in the regression for the
 gradient model, becomes noise due to Monte Carlo noise.
 
\end_layout

\begin_layout Subsection
Factorised posterior.
\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset citation
LatexCommand cite
key "Titsias2009"

\end_inset

, the following form of posterior GP is proposed:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\mu^{q}(x) & =\vk_{x,Z}\mK_{ZZ}\vb\\
k^{q}(x,x') & =k(x,x')-\vk_{xZ}\mK_{ZZ}^{-1}\vk_{Zx'}+\vk_{xZ}\mK_{ZZ}^{-1}\mA\mK_{ZZ}^{-1}\vk_{Zx'}
\end{align*}

\end_inset

where 
\begin_inset Formula $\vb$
\end_inset

 and 
\begin_inset Formula $\mA$
\end_inset

 are variational parameters, which are fit to maximise the ELBO.
 
\end_layout

\begin_layout Standard
Using amortised learning, one can use a more generic form of the posterior
 function.
 For example, the following stochastic function may be used 
\begin_inset Formula 
\[
f=\sum_{m}\alpha_{m}k(z_{m},\cdot),\quad\valpha\sim\cN(\vb,\mA)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ref"
options "plain"

\end_inset


\end_layout

\end_body
\end_document

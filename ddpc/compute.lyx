#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\newcommand{\bx}{\mathbf{x}}
\DeclareMathOperator{\Tr}{Tr}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\noindent
DDC Message Passing
\end_layout

\begin_layout Author
\noindent
Li Kevin Wenliang
\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\bphi}{\bm{\phi}}
{\bm{\phi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bmu}{\bm{\mu}}
{\bm{\mu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bSigma}{\bm{\Sigma}}
{\bm{\Sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bI}{\bm{I}}
{I}
\end_inset


\begin_inset FormulaMacro
\newcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\E}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\H}{\mathbb{H}}
{\mathbb{H}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bpsi}{\bm{\psi}}
{\bm{\psi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cW}{\mathcal{W}}
{\mathcal{W}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\m}{\mathbf{m}}
{\mathbf{m}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bxi}{\bm{\xi}}
{\bm{\xi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cH}{\mathcal{H}}
{\mathcal{H}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cF}{\mathcal{F}}
{\mathcal{F}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cG}{\mathcal{G}}
{\mathcal{G}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cC}{\mathcal{C}}
{\mathcal{C}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cU}{\mathcal{U}}
{\mathcal{U}}
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\lotimes}{\Large{\otimes}}
{\Large{\otimes}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bnu}{\bm{\nu}}
{\bm{\nu}}
\end_inset


\end_layout

\begin_layout Standard
\noindent
This notes includes the derivations of the message passing using DDC, illustrate
d with comparisons to the kernel literature.
\end_layout

\begin_layout Section
\noindent
Message passing
\end_layout

\begin_layout Standard
Usually, message passing is a series of operations over obstract functions.
 It is unclear how the brain would perform these operations, and DDC is
 a proposal of how the brain might implement message passing using vector
 representations putatively encoded in a neural population.
\end_layout

\begin_layout Standard
We review some key ideas that is relevant but not necessarilty related directly
 to the final message passing scheme.
 
\end_layout

\begin_layout Subsection
Kernel belief propagation
\end_layout

\begin_layout Standard
Consider a graph 
\begin_inset Formula $\mathcal{G}$
\end_inset

 of random variables 
\begin_inset Formula $\mathcal{X}=\{X_{a}\}$
\end_inset

 indexed by letters.
 The graph itself, without conditioning on any observations, implies a joint
 distribution 
\begin_inset Formula $P*(\mathcal{X})$
\end_inset

 over all variables.
 For simplicity, let us consider a problem of computing a conditional distributi
on 
\begin_inset Formula $P^{c}(X_{a})=P(x_{a}|X_{b})$
\end_inset

, and we would like to solve this problem by belief propagation in terms
 of RKHS functions.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "SonGreBic2011"

\end_inset

, the derivation of belief propagation follows from the results on reparameteris
ed message which is defined as 
\begin_inset Formula 
\begin{equation}
m_{a\to b}(X_{b})=\frac{m_{a\to b}^{c}(X_{b})}{m_{a\to b}^{*}(X_{b})}\label{eq:normalised_message}
\end{equation}

\end_inset

where 
\begin_inset Formula $m_{a\to b}^{c}(X_{b})$
\end_inset

 is the usual BP message from node 
\begin_inset Formula $a$
\end_inset

 to node 
\begin_inset Formula $b$
\end_inset

 
\series bold
conditioned on observed data
\series default
, and 
\begin_inset Formula $m_{a\to b}^{*}(X_{b})$
\end_inset

 is the usual BP message from node 
\begin_inset Formula $a$
\end_inset

 to node 
\begin_inset Formula $b$
\end_inset


\series bold
 without observing any data
\series default
.
 There are two consequences of this formulation.
\end_layout

\begin_layout Enumerate
Computing the marginal distribution on a node requires not only the messages
 but also the unconditional marginal distribution of the variable
\begin_inset Formula 
\[
P^{c}(X_{a})=\prod_{i\in ne(a)}m_{i\to b}^{c}(X_{a})=\prod_{i\in ne(a)}\left[m_{a\to b}^{*}(X_{b})m_{i\to b}(X_{a})\right]=P^{*}(X_{a})\prod_{i\in ne(a)}m_{i\to b}(X_{a})
\]

\end_inset


\end_layout

\begin_layout Enumerate
Message propagation is computed using not only the messages but also the
 unconditioned marginal distribution, e.g.
 for a message from 
\begin_inset Formula $a$
\end_inset

 sent to 
\begin_inset Formula $b$
\end_inset


\begin_inset Formula 
\begin{align*}
m_{a\to b}(X_{b}) & =\E_{p*(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}m_{u\to a}(X_{a})\right]
\end{align*}

\end_inset

where 
\begin_inset Formula $p*(X_{a}|X_{b})$
\end_inset

 can be estimated from data.
 
\end_layout

\begin_layout Standard
The second result follows from the following algebra.
 First, note that the following unobserved conditional distribution can
 be written as the potentials on the original graph
\begin_inset Formula 
\begin{align}
p^{*}(X_{a}|X_{b}) & =\frac{p^{*}(X_{a},X_{b})}{p^{*}(X_{b})}\nonumber \\
 & =\frac{\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})\prod_{u\in ne(b)\neg a}m_{u\to b}^{*}(X_{b})}{m_{a\to b}^{*}(X_{b})\prod_{u\in ne(b)\neg a}m_{u\to b}^{*}(X_{b})}\nonumber \\
 & =\frac{\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{m_{a\to b}^{*}(X_{b})}\label{eq:unboserveed_conditional}
\end{align}

\end_inset

The original usual belief update is written in terms of messages and potentials
 as 
\begin_inset Formula 
\begin{align*}
m_{a\to b}^{c}(X_{b}) & =\int\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{c}(X_{a})dX_{a}
\end{align*}

\end_inset

Now divide both sizes by the unobserved message 
\begin_inset Formula $m_{a\to b}^{*}(X_{b})$
\end_inset

, and multiply inside the integral the constant 
\begin_inset Formula $\frac{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}$
\end_inset


\begin_inset Formula 
\begin{align*}
\frac{m_{a\to b}^{c}(X_{b})}{m_{a\to b}^{*}(X_{b})} & =\frac{1}{m_{a\to b}^{*}(X_{b})}\int\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{c}(X_{a})\frac{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}dX_{a}\\
 & =\int\frac{\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{m_{a\to b}^{*}(X_{b})}\prod_{u\in ne(a)\neg b}\frac{m_{u\to a}^{c}(X_{a})}{m_{u\to a}^{*}(X_{a})}dX_{a}\\
m_{a\to b}(X_{b}) & =\E_{p(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}m_{u\to a}(X_{a})\right]
\end{align*}

\end_inset

where the last equality holds following Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:unboserveed_conditional"

\end_inset

).
 This is belief propagation under the message definition in Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:normalised_message"

\end_inset

).
\end_layout

\begin_layout Standard
This expectation under the unobserved data needs to be computed for the
 product of messages.
 If we assume that 
\begin_inset Formula $m_{u\to a}(X_{a})=\text{\left\langle m_{u\to a},\ \bphi(x_{a})\right\rangle }_{\cH}$
\end_inset

 where 
\begin_inset Formula $\bphi(X_{a})$
\end_inset

 is a vector in space 
\begin_inset Formula $\cH$
\end_inset

, and similarly that 
\begin_inset Formula $m_{a\to b}(X_{b})=\left\langle m_{u\to b},\ \bphi(X_{b})\right\rangle _{\cF}$
\end_inset

 where 
\begin_inset Formula $\bphi(X_{a})$
\end_inset

 is a vector in space 
\begin_inset Formula $\cF$
\end_inset

, then we can translate the product above into an inner product
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
m_{a\to b}(X_{b}) & =\E_{p(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}m_{u\to a}(X_{a})\right]\nonumber \\
 & =\E_{p(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}\left\langle m_{u\to a},\ \bphi(X_{a})\right\rangle _{\cH}\right]\nonumber \\
 & =\E_{p(X_{a}|X_{b})}\left[\left\langle \underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \underset{deg(a)-1}{\bigotimes}\bphi(X_{a})\right\rangle _{\cH^{\otimes}}\right]\nonumber \\
 & =\left\langle \underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \E_{p(X_{a}|X_{b})}\left[\underset{deg(a)-1}{\bigotimes}\bphi(X_{a})\right]\right\rangle _{\cH^{\otimes}}\nonumber \\
 & \stackrel{(1)}{=}\left\langle \underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \mathcal{W}_{X_{a}^{\otimes}|X_{b}}\bpsi(X_{b})\right\rangle _{\text{\cH^{\otimes}}}\nonumber \\
 & =\left\langle \mathcal{W}_{X_{a}^{\otimes}|X_{b}}^{\dagger}\underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \bpsi(X_{b})\right\rangle _{\text{\cF}}\nonumber \\
m_{a\to b} & =\mathcal{W}_{X_{a}^{\otimes}|X_{b}}^{\dagger}\underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a}\label{eq:kernel_BP}
\end{align}

\end_inset

where in (1) we assume that the conditional expectation 
\begin_inset Formula $\E_{p(X_{a}|x_{b})}\left[\large{\text{\otimes}}^{deg(b)-1}\bphi(X_{a})\right]$
\end_inset

, which is a function of 
\begin_inset Formula $X_{b}$
\end_inset

, is in the space where 
\begin_inset Formula $\bphi(X_{b})$
\end_inset

 is defined.
 
\end_layout

\begin_layout Standard
In particular, the message from an observed node will produce the following
 message
\begin_inset Formula 
\begin{align*}
\frac{m^{c}(X_{b})}{m^{*}(X_{b})} & =\frac{1}{m^{*}(x_{b})}\Phi(x_{b,}y_{b})\Phi(y_{b})\frac{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}\\
 & =\frac{\Phi(x_{b,}y_{b})\Phi(y_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}{m^{*}(x_{b})}\frac{1}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}\\
 & \propto p^{*}(x_{a}|x_{b})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Under this message passing scheme, computing the marginal probability under
 the observed condition requires the knowledge of the corresponding unconditione
d marginal distribution, which is in practise estimated by nonparametric
 methods.
 While appealing for kernelised inference where the model is fully observed,
 the above message passing scheme cannot be directly applied to latent variable
 models, such as Marko state-space model where the true generative parameters
 are unknown or misspecified, rendering 
\begin_inset Formula $P*(\mathcal{X})$
\end_inset

 unavailable.
 
\end_layout

\begin_layout Standard
In particular, we note that message passing is performed by an inner product
 between parameters 
\begin_inset Formula $\mathcal{W}_{X_{a}^{\otimes}|x_{b}}^{\dagger}$
\end_inset

 and the tensor product of incomming messages.
 In the section below, message passing using the mean representations of
 variables also share this structure.
\end_layout

\begin_layout Subsection
Kernel Bayes rule
\end_layout

\begin_layout Standard
Kernel Bayes rule 
\begin_inset CommandInset citation
LatexCommand cite
key "SmoGreSon2007"

\end_inset

 allows message propagation while taking into account normalisation based
 on conditional operators.
 We begin by reviewing the basic kernel probability computations.
 
\end_layout

\begin_layout Subsubsection
Mean representation of distributions
\end_layout

\begin_layout Paragraph
Mean embedding
\end_layout

\begin_layout Standard
Define the the mean embedding for random variable 
\begin_inset Formula $X$
\end_inset

 to be
\begin_inset Formula $\bmu_{X}=\E_{p(X)}\left[\phi(X)\right]$
\end_inset

, where 
\begin_inset Formula $\phi(X)\in\cH$
\end_inset

 is a vector of features, and similarly 
\begin_inset Formula $\bmu_{Y}=\E_{p(Y)}\left[\psi(X)\right]$
\end_inset

 for
\begin_inset Formula $\psi(X)\in\cF$
\end_inset

.
 The mean embedding is useful as it allows evaluation of expectations through
 a linear a linear transformation through the mean embeddings.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\E_{p(X)}\left[f(X)\right]=\E_{p(X)}\left[\left\langle f,\phi(X)\right\rangle \right]=\left\langle f,\E_{p(X)}\left[\phi(X)\right]\right\rangle =\left\langle f,\bmu_{X}\right\rangle \quad\forall f\in\cH,g\in\cF
\]

\end_inset


\end_layout

\begin_layout Standard
Similar to the expectations above, one can also define a mean representation
 for the joint distribution 
\begin_inset Formula $p(X,Y)$
\end_inset

 as the mean over the outer product of the feature maps
\begin_inset Formula 
\[
\cC_{XY}=\E_{p(X,Y)}\left[\bphi(x)\otimes\bpsi(y)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
with the property that any uncentered covariances between RKHS functions
 
\begin_inset Formula $f(x)$
\end_inset

 and 
\begin_inset Formula $g(y)$
\end_inset

 can be evaluated as 
\begin_inset Formula 
\[
\E_{p(X,Y)}\left[f(x)g(y)\right]=\left\langle f\otimes g,\E_{p(X,Y)}\left[\bphi(x)\otimes\bpsi(y)\right]\right\rangle _{\cH\otimes\cF}=\left\langle f\otimes g,C_{XY}\right\rangle _{\cH\otimes\cF}=\left\langle f,C_{XY}g\right\rangle _{H}
\]

\end_inset

where the last equality follows from the analogy with linear algebra 
\begin_inset Formula $\Tr(\y\x^{\intercal}\mathbf{A})=\x^{\intercal}\mathbf{A}\y$
\end_inset

 and 
\begin_inset Formula $C_{XY}$
\end_inset

 is viewed as an operator 
\begin_inset Formula $\cF\to\cH$
\end_inset

.
\end_layout

\begin_layout Paragraph
Conditional mean embeddings and kernel Bayes rule (KBR)
\end_layout

\begin_layout Standard
The conditional distribution 
\begin_inset Formula $p(Y|X=x)$
\end_inset

 can also be defined in a similar way.
 Denote by 
\begin_inset Formula $x$
\end_inset

 the value take by 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\[
\bmu_{Y|x}=\E_{p(Y|X=x)}\left[\bpsi(y)|X=x\right]\in\cF
\]

\end_inset


\end_layout

\begin_layout Standard
This mean maps a function 
\begin_inset Formula $f\in\cH$
\end_inset

 to its conditional expectation.
 
\begin_inset Formula 
\[
\E_{p(Y|X=x)}\left[g(y)\right]=\left\langle g,\E_{p(Y|X=x)}\left[\bpsi(y)\right]\right\rangle =\left\langle g,\bmu_{Y|x}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
In contrast to the 
\begin_inset Formula $\bmu_{X}$
\end_inset

 and 
\begin_inset Formula $\cC_{XY}$
\end_inset

, the conditional embedding 
\begin_inset Formula $\bmu_{Y|x}$
\end_inset

 is an element in 
\begin_inset Formula $\cF$
\end_inset

 as a function of 
\begin_inset Formula $x$
\end_inset

.
 It would be convenient to obtain a operator that achieves this mapping
 without using the definition above.
 Indeed, it can be shown 
\begin_inset CommandInset citation
LatexCommand cite
key "FukKenFra2004,SonHuaJon2009"

\end_inset

 that such an operator 
\begin_inset Formula $\cC_{Y|X}:\cH\to\cF$
\end_inset

 can be written as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\cC_{Y|X}=\cC_{YX}\cC_{XX}^{-1}\label{eq:conditional_operator}
\end{equation}

\end_inset

and when conditioned on a particular value of 
\begin_inset Formula $x$
\end_inset

 
\begin_inset Formula 
\begin{equation}
\bmu_{Y|x}=\cC_{Y|X}\bphi(x)\label{eq:conditional_mean}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
One assumption is that 
\begin_inset Formula $\E_{Y|X}\left[g(y)|X=\cdot\right]\in\cH$
\end_inset

 for all 
\begin_inset Formula $g\in\cF$
\end_inset

 without conditioninig on any particular 
\begin_inset Formula $x$
\end_inset

, so that 
\begin_inset Formula $\left\langle \E_{Y|X}\left[g(y)|X=\cdot\right],\phi(x)\right\rangle =\E_{Y|X}\left[g(y)|X=x\right]$
\end_inset

.
 To briefly show why 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:conditional_mean"

\end_inset

 holds, we need to show that 
\begin_inset Formula $\E_{Y|x}\left[g(y)|X=x\right]=\left\langle g,\cC_{YX}\cC_{XX}^{-1}\bphi(x)\right\rangle $
\end_inset

.
 For any 
\begin_inset Formula $f\in\cH$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\left\langle f,\cC_{XX}\E_{p(Y|X)}\left[g(Y)|X=\cdot\right]\right\rangle _{\cH} & =\left\langle f,\E_{p(X)}\left[\bphi(X)\otimes\bphi(X)\right]\E_{p(Y|X)}\left[g(Y)|X=\cdot\right]\right\rangle _{\cF}\\
 & \stackrel{(1)}{=}\E_{p(X)}\left[f(X)\left\langle \E_{Y|X}\left[g(y)|X=\cdot\right],\phi(X)\right\rangle _{\cF}\right]\\
 & =\E_{p(X)}\left[f(X)\E_{p(Y|X)}\left[g(Y)|X\right]\right]\\
 & =\E_{p(X,Y)}[f(X)\otimes g(y)]\\
 & =\left\langle f,C_{XY}g\right\rangle _{\cH}\\
\cC_{XX}\E_{p(Y|X)}\left[g(Y)|X=\cdot\right] & =C_{XY}g
\end{align*}

\end_inset

where at step (1) we used the assumption 
\begin_inset Formula $\E_{Y|X}\left[g(y)|X=\cdot\right]\in\cH$
\end_inset

 for all 
\begin_inset Formula $g\in\cF$
\end_inset

.
 By taking the tranpose of 
\begin_inset Formula $\cC_{XX}$
\end_inset

 and conditioning on a particular value of 
\begin_inset Formula $X=x$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\E_{Y|X}\left[g(y)|X=x\right] & = & \left\langle \cC_{XX}^{-1}\cC_{XY}g,\bphi(x)\right\rangle _{\cH}\\
 & = & \left\langle g,\cC_{YX}\cC_{XX}^{-1}\bphi(x)\right\rangle _{\cF}\\
\E_{Y|X}\left[\bpsi(y)|X=x\right] & = & \cC_{YX}\cC_{XX}^{-1}\bphi(x)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This means that the Bayes rule 
\begin_inset Formula $p(Y|X=x)=\frac{P(Y,X=x)}{P(X=x)}$
\end_inset

 can be translated to relationships between the corresponding kernel mean
 embeddings as
\begin_inset Formula 
\begin{equation}
\bmu_{Y|x}=C_{YX}\cC_{XX}^{-1}\bphi(x)\label{eq:KBR}
\end{equation}

\end_inset

which is known as the kernel Bayes rule.
\end_layout

\begin_layout Paragraph
Kernel sum rule (KSR)
\end_layout

\begin_layout Standard
Following similar algebra on can show that that the sum rule 
\begin_inset Formula $p(X)=\int dYp(X|Y)p(Y)$
\end_inset

 in terms of mean embeddings is 
\begin_inset Formula 
\begin{equation}
\bmu_{Y}=\cC_{Y|X}\bmu_{X}\label{eq:KSM}
\end{equation}

\end_inset

 since
\begin_inset Formula 
\[
\bmu_{Y}=\E_{p(Y)}\left[\bphi(Y)\right]=\E_{p(X)}\E_{p(Y|X)}\left[\bpsi(y)|X=x\right]=\E_{p(X)}\left[\cC_{Y|X}\bphi(x)\right]=\cC_{Y|X}\E_{p(X)}\left[\bphi(x)\right]=\cC_{Y|X}\bmu_{X}
\]

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Subsubsection
Propagation on a simple chain
\end_layout

\begin_layout Standard
In belief propagation, we wish to successively condition on observations
 and marginalise variables along a path in the graph.
 Consider first a simple Bayesian network of 
\begin_inset Formula $(D=d)\to X\to Y\to(Z=z)$
\end_inset

 where 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

 are observed to be 
\begin_inset Formula $d$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

, respectively .
 Consider the situation where the mean embedding of 
\begin_inset Formula $\bmu_{X|D=d}$
\end_inset

 is computed, and we wish to propagated this mean to obtain the mean embedding
 for 
\begin_inset Formula $Y$
\end_inset

.
 To simplifiy notation, define a common feature map 
\begin_inset Formula $\bphi(\cdot)\in\cH$
\end_inset

 for all variables.
 Repeated use of KBR gives
\begin_inset Formula 
\begin{align*}
\bmu_{Y|Z=z,D=d} & =C_{YZ|d}C_{ZZ|d}^{-1}\bphi(z)\\
 & =\left[C_{YZD}C_{DD}^{-1}\bphi(d)\right]\left[C_{ZZD}C_{DD}^{-1}\bphi(d)\right]^{-1}\bphi(z)
\end{align*}

\end_inset

By definition, 
\begin_inset Formula $\bmu_{X|d}=C_{XD}C_{DD}^{-1}\bphi(d)\implies C_{DD}^{-1}\bphi(d)=C_{XD}^{\dagger}\bmu_{X|d}$
\end_inset

, substituting this into the above gives
\begin_inset Formula 
\[
\bmu_{Y|Z=z,D=d}=\left[C_{YZD}C_{XD}^{\dagger}\bmu_{X|d}\right]\left[C_{ZZD}C_{XD}^{\dagger}\bmu_{X|d}\right]^{-1}\bphi(z)
\]

\end_inset


\end_layout

\begin_layout Standard
This update rule is taken by in the kernel predictive state representation
 of latent chain models 
\begin_inset CommandInset citation
LatexCommand cite
key "BooByrGeo2013"

\end_inset

 and also motivated the architecture of the predictive state recurrent neural
 network 
\begin_inset CommandInset citation
LatexCommand cite
key "DowCarAhm2017"

\end_inset

.
 While being able to take into account all past data on potentially non-Markovia
n latent structure, one needs design features for all previous data history
 
\begin_inset Formula $\psi(D)$
\end_inset

 if 
\begin_inset Formula $D$
\end_inset

 involves previous observed nodes.
 
\end_layout

\begin_layout Standard
Another way of propagating messages leavages the conditional independence
 relationships within the chain 
\begin_inset Formula $p(Y,Z|D)=\int dDp(Y,Z|X)p(X|D)$
\end_inset

, which, using kernel sum rule (Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:KSM"

\end_inset

), can be written in mean embeddings as 
\begin_inset Formula 
\[
C_{YZ|d}=C_{YZ|X}\bmu_{X|d}
\]

\end_inset

and likewise
\begin_inset Formula 
\[
C_{ZZ|d}=C_{ZZ|X}\bmu_{X|d}
\]

\end_inset

And, therefore
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\bmu_{Y|Z=z,D=d} & =\left[C_{YZ|X}\bmu_{X|d}\right]\left[C_{ZZ|X}\bmu_{X|d}\right]^{-1}\bphi(z)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
More generally, using the common notation as above, we can compute messages
 in the following way.
 Note that the conditional independence of the graph suggests that 
\begin_inset Formula $p(X_{a}|X_{u},d_{u})=p(X_{a}|X_{u})$
\end_inset

.
 To shorten notation, denote by 
\begin_inset Formula $\{X_{u}\}$
\end_inset

 the set of latent variables in the upstream neighbourhood of 
\begin_inset Formula $X_{a}$
\end_inset


\begin_inset Formula 
\begin{align*}
\bmu_{a} & =C_{X_{a}Y_{a}|\left\{ d_{u}\right\} }C_{Y_{a}Y_{a}|d_{u}}^{-1}\nu(y_{a})\\
 & =\left[C_{X_{a}Y_{a}|\left\{ X_{u}\right\} ,\left\{ d_{u}\right\} }C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]\left[C_{X_{a}X_{a}|\left\{ X_{u}\right\} ,\left\{ d_{u}\right\} }C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]^{-1}\nu(y_{a})\\
 & =\left[C_{X_{a}Y_{a}\{X_{u}\}}C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]\left[C_{Y_{a}Y_{a}\{X_{u}\}}C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]^{-1}\nu(y_{a})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
However, in practice, due to the regularised inverse, propagating message
 along a large graph (e.g.
 a markov chain) results in accumulation of the errors in the mean embeddings.
 
\end_layout

\begin_layout Subsection
Message mean representation (MMR)
\end_layout

\begin_layout Standard
Here we derive a new message passing scheme based on mean representation
 of unnormalized likelihoods or messages.
 The key idea is that we which to use a finite set of basis function which
 can be used to approximate certain integrals of potentials in the graph.
 Using a infinitely large set of basis functions, such as the features implied
 by universal kernels, then by universality this set of feature functions
 can approximate any bounded continuous functions, which is usually the
 case for many distributions and messages.
 
\end_layout

\begin_layout Standard
We begin by stating the above assumptions.
 As above, define vector features and its corresponding feature space as
 
\begin_inset Formula $\bphi(X_{a})$
\end_inset

 and 
\begin_inset Formula $\bpsi_{u}(X_{u})$
\end_inset

 for 
\begin_inset Formula $u\in ne(a)$
\end_inset

; Additionaly, we make explicit the observed data attached to variable 
\begin_inset Formula $X_{a}$
\end_inset

 to be 
\begin_inset Formula $Y_{a}=y_{a}$
\end_inset

 and define its feature represenation as
\begin_inset Formula $\bnu(y_{a})$
\end_inset

.
 We assume that we can approximate the following
\begin_inset Formula 
\begin{align}
\int dX_{a}\bphi(X_{a})\prod_{u\in ne(a)}\Phi_{a,u}(X_{a,}X_{u},y_{a})\Phi_{u}(X_{u},y_{a}) & \approx\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\bpsi_{u}(X_{u})\right\rangle \label{eq:approx_expectation}\\
 & =\sum_{i_{1},\dots,i_{deg(a)},k}W_{i_{1},\dots,i_{deg(a)}}^{(a)}\nu_{k}(y_{a})\prod_{u\in ne(a)}\psi_{u,i_{u}}(X_{u})~\forall a\in\cG
\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $W_{i_{1},\dots,i_{deg(a)}}^{(a)}$
\end_inset

 are parameters and 
\begin_inset Formula $deg(a)$
\end_inset

 means the number of incoming mean representations.
 Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_expectation"

\end_inset

 states that the integral of the product of the relevant potentials over
 the variable we are computing the mean representation 
\begin_inset Formula $X_{a}$
\end_inset

 (similar to the out-going message from 
\begin_inset Formula $X_{a}$
\end_inset

) should be well approximatd by the set of basis function formed by the
 outer product of all the basis functions associated with the other incoming
 neighbouring variables 
\begin_inset Formula $X_{u}$
\end_inset

 and data 
\begin_inset Formula $y_{a}$
\end_inset

.
 This is analogous to the assumption made at in step (1) in deriving Equation
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kernel_BP"

\end_inset

).
\end_layout

\begin_layout Standard
Instead of messages, we consider (possibly unnormalised) expectations of
 nonlinear functions for variables 
\begin_inset Formula $\tilde{\bmu}_{u}:=\E_{\tilde{p}(X_{u}|d_{u})}\left[\bpsi(X_{u})\right]$
\end_inset

 where 
\begin_inset Formula $d_{u}$
\end_inset

 is the data conditioned on 
\begin_inset Formula $D_{u}$
\end_inset

 the upstream of 
\begin_inset Formula $u$
\end_inset

.
 We restrict 
\begin_inset Formula $u\in ne(a)=\{1,...,deg(a)\}$
\end_inset

 to be in the neighbours from which messages propagate from.
 
\begin_inset Formula $\tilde{\bmu}_{u}$
\end_inset

 is essentially the unnormalized mean representation of 
\begin_inset Formula $\tilde{p}(X_{u}|d_{u})$
\end_inset

.
 Given such previously computed means conditioned on the set of observations
 
\begin_inset Formula $\left\{ D_{u}=d_{n}\right\} _{u\in ne(a)}$
\end_inset

, we want to obtain the mean representation
\begin_inset Formula $\tilde{\bmu}_{a}:=\E_{\tilde{p}(X_{a}|\left\{ d_{u}\right\} _{u\in ne(a)},y_{d})}\left[\bphi(X_{u})\right]$
\end_inset

 given new data 
\begin_inset Formula $y_{a}$
\end_inset

 attached to the relevant potentials.
\begin_inset Formula 
\begin{align}
\tilde{\bmu}_{a} & =\int dX_{a}\bphi(X_{a})\tilde{p}(X_{a}|\left\{ d_{u}\right\} _{u\in ne(a)},X_{d})\nonumber \\
 & =\int dX_{a}\bphi(X_{a})\prod_{u\in ne(a)}\int dX_{u}\tilde{p}(X_{u}|d_{u})\Phi_{a,u}(X_{a,}X_{u},X_{d})\Phi_{u}(X_{u},y_{a})\nonumber \\
 & =\int\dots\int\left[\prod_{u\in ne(a)}dX_{u}\tilde{p}(X_{u}|d_{u})\right]\int dX_{a}\bphi(X_{a})\prod_{u\in ne(a)}\Phi_{a,u}(X_{a,}X_{u},y_{a})\Phi_{u}(X_{u},y_{a})\nonumber \\
 & \stackrel{(1)}{}\approx\int\dots\int\left[\prod_{u\in ne(a)}dX_{u}\tilde{p}(X_{u}|d_{u})\right]\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\bpsi_{u}(X_{u})\right\rangle \nonumber \\
 & =\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\left[\int dX_{u}\tilde{p}(X_{u}|d_{u})\bpsi_{u}(X_{u})\right]\right\rangle \nonumber \\
 & =\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\tilde{\bmu}_{u}(X_{u})\right\rangle \label{eq:approx_dist_multilinear}
\end{align}

\end_inset

where in step (1) we used the assumption of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_expectation"

\end_inset

.
 It is worth noting that the means are unnormalized, which does not matter
 for inference.
 To normalize the means, one can use divisive normalisation.
 For example, to normalize 
\begin_inset Formula $\tilde{\bmu}$
\end_inset

, find a set of weights 
\begin_inset Formula $\mathbf{n}$
\end_inset

 to approximate the constant function with the basis function
\begin_inset Formula $\sum_{i}n_{i}\phi_{i}(X)=1$
\end_inset

, then the normalizer for 
\begin_inset Formula $\tilde{p}(X|D)$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
Z(D) & =\int\tilde{p}(X|D)dX\\
 & =\int\tilde{p}(X|D)\sum_{i}n_{i}\phi_{i}(X)dX\\
 & =\sum_{i}n_{i}\tilde{\mu}_{i}\\
 & =\mathbf{n}^{\intercal}\tilde{\bmu}
\end{align*}

\end_inset

and the normalized mean is 
\begin_inset Formula 
\begin{align}
\bmu & =\frac{\tilde{\bmu}}{\mathbf{n}^{\intercal}\tilde{\bmu}}\label{eq:approx_dist_normalize}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Although it is not easy to find the weigths 
\begin_inset Formula $\mathcal{W}$
\end_inset

 using traning samples when the messages involve a data observation (we
 illustrate the reason for a special case in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:ssm_case"

\end_inset

), this message passing scheme that combines 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_dist_multilinear"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_dist_normalize"

\end_inset

 motivates a neural network structure that combines inputs multilinearly
 in the feature space.
 Given a task and an error signal, we can hopefully obtain the weights using
 backpropagation.
\end_layout

\begin_layout Subsection
Distribution regression to distribution (DRD)
\end_layout

\begin_layout Standard
In the previous sections, we discussed ideas about how to perform that was
 trained using samples but was intended for use on mean representations
 during test time.
 Is it possible to train also using mean representations? We can treat the
 mean representation of upstream messages 
\begin_inset Formula $\left\{ \bmu_{u}(X_{u})\right\} {}_{u\in ne(a)}$
\end_inset

 as input vectors, and simply learn a function that map them, together with
 features of observed data 
\begin_inset Formula $\bnu(y_{a})$
\end_inset

, to the next mean embedding.
 
\begin_inset Formula 
\begin{equation}
\bmu_{a}=g\left(\left\{ \bmu_{u}(X_{u})\right\} {}_{u\in ne(a)},y_{a}\right)=\left\langle g,\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\bmu_{u}(X_{u})\right\rangle \label{eq:distribution_regression}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We call this approach distribution regression.
 The function 
\begin_inset Formula $h(\cdot)$
\end_inset

 is assumed to be linear in feature space.
 It is also worth noting that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:distribution_regression"

\end_inset

 is very similar to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kernel_BP"

\end_inset

 in that the forms of message passing are identical, even though the messages
 combined were likelihoods (normalized by the unconditioned message) for
 kernel BP and mean representations for distribution regression.
 Unlike kernel belief propagation where the operators 
\begin_inset Formula $\cC$
\end_inset

's are estimated from samples in a way that respects the true underlying
 relationships between the variables, i.e.
 using KSR to propagate message to another latent node followed KBR that
 takes the message from the observation attached to that node, the regression
 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:distribution_regression"

\end_inset

) does not care about the actual relationships between the variables, and
 also obviates the need for the assumption in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_expectation"

\end_inset

) as long as the function 
\begin_inset Formula $h(\cdot)$
\end_inset

 is able to discover the relationships between the input and output messages.
 A similar situation has been studied by Szabo et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "szabo2016learning"

\end_inset

 in the infinite dimensional feature case where the inputs are distributions
 and output is a scalar or label of interest.
 
\end_layout

\begin_layout Standard
To learn the function 
\begin_inset Formula $g\left(\cdot\right)$
\end_inset

 on a directed graph, one can simulate the model by initializing the prior
 nodes with samples whose feature vectors are averaged to create the prior
 mean representations.
 Conditioned on each set of prior samples, different sets of samples need
 to be drawn in order to create enough training mean representations for
 regresion.
 One can iteratively learn the regressor 
\begin_inset Formula $h(\cdot)$
\end_inset

 by training a regressor to take mean representations of the parent nodes
 to predict feature vectors of samples at the child nodes under the squared
 loss.
 The predicted feature vectors can then be used as the mean representation
 for the child nodes in the next regression.
 Note that the set of data points for regression are in fact derived from
 the many sets of samples drawn from the graph.
\end_layout

\begin_layout Standard
Learning 
\begin_inset Formula $g\left(\cdot\right)$
\end_inset

 on an undirected graph has not been considered yet, the difficulty being
 that the there is no known prior distributions to start the recursive trainng.
 
\end_layout

\begin_layout Section
Message passing for state-space models
\begin_inset CommandInset label
LatexCommand label
name "sec:ssm_case"

\end_inset


\end_layout

\begin_layout Standard
We now consider the particular type of inference problem described by the
 following set of distributions describing a state-space model 
\begin_inset Formula $p(Z_{1}),p(Z_{t}|Z_{t-1}),p(Z_{t}|X_{t})$
\end_inset

 where 
\begin_inset Formula $Z_{t}$
\end_inset

 is latent and 
\begin_inset Formula $X_{t}$
\end_inset

 are observed.
 We denote by 
\begin_inset Formula $\bmu_{Z_{t}|x_{1:t}}=\E_{p(Z_{t}|x_{1:t})}\left[\bpsi(Z_{t})\right]$
\end_inset

 the mean representation of posterior 
\begin_inset Formula $p(Z_{t}|x_{1:t})$
\end_inset

 and 
\begin_inset Formula $\bphi(x_{t})$
\end_inset

 the feature of observation.
 Following the message passing schemes described above, we have the following
 ways of performing the filtering operation.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\text{KBR:} &  & \bmu_{Z_{t}|x_{1:t}}= & \left[C_{Z_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]\left[C_{X_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]^{-1}\bphi(x_{t})\\
\text{MMR:} &  & \tilde{\bmu}{}_{Z_{t}|x_{1:t}}= & \left\langle \cW,\bmu{}_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\right\rangle  & \bmu{}_{Z_{t}|x_{1:t}}=\frac{\tilde{\bmu}{}_{Z_{t}|x_{1:t}}}{\mathbf{n}^{\intercal}\tilde{\bmu}{}_{Z_{t}|x_{1:t}}}\\
\text{DRD:} &  & \bmu_{Z_{t}|x_{1:t}}= & \left\langle g_{t},\bmu{}_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\right\rangle 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For KBR, the parameters 
\begin_inset Formula $C_{Y_{t}X_{t}|Y_{t-1}}$
\end_inset

 and 
\begin_inset Formula $C_{X_{t}X_{t}|Y_{t-1}}$
\end_inset

 are estimated from samples drawn from the stationary distribution.
 For DRD, the regressor 
\begin_inset Formula $h_{t}$
\end_inset

 is trained at each time step starting from an arbitrary prior until the
 chains reach stationarity.
 When the posteriors only weakly depends on the first observation 
\begin_inset Formula $\bmu_{Y_{t}|x_{1:t}}\approx\bmu_{Y_{t}|x_{t-\tau::t}}$
\end_inset

 for some 
\begin_inset Formula $0<\tau<t$
\end_inset

 and for all 
\begin_inset Formula $t$
\end_inset

, we can assume that 
\begin_inset Formula $h_{t}$
\end_inset

 is stationary and thus can be used at all future time steps.
 For MMR, however, the weights cannot be obtained directly.
 To see why, we write the filtering operation in the mean representation
 of messsages as
\begin_inset Formula 
\begin{align}
\tilde{\bmu}{}_{Z_{t}|x_{1:t}} & =\int dz_{t}\bpsi(z_{t})\tilde{p}(z_{t}|x_{1:t})\label{eq:ssm-mme-def}\\
 & =\int dz_{t}\bpsi(z_{t})\int dz_{t-1}p(z_{t}|z_{t-1})p(x_{t}|z_{t})p(z_{t-1}|x_{1:t-1})\\
 & =\int dz_{t-1}\left[\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1})p(x_{t}|z_{t})\right]p(z_{t-1}|x_{1:t-1})\label{eq:ssm-mme-integral}\\
 & \stackrel{(1)}{\approx}\int dz_{t-1}\left[\left\langle \cW,\bphi(x_{t})\otimes\bpsi(z_{t})\right\rangle \right]p(z_{t-1}|x_{1:t-1})\label{ssm-mme}
\end{align}

\end_inset

where in step (1) we used the equivalent of Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_expectation"

\end_inset

 to approximate the integral in the square brackets.
 In a simpler scenario where there is no data likelihood 
\begin_inset Formula $p(x_{t}|z_{t})$
\end_inset

), this integral is a conditional expectation and the weights 
\begin_inset Formula $\cW$
\end_inset

 becomes a regressor from 
\begin_inset Formula $z_{t-1}$
\end_inset

 to 
\begin_inset Formula $z_{t}$
\end_inset

.
 With the likelihood, this integral becomes an unnormalized expectation
 and cannot be approximated by regression.
 We discuss potential remedies in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:remedies-for-unnorm-exp"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Similarities between DRD and KBR for filtering
\end_layout

\begin_layout Standard
Here, we focus on the novel DRD scheme and compare it with the KBR solution
 which is known to be consistent using infinitely many features.
 The function 
\begin_inset Formula $g$
\end_inset

, if multilinear in its input features, can be written as
\begin_inset Formula 
\begin{align}
g_{t} & =\underbrace{\E_{p(x_{1:t},z_{t})}\left[\bpsi(z_{t})\otimes\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right]}_{\cW_{\bpsi_{t}(\bphi_{t}\bmu_{t-1})}}\underbrace{\E_{p(x_{1:t})}\left[\left(\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right)\otimes\left(\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right)\right]^{-1}}_{\cW_{(\bphi_{t}\bmu_{t-1})(\bphi_{t}\bmu_{t-1})}^{-1}}\label{eq:ssm_drd_regressor}
\end{align}

\end_inset

Denoting the random observation history 
\begin_inset Formula $X_{1:t}$
\end_inset

 as 
\begin_inset Formula $H_{t}$
\end_inset

, the corresponding observations 
\begin_inset Formula $x_{1:t}$
\end_inset

 as 
\begin_inset Formula $h_{t}$
\end_inset

 and its feature as 
\begin_inset Formula $\bphi^{\otimes t}(h_{t})$
\end_inset

 
\begin_inset Foot
status open

\begin_layout Plain Layout
since the feature for the history up to time 
\begin_inset Formula $t$
\end_inset

 is the tensor product 
\begin_inset Formula $\bigotimes_{i=1}^{t}\bphi(x_{i}$
\end_inset

)
\end_layout

\end_inset

.
 The first expectation in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm_drd_regressor"

\end_inset

can be simplified as 
\begin_inset Formula 
\begin{align*}
\cW_{\bpsi_{t}(\bphi_{t}\bmu_{t-1})} & =\E_{p(h_{t-1})}\left[\E_{p(z_{t},x_{t}|h_{t-1})}\left[\bpsi(z_{t})\otimes\bphi(x_{t})\right]\otimes\bmu_{Z_{t-1}|h_{t-1}}\right]\\
 & =\E_{p(h_{t-1})}\left[\cC_{Z_{t}X_{t}|h_{t-1}}\otimes\bmu_{Z_{t-1}|h_{t-1}}\right]\\
 & =\E_{p(h_{t-1})}\left[\left\langle \cC_{Z_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \otimes\left\langle \cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \right]\\
 & =\E_{p(h_{t-1})}\left[\left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\otimes\bphi^{\otimes t-t}(h_{t-1})\right\rangle \right]\\
 & =\left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\E\left[\bphi^{\otimes t-1}(h_{t-1})\otimes\bphi^{\otimes t-1}(h_{t-1})\right]\right\rangle \\
 & =\left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes2}}\right\rangle 
\end{align*}

\end_inset

where 
\begin_inset Formula $\cC_{H_{t-1}^{\otimes n}}:=\E\left[\bigotimes_{i}^{n}\left[\bphi^{\otimes t-1}(h_{t-1})\right]\right]$
\end_inset

.
 The second expectation in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm_drd_regressor"

\end_inset

 can be simplified similarly
\begin_inset Foot
status open

\begin_layout Plain Layout
The tensor product, shortened as 
\begin_inset Formula $\left(\bphi\otimes\bmu\right)\otimes\left(\bphi\otimes\bmu\right)$
\end_inset

, is implemend using matrix operations.
 For the following, we use
\begin_inset Formula $\otimes$
\end_inset

 as Kronecker product and 
\begin_inset Formula $vec(\dot{\cdot)}$
\end_inset

 as the operation that concatenates the columns into a vector.
 The term inside the expectation is in fact
\begin_inset Formula 
\[
vec(\bphi\otimes\bmu^{T})vec^{T}(\bphi\otimes\bmu^{T})=(\bphi\bphi^{T})\otimes(\bmu\bmu^{T})
\]

\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\begin{align*}
\cW_{(\bphi_{t}\bmu_{t-1})(\bphi_{t}\bmu_{t-1})} & =\E_{p(h_{t-1})}\left[\cC_{X_{t}X_{t}|h_{t-1}}\otimes\left\langle \cC_{Z_{t-1}|h_{t-1}},\cC_{Z_{t-1}|h_{t-1}}\right\rangle \right]\\
 & =\E_{p(h_{t-1})}\left[\left\langle \cC_{X_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \otimes\left\langle \cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\otimes\bphi^{\otimes t-1}(h_{t-1})\right\rangle \right]\\
 & =\left\langle \cC_{X_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes3}}\right\rangle 
\end{align*}

\end_inset

which gives the recursion.
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}|x_{1:t}} & =\left\langle \left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes2}}\right\rangle \left\langle \cC_{X_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes3}}\right\rangle ^{-1},\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right\rangle \\
 & =\left\langle \left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes2}}\right\rangle \left\langle \cC_{X_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes3}}\right\rangle ^{-1}\cC_{Z_{t-1}|H_{t-1}},\bphi(x_{t})\otimes\bphi^{\otimes t-1}(h_{t-1})\right\rangle 
\end{align*}

\end_inset

Compare with KBR which can be written as 
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}|x_{1:t}} & =\left[C_{Z_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]\left[C_{X_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]^{-1}\bphi(x_{t})\\
 & =\left\langle C_{Z_{t}X_{t}|Z_{t-1}}\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \left\langle C_{X_{t}X_{t}|Z_{t-1}}\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle ^{-1}\bphi(x_{t})\\
 & =\left\langle C_{Z_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \left\langle C_{X_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle ^{-1}\bphi(x_{t})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Both expressions involve the key covariance operators 
\begin_inset Formula $\cC_{Z_{t}X{}_{t}|H_{t-1}}$
\end_inset

 and 
\begin_inset Formula $\cC_{X_{t}X{}_{t}|H_{t-1}}$
\end_inset

.
 However, in the regression operator of DRD, there are also terms that involve
 the filtering posterior operator 
\begin_inset Formula $\cC_{Z_{t-1}|H_{t-1}}$
\end_inset

 and 2nd and 3rd moment representations of the history 
\begin_inset Formula $\cC_{H_{t-1}^{\otimes3}}$
\end_inset

, whereas in KBR no such terms are involved.
 Critically, there is no interaction between the two covariance operators
 and filtering posterior operator.
 Thus, intuitively, if the filtering operator is only approximate (due to
 regularisation), DRD is able to take into account any errors in the posterior.
 Higher order moments of the observation history may also help enhance the
 regression.
\end_layout

\begin_layout Subsection
Strategies for evaluating the integral inside 
\begin_inset CommandInset label
LatexCommand label
name "subsec:remedies-for-unnorm-exp"

\end_inset


\end_layout

\begin_layout Standard
Here we a discuss the issue of evaluating the integral in the square bracket
 of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm-mme-integral"

\end_inset

as part of process for learning a filter by MMR.
 Denote this expectation as 
\begin_inset Formula $E_{t}=\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1})p(x_{t}|z_{t})$
\end_inset

.
 Since it is not a expectation under a normalized density, training a regressor
 from 
\begin_inset Formula $z_{t-1}$
\end_inset

 and 
\begin_inset Formula $x_{t}$
\end_inset

 to predict 
\begin_inset Formula $z_{t}$
\end_inset

 will not yield the desired weights 
\begin_inset Formula $\cW$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "ssm-mme"

\end_inset

.
 This can be further verified by rewriting 
\begin_inset Formula $E_{t}$
\end_inset

 as follows, noting the fact that 
\begin_inset Formula $p(z_{t}|z_{t-1},x_{t})=\frac{p(z_{t}|z_{t-1})p(x_{t}|z_{t})}{p(x_{t}|z_{t-1})}$
\end_inset


\begin_inset Formula 
\begin{align*}
E_{t} & =\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1},x_{t})p(x_{t}|z_{t-1})\\
 & \approx\bmu_{Z_{t}|z_{t-1},x_{t}}p(x_{t}|z_{t-1})
\end{align*}

\end_inset

The regression method will produce the correct estimate of 
\begin_inset Formula $\bmu_{Z_{t}|z_{t-1},x_{t}}$
\end_inset

 but will miss a factor 
\begin_inset Formula $p(x_{t}|z_{t-1})$
\end_inset

 that depends on 
\begin_inset Formula $z_{t-1}$
\end_inset

 for 
\begin_inset Formula $E_{t}$
\end_inset

.
 The update equation for the next normalized mean embedding in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm-mme-def"

\end_inset

 becomes
\begin_inset Formula 
\[
\tilde{\bmu}{}_{Z_{t}|x_{1:t}}=\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}p(x_{t}|z_{t-1})p(z_{t-1}|x_{1:t-1})
\]

\end_inset

To take into account the normalizer, we can express the normalized mean
 embedding in several ways.
 
\begin_inset Formula 
\begin{align}
\bmu{}_{Z_{t}|x_{1:t}} & =\frac{1}{p(x_{t}|x_{1:t-1})}\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}p(x_{t}|z_{t-1})p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}\frac{p(x_{t}|z_{t-1})}{p(x_{t}|x_{1:t-1})}p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}\frac{p(x_{t},z_{t-1}|x_{1:t-1})}{p(x_{t}|x_{1:t-1})p(z_{t-1}|x_{1:t-1})}p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}\frac{p(z_{t-1}|x_{1:t})}{p(z_{t-1}|x_{1:t-1})}p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}p(z_{t-1}|x_{1:t})\label{eq:ssm-mme-recursion-lag-input}\\
 & \ne\left\langle \cC_{Z_{t}|Z_{t-1},x_{t}},\bmu_{z_{t-1}|x_{1:t-1}}\right\rangle \nonumber 
\end{align}

\end_inset

where 
\begin_inset Formula $\cC_{Z_{t}|Z_{t-1},x_{t}}$
\end_inset

 is a conditional operator such that 
\begin_inset Formula $\bmu_{Z_{t}|z_{t-1},x_{t}}=\left\langle \cC_{Z_{t}|Z_{t-1},x_{t}},\psi(z_{t-1})\right\rangle $
\end_inset

 and from the last line, we see that the desired input to the learned regressor
 would be the mean representation of 
\begin_inset Formula $p(z_{t-1}|x_{1:t})$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Known likelihood function
\end_layout

\begin_layout Standard
If this likelihood function is known, we could approximate it with 
\begin_inset Formula $p(x_{t}|z_{t})\approx\left\langle p,\bphi(z_{t})\otimes\bpsi(x_{t})\right\rangle $
\end_inset

, and this integral becomes
\begin_inset Formula 
\begin{align*}
E_{t} & =\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1})p(x_{t}|z_{t})\\
 & =\int dz_{t}\bpsi(z_{t})\left\langle p,\bphi(z_{t})\otimes\bpsi(x_{t})\right\rangle p(z_{t}|z_{t-1})\\
 & =\left\langle p,\bphi(x_{t})\otimes C_{Z_{t}Z_{t}|z_{t-1}}\right\rangle 
\end{align*}

\end_inset

Nontheless, in general we would like to avoid approximating the likelihood
 function as it is inaccessible in many situations.
 
\end_layout

\begin_layout Subsubsection
Fixed-lag smoothing
\end_layout

\begin_layout Standard
Another solution can be seen by recursing on a slightly different object.
 Instead of 
\begin_inset Formula $p(z_{t}|x_{1:t})$
\end_inset

, if we are interested in a fix-lag version of the posterior given some
 future observations
\begin_inset Formula $p(z_{t}|x_{1:t+\tau})$
\end_inset

 for some 
\begin_inset Formula $\tau>0$
\end_inset

, then we may assume that 
\begin_inset Formula $p(z_{t}|x_{1:t+\tau})\approx p(z_{t}|x_{1:t+\tau-1})$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm-mme-recursion-lag-input"

\end_inset

 it is easy to see 
\begin_inset Formula 
\begin{align*}
\bmu{}_{Z_{t}|x_{1:t+\tau}} & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t:t+\tau}}p(z_{t-1}|x_{1:t+\tau})\\
 & \approx\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t:t+\tau}}p(z_{t-1}|x_{1:t+\tau-1})\\
 & =\left\langle \cC_{Z_{t}|Z_{t-1},x_{t:t+\tau}},\bmu_{Z_{t-1}|x_{1:t+\tau-1}}\right\rangle 
\end{align*}

\end_inset

which requires a regressor that takes 
\begin_inset Formula $z_{t-1}$
\end_inset

 and 
\begin_inset Formula $x_{t:t+\tau}$
\end_inset

 as input to predict 
\begin_inset Formula $z_{t}$
\end_inset

, and the predictions can be used as 
\begin_inset Formula $\bmu_{Z_{t}|z_{t-1},x_{t:t+\tau}}$
\end_inset

.
 There is a trade-off between how well this dropped condition approximation
 holds and how complicated the regressor have to be.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ref"
options "plain"

\end_inset


\end_layout

\end_body
\end_document

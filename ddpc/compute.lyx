#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{amsmath}
\newcommand{\bx}{\mathbf{x}}
\DeclareMathOperator{\Tr}{Tr}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\noindent
DDC Message Passing
\end_layout

\begin_layout Author
\noindent
Kevin W Li
\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\bphi}{\bm{\phi}}
{\bm{\phi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bmu}{\bm{\mu}}
{\bm{\mu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bSigma}{\bm{\Sigma}}
{\bm{\Sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bI}{\bm{I}}
{I}
\end_inset


\begin_inset FormulaMacro
\newcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\E}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\H}{\mathbb{H}}
{\mathbb{H}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bpsi}{\bm{\psi}}
{\bm{\psi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cW}{\mathcal{W}}
{\mathcal{W}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\m}{\mathbf{m}}
{\mathbf{m}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bxi}{\bm{\xi}}
{\bm{\xi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cH}{\mathcal{H}}
{\mathcal{H}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cF}{\mathcal{F}}
{\mathcal{F}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cG}{\mathcal{G}}
{\mathcal{G}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cC}{\mathcal{C}}
{\mathcal{C}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cU}{\mathcal{U}}
{\mathcal{U}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\cN}{\mathcal{N}}
{\mathcal{N}}
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\lotimes}{\Large{\otimes}}
{\Large{\otimes}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\bnu}{\bm{\nu}}
{\bm{\nu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
{\textrm{arg min}}
\end_inset


\end_layout

\begin_layout Section
Learning deep generative models using wake-sleep
\end_layout

\begin_layout Standard
In this section we consider using DDC to learn a deep generative model whose
 structure is specified by the a prior distribution 
\begin_inset Formula $p_{0}(Z_{0}|\theta_{0})$
\end_inset

 and sequence of conditional 
\begin_inset Formula $p_{l+1}(Z_{l+1}|Z_{l};\theta_{l+1})$
\end_inset

 for 
\begin_inset Formula $l\in\{0,\dots L-1\}$
\end_inset

 for an 
\begin_inset Formula $L$
\end_inset

 layer generative model.
 Let 
\begin_inset Formula $x=z_{L}$
\end_inset

 to be the only observed variable.
 We make two assumptions about the generative model
\end_layout

\begin_layout Enumerate
It is easy to obtain samples from the prior and each conditional
\end_layout

\begin_layout Enumerate
The gradients of the log normalized densities with respect to parameters
 exist everywhere and are easy to compute in closed form.
\end_layout

\begin_layout Standard
To learn the parameters, one can maximise the free energy by adjusting the
 parameters of the model, which requires an approximate posterior of 
\begin_inset Formula $q(Z|X)$
\end_inset

.
 This process corresponds to 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\Delta\theta=\left\langle \frac{\partial}{d\theta}\log p_{\theta}(Z,X)\right\rangle _{q(Z|X)}.\label{eq:general_wake}
\end{equation}

\end_inset

 The approximate posterior can be found by an amortised learning scheme
 where we build a recognition model to output some parameterisation of the
 posterior distribution 
\begin_inset Formula $q_{\psi}(Z|X)$
\end_inset

, and the parameters 
\begin_inset Formula $\psi$
\end_inset

 are adjusted by maximizing the likelihoods of samples drawn from the current
 generative model under the recognition model
\begin_inset Formula 
\begin{equation}
\Delta\psi=\left\langle \frac{\partial}{\partial\psi}\log q_{\psi}(Z|X)\right\rangle _{p_{\theta}(X,Z)}\label{eq:general_sleep}
\end{equation}

\end_inset

This alternating learning scheme is known as the wake-sleep algorithm, with
 the wake phase corresponding to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:general_wake"

\end_inset

 and the sleep phase to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:general_sleep"

\end_inset

.
\end_layout

\begin_layout Section
Sleep phase using DDC
\end_layout

\begin_layout Subsection
Gradient approximation using fixed basis functions on one variable
\end_layout

\begin_layout Standard
Since approximating an expectation is easy using DDC, we can choose a set
 of basis functions 
\begin_inset Formula $\left\{ \phi_{l}(z_{l})\right\} $
\end_inset

 that span domain where 
\begin_inset Formula $\frac{\partial}{d\theta}\log p_{\theta}(Z,X)$
\end_inset

 is largely supported, and apply the DDC trick in approximating an expectation.
 In the sleep phase, a recognition network was trained to produce the DDC
 encoding an approximate posterior 
\begin_inset Formula $q(Z|X)$
\end_inset

, e.g.
 for the 
\begin_inset Formula $l$
\end_inset

'th layer
\begin_inset Formula $\E_{q(Z|X)}\left[\phi_{l}(z_{l})\right]$
\end_inset

.
 At the bottom layer 
\begin_inset Formula $L$
\end_inset

 we can find these means by regression from samples of 
\begin_inset Formula $x$
\end_inset

 to feature function evaluations of samples of 
\begin_inset Formula $z_{l-1}$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
W_{L}=\arg\min_{W}\sum_{s}\|W\phi_{L}(X_{L}^{(s)})-\phi_{L-1}(Z_{L-1}^{(s)})\|_{2}^{2}
\]

\end_inset

Given sufficient training examples, minimising the squared loss will make
 
\begin_inset Formula $W_{L}\phi_{L}(x_{L})$
\end_inset

 approach the posterior mean 
\begin_inset Formula 
\[
W_{L}\phi_{L}(X_{L})\approx\E_{q_{L}(Z_{L-1}|X)}\left[\phi_{L-1}(Z_{L-1})\right]
\]

\end_inset

This regression is performed between each two adjacent layers.
 The mean representations of higher layers from samples can be found recursively
 by noticing the Markov structure of the graph
\begin_inset Formula 
\[
\E_{p(Z_{l-1}|X)}\left[\phi_{l-1}\right]=\E_{p_{l}(Z_{l}|X)}\left[\E_{p(Z_{l-1}|Z_{l})}\left[\phi_{l-1}\right]\right]\approx\E_{q(Z_{l}|X)}\left[\E_{q(Z_{l-1}|Z_{l})}\left[\phi_{l-1}\right]\right]=\E_{q(Z_{l}|X)}\left[W_{l}\phi_{l}\right]=W_{l}\E_{q(Z_{l}|X)}\left[\phi_{l}\right]
\]

\end_inset

In addition, we also approximate the required gradients 
\begin_inset Formula $\frac{\partial}{d\theta}\log p_{\theta}(Z,X)$
\end_inset

 as expressed in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:general_wake"

\end_inset

 using the set of feature chosen.
 At the 
\begin_inset Formula $l$
\end_inset

'th layer, the required gradient 
\begin_inset Formula $\frac{\partial}{d\theta}\log p_{\theta,l}(Z_{l}|Z_{l-1})$
\end_inset

 is a function of two variables.
 On may approximate the it by a feature defined on both 
\begin_inset Formula $Z_{l}$
\end_inset

 and 
\begin_inset Formula $Z_{l-1}$
\end_inset

, although for high dimensions of latent variables, learning the approximate
 over two variables may require a large number of features functions and
 samples.
 Nonetheless, it is still possible to learn the posterior mean of the gradient
 using a feature on just one variable using at least two measures both of
 which originates from the conditional independence structure:
\series bold

\begin_inset Formula 
\begin{equation}
\E_{q(Z_{l},Z_{l+1}|X)}\left[\frac{\partial}{d\theta}\log p_{l+1}(Z_{l+1}|Z_{l},\theta_{l+1})\right]=\E_{q(Z_{l+1}|X)}\left[\E_{p(Z_{l}|Z_{l+1})}\left[\frac{\partial}{d\theta}\log p_{l+1}(Z_{l+1}|Z_{l},\theta_{l+1})\right]\right]\label{eq:cond_indept_sample_sleep}
\end{equation}

\end_inset


\series default
the inner expectation can be approximated using samples of 
\begin_inset Formula $Z_{l}$
\end_inset

 from the sleep phase.
 Specifically, we can replace the inner expectation with a sample of 
\begin_inset Formula $z_{l}$
\end_inset

 that is jointly drawn with 
\begin_inset Formula $z_{l+1}$
\end_inset

 which is also used to evaluate feature 
\begin_inset Formula $\phi_{l+1}(z_{l+1})$
\end_inset

.
 Given sufficiently many joint samples, the joint expectation over 
\begin_inset Formula $q(Z_{l},Z_{l+1}|X)$
\end_inset

 can be well approximated using the basis functions.
\end_layout

\begin_layout Paragraph
Regression to log gradients (logp)
\end_layout

\begin_layout Standard
In the general case where we only require the assumptions stated above,
 the expectation in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:cond_indept_sample_sleep"

\end_inset

 can be learned using sleep samples by learning a linear regression with
 weights 
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\alpha_{0}:\phi_{0}(z_{0}) & \rightarrow\frac{\partial}{d\theta}\log p_{0}(z_{0},\theta_{0})\\
\alpha_{l+1}:\phi_{l+1}(z_{l+1}) & \rightarrow\frac{\partial}{d\theta}\log p_{l+1}(z_{l+1}|z_{l}^{(s)},\theta_{l+1}),l\in[0,\dots L-1]
\end{align*}

\end_inset

where 
\begin_inset Formula $z_{l-1}^{(s)}$
\end_inset

 denotes samples of from the generative model.
 
\end_layout

\begin_layout Paragraph
Regression for exponential family (exp-fam)
\end_layout

\begin_layout Standard
If, in addition to the two assumptions above, we also assume that the log
 densities are in the exponentail family, then the log gradients can be
 broken down into 1-2 separate regressions that are potentially easier to
 learn compared with the gradient of log density itself.
 Specifically, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left\langle \log p_{l+1}(z_{l+1}|z_{l},\theta_{l+1})\right\rangle _{q(z_{l},z_{l+1}|x)}=\left\langle \frac{\partial}{\partial\theta}g(\theta_{l+1},z_{l})S(z_{l+1})\right\rangle _{q(z_{l},z_{l+1}|x)}-\left\langle \frac{\partial}{\partial\theta}g(\theta_{l+1},z_{l})\mu_{z_{l+1}|z_{l}}\right\rangle _{q(z_{l}|x)}
\]

\end_inset

by using the same trick implied by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:cond_indept_sample_sleep"

\end_inset

 for the first term, the required regression becomes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\delta_{0}:\phi_{0}(z_{0})\rightarrow & S(z_{0})\\
\delta_{l}:\phi_{l}(z_{l})\rightarrow & \frac{\partial}{\partial\theta_{l}}g(\theta_{l},z_{l-1}^{(s)})S(z_{l}),l\in[1,\dots L-1]\\
\gamma_{l}:\phi_{l}(z_{l})\rightarrow & \frac{\partial}{\partial\theta_{l+1}}g(\theta_{l+1},z_{l})\mu_{z_{l+1}|z_{l}},l\in[0,\dots L-1]\\
\delta_{L-1}:\phi_{L-1}(z_{L-1})\rightarrow & \frac{\partial}{\partial\theta_{L}}g(\theta_{L},z_{L-1})
\end{align*}

\end_inset

where 
\begin_inset Formula $z_{l}^{(s)}$
\end_inset

 denotes samples of from the generative model.
 
\end_layout

\begin_layout Standard
The first method does not rely on the exponential faimily assumption, but
 the gradients may be difficult to learn and requires a log of samples for
 higher dimensions for 
\begin_inset Formula $z$
\end_inset

, since, intuitively there are roughly as exponentially many samples as
 dimensions of a higher layer variable that can generate the same sample
 in the lower layer.
 Exp-fam avoids this problem except for 
\begin_inset Formula $\left\langle \frac{\partial}{\partial\theta}g(\theta_{l},z_{l-1})S(z_{l})\right\rangle _{q(z_{l-1},z_{l}|x)}$
\end_inset

, but if 
\begin_inset Formula $\frac{\partial}{\partial\theta}g(\theta_{l},z_{l-1})S(z_{l})$
\end_inset

 is smooth, such as for the sigmoid belief network where this gradient is
 bilinear in 
\begin_inset Formula $z_{l-1}$
\end_inset

 and 
\begin_inset Formula $z_{l}$
\end_inset

, the regression may be easier to learn than the full log pdf.
 The problem with exp-fam is that the two terms in the log gradient expression
 are correlated, and taking the difference between two separate function
 approximations will exaggerate the errors in each of them.
 This correlation may harm gradient approximation in continuous distributions
 when the noise is low, or in other words, when 
\begin_inset Formula $S(z_{l})$
\end_inset

 evaluated at the samples are very close to its conditional mean.
 
\end_layout

\begin_layout Standard
One way to reduce the correlation could be to use separate sets of features
 or sleep samples for the two regressions (??)
\end_layout

\begin_layout Subsection
Using features on pairwise variables 
\begin_inset Formula $\phi(z_{l},z_{l+1})$
\end_inset

??
\end_layout

\begin_layout Section
Experiments
\end_layout

\begin_layout Standard
We compare the how well wake-sleep can recover the true generative parameters
 on toy datasets when using the two different types of gradient approximation
 schemes in the sleep phase.
 For each run, we randomly initialize some paraemters in the true model,
 and generate 1000 samples as training data; 100 logistic basis fucntions
 with random weights were used for the DDC representation of posteriors
 and kept fixed through out training.
 1000 sleep samples are drawn in the sleep phase of each iteration.
 Before learning, the parameters are initialized from the true values.
 Parameters are adjusted using vanilla gradient descent with step size 0.01.
\end_layout

\begin_layout Subsection
Factor analysis
\end_layout

\begin_layout Standard
Here, the true model is as follows
\begin_inset Formula 
\begin{align*}
z_{0}\sim & \cN(0,1.0\bI)\\
x|z_{0}\sim & N(Az_{0},\Psi)
\end{align*}

\end_inset

where A is an identity plus a square matrix with entries drawn from 
\begin_inset Formula $\cN(0,0.3^{2})$
\end_inset

 and 
\begin_inset Formula $\Psi$
\end_inset

 is diagonal with values 
\begin_inset Formula $0.1^{2}$
\end_inset

 and 
\begin_inset Formula $0.2^{2}$
\end_inset

.
 Distribution on 
\begin_inset Formula $z_{0}$
\end_inset

 is fixed throughout learning.
 For this particular case, the posterior is tractable, so we can compute
 the analytical gradients and compare whether the gradients approximated
 using the DDC sleep phase methods.
 We initialised 1000 models with different parameters.
 First, we compare how well the two DDC sleep phases can approximate the
 gradients in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:general_wake"

\end_inset

.
 1000 data points are drawn and the true analytical gradients and DDC approximat
e gradients using exp and logp were computed under these points.
 The 2-norm squared of the difference between approxiamte and true gradients
 are taken for each point, which are then averaged to produce a number that
 summarises how far away, on average, the approxiate gradient is from the
 true gradients under a particular model.
 The distribution of this difference under 100 models are shown in Figure
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:FA_errors"

\end_inset

 (left).
 It is clear that the gradients under logp has much smaller error.
 
\end_layout

\begin_layout Standard
Although exp performs worse gradients evaluated at each individual point,
 but if averaged across many training data, would the resulting parameter
 update (under unit step size) be smaller? Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:FA_errors"

\end_inset

 (middle) shows the distribution of the MSD after averaging the gradients
 across all 1000 data points.
 Again, logp has significantly smaller MSD than exp.
 
\end_layout

\begin_layout Standard
We then run the full wake-sleep algorithm using the two sleep phases for
 100 steps.
 We then compared samples drawn from the learned model to the true model.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:FA_errors"

\end_inset

 (right) shows the distribution of MMD (Gaussian kernel with median heuristic)
 from samples drawn from models learned with the two sleep phases to the
 samples drawn from the true model.
 Although the difference between exp and logp is not as noticeable as the
 gradient errors, logp outperforms exp in general.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center

\emph on
\begin_inset Graphics
	filename figs/FA_errors.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Comparing the performance of exp and dlogp on learning factor analysis models.
 
\series bold
Left
\series default
, The gradients at each data point under dlogp are significantly closer
 to the true values than those obtained using exp .
 
\series bold
Middle
\series default
, the true and DDC gradients are first averaged over the 1000 data points
 before taking the mean squared difference, thereby giving the difference
 in actual parameter updates for unit step size evaluated on all the 1000
 data points.
 The parameter updates are significantly closer to the true value using
 dlogp.
 
\series bold
Right
\series default
, MMD of 2000 samples drawn from the learned model after 100 SGD steps.
 Samples drawn from models trained using logp is significantly closer to
 real samples.
\begin_inset CommandInset label
LatexCommand label
name "fig:FA_errors"

\end_inset

.
 p-values are MannMann–Whitney U test.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Sparse distributions
\end_layout

\begin_layout Standard
Here, the training data are drawn from the following generative model
\begin_inset Formula 
\begin{align*}
z_{0}\sim & 0.5\cN(-3,0.1^{2})+0.5\cN(3,0.1^{2})\\
z_{1}|z_{0}\sim & Laplace(0,softplus(Bx_{0}))\\
x|z_{1}\sim & \cN(Az_{1},\Psi)
\end{align*}

\end_inset

where 
\begin_inset Formula $B$
\end_inset

 is 2-by-1 and 
\begin_inset Formula $A$
\end_inset

 is 2-by-2, both with entries drawn from a standard Gaussian.
 
\begin_inset Formula $\Psi$
\end_inset

 is 
\begin_inset Formula $0.5I$
\end_inset

.
 Thoughout learning, distribution of 
\begin_inset Formula $z_{0}$
\end_inset

 was kept fixed.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sparse_samples"

\end_inset

 shows the samples drawn from the learned models trained using exp-fam (a)
 and logp (b).
 Using exp-fam as sleep phase produced more mis-match in both 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $z_{1}$
\end_inset

 compared with logp.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sparse_sigmoid_mmd"

\end_inset

(a) shows the MMDs obtained from training 30 different models, and clearly
 logp outperforms exp-fam.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/ddc_exp_sparse.jpg
	lyxscale 30
	width 100text%
	BoundingBox 0bp 0bp 3000bp 670bp
	clip

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
exp-fam
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename figs/ddc_logp_sparse.jpg
	lyxscale 30
	width 100text%
	BoundingBox 0bp 0bp 3000bp 670bp
	clip

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
logp
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Samples drawn from the true sparse distributions (blue) and from the model
 after running DDC learning for 100 iterations with (a) exp-fam and (b)
 logp sleep phase (orange).
\begin_inset CommandInset label
LatexCommand label
name "fig:sparse_samples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Sigmoid belief networks (SBN)
\end_layout

\begin_layout Standard
Here, the training data are drawn from the following generative model
\begin_inset Formula 
\begin{align*}
z_{0,i}\sim & Bern(\sigma(W_{0,i}))\\
z_{1,i}|z_{0}\sim & Bern(\sigma(W_{1,i}z_{0}))\\
x_{i}|z_{1}\sim & Bern(\sigma(W_{2,i}z_{1}))
\end{align*}

\end_inset

where each variable is 50 dimensions and
\begin_inset Formula $W$
\end_inset

's are vectors with entries drawn from a standard Gaussian.
 Learning was simulated on 30 set of parameters and the sample MMDs are
 shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:sparse_sigmoid_mmd"

\end_inset

(b).
 There were no significant differences between the MMDs from exp or logp,
 suggesting that, in this case, these two methods are comparable.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "40text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figs/sparse_errors.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Sparse distributions
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "40text%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename figs/binary_errors.pdf
	width 100text%

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SBN
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MMD of samples drawn from the model trained using exp-fam (blue) and logp
 (orange) as sleep phases.
 30 models are simulated for the sparse distributions (a) and SBN (b).
\begin_inset CommandInset label
LatexCommand label
name "fig:sparse_sigmoid_mmd"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
\noindent
Message passing
\end_layout

\begin_layout Standard
Usually, message passing is a series of operations over obstract functions.
 It is unclear how the brain would perform these operations, and DDC is
 a proposal of how the brain might implement message passing using vector
 representations putatively encoded in a neural population.
\end_layout

\begin_layout Standard
We review some key ideas that is relevant but not necessarilty related directly
 to the final message passing scheme.
 
\end_layout

\begin_layout Subsection
Kernel belief propagation
\end_layout

\begin_layout Standard
Consider a graph 
\begin_inset Formula $\mathcal{G}$
\end_inset

 of random variables 
\begin_inset Formula $\mathcal{X}=\{X_{a}\}$
\end_inset

 indexed by letters.
 The graph itself, without conditioning on any observations, implies a joint
 distribution 
\begin_inset Formula $P*(\mathcal{X})$
\end_inset

 over all variables.
 For simplicity, let us consider a problem of computing a conditional distributi
on 
\begin_inset Formula $P^{c}(X_{a})=P(x_{a}|X_{b})$
\end_inset

, and we would like to solve this problem by belief propagation in terms
 of RKHS functions.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "SonGreBic2011"

\end_inset

, the derivation of belief propagation follows from the results on reparameteris
ed message which is defined as 
\begin_inset Formula 
\begin{equation}
m_{a\to b}(X_{b})=\frac{m_{a\to b}^{c}(X_{b})}{m_{a\to b}^{*}(X_{b})}\label{eq:normalised_message}
\end{equation}

\end_inset

where 
\begin_inset Formula $m_{a\to b}^{c}(X_{b})$
\end_inset

 is the usual BP message from node 
\begin_inset Formula $a$
\end_inset

 to node 
\begin_inset Formula $b$
\end_inset

 
\series bold
conditioned on observed data
\series default
, and 
\begin_inset Formula $m_{a\to b}^{*}(X_{b})$
\end_inset

 is the usual BP message from node 
\begin_inset Formula $a$
\end_inset

 to node 
\begin_inset Formula $b$
\end_inset


\series bold
 without observing any data
\series default
.
 There are two consequences of this formulation.
\end_layout

\begin_layout Enumerate
Computing the marginal distribution on a node requires not only the messages
 but also the unconditional marginal distribution of the variable
\begin_inset Formula 
\[
P^{c}(X_{a})=\prod_{i\in ne(a)}m_{i\to b}^{c}(X_{a})=\prod_{i\in ne(a)}\left[m_{a\to b}^{*}(X_{b})m_{i\to b}(X_{a})\right]=P^{*}(X_{a})\prod_{i\in ne(a)}m_{i\to b}(X_{a})
\]

\end_inset


\end_layout

\begin_layout Enumerate
Message propagation is computed using not only the messages but also the
 unconditioned marginal distribution, e.g.
 for a message from 
\begin_inset Formula $a$
\end_inset

 sent to 
\begin_inset Formula $b$
\end_inset


\begin_inset Formula 
\begin{align*}
m_{a\to b}(X_{b}) & =\E_{p*(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}m_{u\to a}(X_{a})\right]
\end{align*}

\end_inset

where 
\begin_inset Formula $p*(X_{a}|X_{b})$
\end_inset

 can be estimated from data.
 
\end_layout

\begin_layout Standard
The second result follows from the following algebra.
 First, note that the following unobserved conditional distribution can
 be written as the potentials on the original graph
\begin_inset Formula 
\begin{align}
p^{*}(X_{a}|X_{b}) & =\frac{p^{*}(X_{a},X_{b})}{p^{*}(X_{b})}\nonumber \\
 & =\frac{\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})\prod_{u\in ne(b)\neg a}m_{u\to b}^{*}(X_{b})}{m_{a\to b}^{*}(X_{b})\prod_{u\in ne(b)\neg a}m_{u\to b}^{*}(X_{b})}\nonumber \\
 & =\frac{\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{m_{a\to b}^{*}(X_{b})}\label{eq:unboserveed_conditional}
\end{align}

\end_inset

The original usual belief update is written in terms of messages and potentials
 as 
\begin_inset Formula 
\begin{align*}
m_{a\to b}^{c}(X_{b}) & =\int\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{c}(X_{a})dX_{a}
\end{align*}

\end_inset

Now divide both sizes by the unobserved message 
\begin_inset Formula $m_{a\to b}^{*}(X_{b})$
\end_inset

, and multiply inside the integral the constant 
\begin_inset Formula $\frac{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}$
\end_inset


\begin_inset Formula 
\begin{align*}
\frac{m_{a\to b}^{c}(X_{b})}{m_{a\to b}^{*}(X_{b})} & =\frac{1}{m_{a\to b}^{*}(X_{b})}\int\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{c}(X_{a})\frac{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}dX_{a}\\
 & =\int\frac{\Phi(X_{a,}X_{b})\Phi(X_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(X_{a})}{m_{a\to b}^{*}(X_{b})}\prod_{u\in ne(a)\neg b}\frac{m_{u\to a}^{c}(X_{a})}{m_{u\to a}^{*}(X_{a})}dX_{a}\\
m_{a\to b}(X_{b}) & =\E_{p(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}m_{u\to a}(X_{a})\right]
\end{align*}

\end_inset

where the last equality holds following Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:unboserveed_conditional"

\end_inset

).
 This is belief propagation under the message definition in Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:normalised_message"

\end_inset

).
\end_layout

\begin_layout Standard
This expectation under the unobserved data needs to be computed for the
 product of messages.
 If we assume that 
\begin_inset Formula $m_{u\to a}(X_{a})=\text{\left\langle m_{u\to a},\ \bphi(x_{a})\right\rangle }_{\cH}$
\end_inset

 where 
\begin_inset Formula $\bphi(X_{a})$
\end_inset

 is a vector in space 
\begin_inset Formula $\cH$
\end_inset

, and similarly that 
\begin_inset Formula $m_{a\to b}(X_{b})=\left\langle m_{u\to b},\ \bphi(X_{b})\right\rangle _{\cF}$
\end_inset

 where 
\begin_inset Formula $\bphi(X_{a})$
\end_inset

 is a vector in space 
\begin_inset Formula $\cF$
\end_inset

, then we can translate the product above into an inner product
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
m_{a\to b}(X_{b}) & =\E_{p(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}m_{u\to a}(X_{a})\right]\nonumber \\
 & =\E_{p(X_{a}|X_{b})}\left[\prod_{u\in ne(a)\neg b}\left\langle m_{u\to a},\ \bphi(X_{a})\right\rangle _{\cH}\right]\nonumber \\
 & =\E_{p(X_{a}|X_{b})}\left[\left\langle \underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \underset{deg(a)-1}{\bigotimes}\bphi(X_{a})\right\rangle _{\cH^{\otimes}}\right]\nonumber \\
 & =\left\langle \underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \E_{p(X_{a}|X_{b})}\left[\underset{deg(a)-1}{\bigotimes}\bphi(X_{a})\right]\right\rangle _{\cH^{\otimes}}\nonumber \\
 & \stackrel{(1)}{=}\left\langle \underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \mathcal{W}_{X_{a}^{\otimes}|X_{b}}\bpsi(X_{b})\right\rangle _{\text{\cH^{\otimes}}}\nonumber \\
 & =\left\langle \mathcal{W}_{X_{a}^{\otimes}|X_{b}}^{\dagger}\underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a},\ \bpsi(X_{b})\right\rangle _{\text{\cF}}\nonumber \\
m_{a\to b} & =\mathcal{W}_{X_{a}^{\otimes}|X_{b}}^{\dagger}\underset{u\in ne(b)\neg a}{\bigotimes}m_{u\to a}\label{eq:kernel_BP}
\end{align}

\end_inset

where in (1) we assume that the conditional expectation 
\begin_inset Formula $\E_{p(X_{a}|x_{b})}\left[\large{\text{\otimes}}^{deg(b)-1}\bphi(X_{a})\right]$
\end_inset

, which is a function of 
\begin_inset Formula $X_{b}$
\end_inset

, is in the space where 
\begin_inset Formula $\bphi(X_{b})$
\end_inset

 is defined.
 
\end_layout

\begin_layout Standard
In particular, the message from an observed node will produce the following
 message
\begin_inset Formula 
\begin{align*}
\frac{m^{c}(X_{b})}{m^{*}(X_{b})} & =\frac{1}{m^{*}(x_{b})}\Phi(x_{b,}y_{b})\Phi(y_{b})\frac{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}\\
 & =\frac{\Phi(x_{b,}y_{b})\Phi(y_{b})\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}{m^{*}(x_{b})}\frac{1}{\prod_{u\in ne(a)\neg b}m_{u\to a}^{*}(y_{b})}\\
 & \propto p^{*}(x_{a}|x_{b})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Under this message passing scheme, computing the marginal probability under
 the observed condition requires the knowledge of the corresponding unconditione
d marginal distribution, which is in practise estimated by nonparametric
 methods.
 While appealing for kernelised inference where the model is fully observed,
 the above message passing scheme cannot be directly applied to latent variable
 models, such as Marko state-space model where the true generative parameters
 are unknown or misspecified, rendering 
\begin_inset Formula $P*(\mathcal{X})$
\end_inset

 unavailable.
 
\end_layout

\begin_layout Standard
In particular, we note that message passing is performed by an inner product
 between parameters 
\begin_inset Formula $\mathcal{W}_{X_{a}^{\otimes}|x_{b}}^{\dagger}$
\end_inset

 and the tensor product of incomming messages.
 In the section below, message passing using the mean representations of
 variables also share this structure.
\end_layout

\begin_layout Subsection
Kernel Bayes rule
\end_layout

\begin_layout Standard
Kernel Bayes rule 
\begin_inset CommandInset citation
LatexCommand cite
key "SmoGreSon2007"

\end_inset

 allows message propagation while taking into account normalisation based
 on conditional operators.
 We begin by reviewing the basic kernel probability computations.
 
\end_layout

\begin_layout Subsubsection
Mean representation of distributions
\end_layout

\begin_layout Paragraph
Mean embedding
\end_layout

\begin_layout Standard
Define the the mean embedding for random variable 
\begin_inset Formula $X$
\end_inset

 to be
\begin_inset Formula $\bmu_{X}=\E_{p(X)}\left[\phi(X)\right]$
\end_inset

, where 
\begin_inset Formula $\phi(X)\in\cH$
\end_inset

 is a vector of features, and similarly 
\begin_inset Formula $\bmu_{Y}=\E_{p(Y)}\left[\psi(X)\right]$
\end_inset

 for
\begin_inset Formula $\psi(X)\in\cF$
\end_inset

.
 The mean embedding is useful as it allows evaluation of expectations through
 a linear a linear transformation through the mean embeddings.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\E_{p(X)}\left[f(X)\right]=\E_{p(X)}\left[\left\langle f,\phi(X)\right\rangle \right]=\left\langle f,\E_{p(X)}\left[\phi(X)\right]\right\rangle =\left\langle f,\bmu_{X}\right\rangle \quad\forall f\in\cH,g\in\cF
\]

\end_inset


\end_layout

\begin_layout Standard
Similar to the expectations above, one can also define a mean representation
 for the joint distribution 
\begin_inset Formula $p(X,Y)$
\end_inset

 as the mean over the outer product of the feature maps
\begin_inset Formula 
\[
\cC_{XY}=\E_{p(X,Y)}\left[\bphi(x)\otimes\bpsi(y)\right]
\]

\end_inset


\end_layout

\begin_layout Standard
with the property that any uncentered covariances between RKHS functions
 
\begin_inset Formula $f(x)$
\end_inset

 and 
\begin_inset Formula $g(y)$
\end_inset

 can be evaluated as 
\begin_inset Formula 
\[
\E_{p(X,Y)}\left[f(x)g(y)\right]=\left\langle f\otimes g,\E_{p(X,Y)}\left[\bphi(x)\otimes\bpsi(y)\right]\right\rangle _{\cH\otimes\cF}=\left\langle f\otimes g,C_{XY}\right\rangle _{\cH\otimes\cF}=\left\langle f,C_{XY}g\right\rangle _{H}
\]

\end_inset

where the last equality follows from the analogy with linear algebra 
\begin_inset Formula $\Tr(\y\x^{\intercal}\mathbf{A})=\x^{\intercal}\mathbf{A}\y$
\end_inset

 and 
\begin_inset Formula $C_{XY}$
\end_inset

 is viewed as an operator 
\begin_inset Formula $\cF\to\cH$
\end_inset

.
\end_layout

\begin_layout Paragraph
Conditional mean embeddings and kernel Bayes rule (KBR)
\end_layout

\begin_layout Standard
The conditional distribution 
\begin_inset Formula $p(Y|X=x)$
\end_inset

 can also be defined in a similar way.
 Denote by 
\begin_inset Formula $x$
\end_inset

 the value take by 
\begin_inset Formula $X$
\end_inset


\begin_inset Formula 
\[
\bmu_{Y|x}=\E_{p(Y|X=x)}\left[\bpsi(y)|X=x\right]\in\cF
\]

\end_inset


\end_layout

\begin_layout Standard
This mean maps a function 
\begin_inset Formula $f\in\cH$
\end_inset

 to its conditional expectation.
 
\begin_inset Formula 
\[
\E_{p(Y|X=x)}\left[g(y)\right]=\left\langle g,\E_{p(Y|X=x)}\left[\bpsi(y)\right]\right\rangle =\left\langle g,\bmu_{Y|x}\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
In contrast to the 
\begin_inset Formula $\bmu_{X}$
\end_inset

 and 
\begin_inset Formula $\cC_{XY}$
\end_inset

, the conditional embedding 
\begin_inset Formula $\bmu_{Y|x}$
\end_inset

 is an element in 
\begin_inset Formula $\cF$
\end_inset

 as a function of 
\begin_inset Formula $x$
\end_inset

.
 It would be convenient to obtain a operator that achieves this mapping
 without using the definition above.
 Indeed, it can be shown 
\begin_inset CommandInset citation
LatexCommand cite
key "FukKenFra2004,SonHuaJon2009"

\end_inset

 that such an operator 
\begin_inset Formula $\cC_{Y|X}:\cH\to\cF$
\end_inset

 can be written as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\cC_{Y|X}=\cC_{YX}\cC_{XX}^{-1}\label{eq:conditional_operator}
\end{equation}

\end_inset

and when conditioned on a particular value of 
\begin_inset Formula $x$
\end_inset

 
\begin_inset Formula 
\begin{equation}
\bmu_{Y|x}=\cC_{Y|X}\bphi(x)\label{eq:conditional_mean}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
One assumption is that 
\begin_inset Formula $\E_{Y|X}\left[g(y)|X=\cdot\right]\in\cH$
\end_inset

 for all 
\begin_inset Formula $g\in\cF$
\end_inset

 without conditioninig on any particular 
\begin_inset Formula $x$
\end_inset

, so that 
\begin_inset Formula $\left\langle \E_{Y|X}\left[g(y)|X=\cdot\right],\phi(x)\right\rangle =\E_{Y|X}\left[g(y)|X=x\right]$
\end_inset

.
 To briefly show why 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:conditional_mean"

\end_inset

 holds, we need to show that 
\begin_inset Formula $\E_{Y|x}\left[g(y)|X=x\right]=\left\langle g,\cC_{YX}\cC_{XX}^{-1}\bphi(x)\right\rangle $
\end_inset

.
 For any 
\begin_inset Formula $f\in\cH$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\left\langle f,\cC_{XX}\E_{p(Y|X)}\left[g(Y)|X=\cdot\right]\right\rangle _{\cH} & =\left\langle f,\E_{p(X)}\left[\bphi(X)\otimes\bphi(X)\right]\E_{p(Y|X)}\left[g(Y)|X=\cdot\right]\right\rangle _{\cF}\\
 & \stackrel{(1)}{=}\E_{p(X)}\left[f(X)\left\langle \E_{Y|X}\left[g(y)|X=\cdot\right],\phi(X)\right\rangle _{\cF}\right]\\
 & =\E_{p(X)}\left[f(X)\E_{p(Y|X)}\left[g(Y)|X\right]\right]\\
 & =\E_{p(X,Y)}[f(X)\otimes g(y)]\\
 & =\left\langle f,C_{XY}g\right\rangle _{\cH}\\
\cC_{XX}\E_{p(Y|X)}\left[g(Y)|X=\cdot\right] & =C_{XY}g
\end{align*}

\end_inset

where at step (1) we used the assumption 
\begin_inset Formula $\E_{Y|X}\left[g(y)|X=\cdot\right]\in\cH$
\end_inset

 for all 
\begin_inset Formula $g\in\cF$
\end_inset

.
 By taking the tranpose of 
\begin_inset Formula $\cC_{XX}$
\end_inset

 and conditioning on a particular value of 
\begin_inset Formula $X=x$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\E_{Y|X}\left[g(y)|X=x\right] & = & \left\langle \cC_{XX}^{-1}\cC_{XY}g,\bphi(x)\right\rangle _{\cH}\\
 & = & \left\langle g,\cC_{YX}\cC_{XX}^{-1}\bphi(x)\right\rangle _{\cF}\\
\E_{Y|X}\left[\bpsi(y)|X=x\right] & = & \cC_{YX}\cC_{XX}^{-1}\bphi(x)
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Standard
This means that the Bayes rule 
\begin_inset Formula $p(Y|X=x)=\frac{P(Y,X=x)}{P(X=x)}$
\end_inset

 can be translated to relationships between the corresponding kernel mean
 embeddings as
\begin_inset Formula 
\begin{equation}
\bmu_{Y|x}=C_{YX}\cC_{XX}^{-1}\bphi(x)\label{eq:KBR}
\end{equation}

\end_inset

which is known as the kernel Bayes rule.
\end_layout

\begin_layout Paragraph
Kernel sum rule (KSR)
\end_layout

\begin_layout Standard
Following similar algebra on can show that that the sum rule 
\begin_inset Formula $p(X)=\int dYp(X|Y)p(Y)$
\end_inset

 in terms of mean embeddings is 
\begin_inset Formula 
\begin{equation}
\bmu_{Y}=\cC_{Y|X}\bmu_{X}\label{eq:KSM}
\end{equation}

\end_inset

 since
\begin_inset Formula 
\[
\bmu_{Y}=\E_{p(Y)}\left[\bphi(Y)\right]=\E_{p(X)}\E_{p(Y|X)}\left[\bpsi(y)|X=x\right]=\E_{p(X)}\left[\cC_{Y|X}\bphi(x)\right]=\cC_{Y|X}\E_{p(X)}\left[\bphi(x)\right]=\cC_{Y|X}\bmu_{X}
\]

\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Subsubsection
Propagation on a simple chain
\end_layout

\begin_layout Standard
In belief propagation, we wish to successively condition on observations
 and marginalise variables along a path in the graph.
 Consider first a simple Bayesian network of 
\begin_inset Formula $(D=d)\to X\to Y\to(Z=z)$
\end_inset

 where 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

 are observed to be 
\begin_inset Formula $d$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

, respectively .
 Consider the situation where the mean embedding of 
\begin_inset Formula $\bmu_{X|D=d}$
\end_inset

 is computed, and we wish to propagated this mean to obtain the mean embedding
 for 
\begin_inset Formula $Y$
\end_inset

.
 To simplifiy notation, define a common feature map 
\begin_inset Formula $\bphi(\cdot)\in\cH$
\end_inset

 for all variables.
 Repeated use of KBR gives
\begin_inset Formula 
\begin{align*}
\bmu_{Y|Z=z,D=d} & =C_{YZ|d}C_{ZZ|d}^{-1}\bphi(z)\\
 & =\left[C_{YZD}C_{DD}^{-1}\bphi(d)\right]\left[C_{ZZD}C_{DD}^{-1}\bphi(d)\right]^{-1}\bphi(z)
\end{align*}

\end_inset

By definition, 
\begin_inset Formula $\bmu_{X|d}=C_{XD}C_{DD}^{-1}\bphi(d)\implies C_{DD}^{-1}\bphi(d)=C_{XD}^{\dagger}\bmu_{X|d}$
\end_inset

, substituting this into the above gives
\begin_inset Formula 
\[
\bmu_{Y|Z=z,D=d}=\left[C_{YZD}C_{XD}^{\dagger}\bmu_{X|d}\right]\left[C_{ZZD}C_{XD}^{\dagger}\bmu_{X|d}\right]^{-1}\bphi(z)
\]

\end_inset


\end_layout

\begin_layout Standard
This update rule is taken by in the kernel predictive state representation
 of latent chain models 
\begin_inset CommandInset citation
LatexCommand cite
key "BooByrGeo2013"

\end_inset

 and also motivated the architecture of the predictive state recurrent neural
 network 
\begin_inset CommandInset citation
LatexCommand cite
key "DowCarAhm2017"

\end_inset

.
 While being able to take into account all past data on potentially non-Markovia
n latent structure, one needs design features for all previous data history
 
\begin_inset Formula $\psi(D)$
\end_inset

 if 
\begin_inset Formula $D$
\end_inset

 involves previous observed nodes.
 
\end_layout

\begin_layout Standard
Another way of propagating messages leavages the conditional independence
 relationships within the chain 
\begin_inset Formula $p(Y,Z|D)=\int dDp(Y,Z|X)p(X|D)$
\end_inset

, which, using kernel sum rule (Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:KSM"

\end_inset

), can be written in mean embeddings as 
\begin_inset Formula 
\[
C_{YZ|d}=C_{YZ|X}\bmu_{X|d}
\]

\end_inset

and likewise
\begin_inset Formula 
\[
C_{ZZ|d}=C_{ZZ|X}\bmu_{X|d}
\]

\end_inset

And, therefore
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\bmu_{Y|Z=z,D=d} & =\left[C_{YZ|X}\bmu_{X|d}\right]\left[C_{ZZ|X}\bmu_{X|d}\right]^{-1}\bphi(z)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
More generally, using the common notation as above, we can compute messages
 in the following way.
 Note that the conditional independence of the graph suggests that 
\begin_inset Formula $p(X_{a}|X_{u},d_{u})=p(X_{a}|X_{u})$
\end_inset

.
 To shorten notation, denote by 
\begin_inset Formula $\{X_{u}\}$
\end_inset

 the set of latent variables in the upstream neighbourhood of 
\begin_inset Formula $X_{a}$
\end_inset


\begin_inset Formula 
\begin{align*}
\bmu_{a} & =C_{X_{a}Y_{a}|\left\{ d_{u}\right\} }C_{Y_{a}Y_{a}|d_{u}}^{-1}\nu(y_{a})\\
 & =\left[C_{X_{a}Y_{a}|\left\{ X_{u}\right\} ,\left\{ d_{u}\right\} }C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]\left[C_{X_{a}X_{a}|\left\{ X_{u}\right\} ,\left\{ d_{u}\right\} }C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]^{-1}\nu(y_{a})\\
 & =\left[C_{X_{a}Y_{a}\{X_{u}\}}C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]\left[C_{Y_{a}Y_{a}\{X_{u}\}}C_{\left\{ X_{u}\right\} \left\{ X_{u}\right\} |\left\{ d_{u}\right\} }^{-1}\bmu_{\left\{ X_{u}\right\} }\right]^{-1}\nu(y_{a})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
However, in practice, due to the regularised inverse, propagating message
 along a large graph (e.g.
 a markov chain) results in accumulation of the errors in the mean embeddings.
 
\end_layout

\begin_layout Subsection
Message mean representation (MMR)
\end_layout

\begin_layout Standard
Here we derive a new message passing scheme based on mean representation
 of unnormalized likelihoods or messages.
 The key idea is that we which to use a finite set of basis function which
 can be used to approximate certain integrals of potentials in the graph.
 Using a infinitely large set of basis functions, such as the features implied
 by universal kernels, then by universality this set of feature functions
 can approximate any bounded continuous functions, which is usually the
 case for many distributions and messages.
 
\end_layout

\begin_layout Standard
We begin by stating the above assumptions.
 As above, define vector features and its corresponding feature space as
 
\begin_inset Formula $\bphi(X_{a})$
\end_inset

 and 
\begin_inset Formula $\bpsi_{u}(X_{u})$
\end_inset

 for 
\begin_inset Formula $u\in ne(a)$
\end_inset

; Additionaly, we make explicit the observed data attached to variable 
\begin_inset Formula $X_{a}$
\end_inset

 to be 
\begin_inset Formula $Y_{a}=y_{a}$
\end_inset

 and define its feature represenation as
\begin_inset Formula $\bnu(y_{a})$
\end_inset

.
 We assume that we can approximate the following
\begin_inset Formula 
\begin{align}
\int dX_{a}\bphi(X_{a})\prod_{u\in ne(a)}\Phi_{a,u}(X_{a,}X_{u},y_{a})\Phi_{u}(X_{u},y_{a}) & \approx\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\bpsi_{u}(X_{u})\right\rangle \label{eq:approx_expectation}\\
 & =\sum_{i_{1},\dots,i_{deg(a)},k}W_{i_{1},\dots,i_{deg(a)}}^{(a)}\nu_{k}(y_{a})\prod_{u\in ne(a)}\psi_{u,i_{u}}(X_{u})~\forall a\in\cG
\end{align}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $W_{i_{1},\dots,i_{deg(a)}}^{(a)}$
\end_inset

 are parameters and 
\begin_inset Formula $deg(a)$
\end_inset

 means the number of incoming mean representations.
 Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_expectation"

\end_inset

 states that the integral of the product of the relevant potentials over
 the variable we are computing the mean representation 
\begin_inset Formula $X_{a}$
\end_inset

 (similar to the out-going message from 
\begin_inset Formula $X_{a}$
\end_inset

) should be well approximatd by the set of basis function formed by the
 outer product of all the basis functions associated with the other incoming
 neighbouring variables 
\begin_inset Formula $X_{u}$
\end_inset

 and data 
\begin_inset Formula $y_{a}$
\end_inset

.
 This is analogous to the assumption made at in step (1) in deriving Equation
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kernel_BP"

\end_inset

).
\end_layout

\begin_layout Standard
Instead of messages, we consider (possibly unnormalised) expectations of
 nonlinear functions for variables 
\begin_inset Formula $\tilde{\bmu}_{u}:=\E_{\tilde{p}(X_{u}|d_{u})}\left[\bpsi(X_{u})\right]$
\end_inset

 where 
\begin_inset Formula $d_{u}$
\end_inset

 is the data conditioned on 
\begin_inset Formula $D_{u}$
\end_inset

 the upstream of 
\begin_inset Formula $u$
\end_inset

.
 We restrict 
\begin_inset Formula $u\in ne(a)=\{1,...,deg(a)\}$
\end_inset

 to be in the neighbours from which messages propagate from.
 
\begin_inset Formula $\tilde{\bmu}_{u}$
\end_inset

 is essentially the unnormalized mean representation of 
\begin_inset Formula $\tilde{p}(X_{u}|d_{u})$
\end_inset

.
 Given such previously computed means conditioned on the set of observations
 
\begin_inset Formula $\left\{ D_{u}=d_{n}\right\} _{u\in ne(a)}$
\end_inset

, we want to obtain the mean representation
\begin_inset Formula $\tilde{\bmu}_{a}:=\E_{\tilde{p}(X_{a}|\left\{ d_{u}\right\} _{u\in ne(a)},y_{d})}\left[\bphi(X_{u})\right]$
\end_inset

 given new data 
\begin_inset Formula $y_{a}$
\end_inset

 attached to the relevant potentials.
\begin_inset Formula 
\begin{align}
\tilde{\bmu}_{a} & =\int dX_{a}\bphi(X_{a})\tilde{p}(X_{a}|\left\{ d_{u}\right\} _{u\in ne(a)},X_{d})\nonumber \\
 & =\int dX_{a}\bphi(X_{a})\prod_{u\in ne(a)}\int dX_{u}\tilde{p}(X_{u}|d_{u})\Phi_{a,u}(X_{a,}X_{u},X_{d})\Phi_{u}(X_{u},y_{a})\nonumber \\
 & =\int\dots\int\left[\prod_{u\in ne(a)}dX_{u}\tilde{p}(X_{u}|d_{u})\right]\int dX_{a}\bphi(X_{a})\prod_{u\in ne(a)}\Phi_{a,u}(X_{a,}X_{u},y_{a})\Phi_{u}(X_{u},y_{a})\nonumber \\
 & \stackrel{(1)}{}\approx\int\dots\int\left[\prod_{u\in ne(a)}dX_{u}\tilde{p}(X_{u}|d_{u})\right]\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\bpsi_{u}(X_{u})\right\rangle \nonumber \\
 & =\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\left[\int dX_{u}\tilde{p}(X_{u}|d_{u})\bpsi_{u}(X_{u})\right]\right\rangle \nonumber \\
 & =\left\langle \mathcal{W},\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\tilde{\bmu}_{u}(X_{u})\right\rangle \label{eq:approx_dist_multilinear}
\end{align}

\end_inset

where in step (1) we used the assumption of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_expectation"

\end_inset

.
 It is worth noting that the means are unnormalized, which does not matter
 for inference.
 To normalize the means, one can use divisive normalisation.
 For example, to normalize 
\begin_inset Formula $\tilde{\bmu}$
\end_inset

, find a set of weights 
\begin_inset Formula $\mathbf{n}$
\end_inset

 to approximate the constant function with the basis function
\begin_inset Formula $\sum_{i}n_{i}\phi_{i}(X)=1$
\end_inset

, then the normalizer for 
\begin_inset Formula $\tilde{p}(X|D)$
\end_inset

 is 
\begin_inset Formula 
\begin{align*}
Z(D) & =\int\tilde{p}(X|D)dX\\
 & =\int\tilde{p}(X|D)\sum_{i}n_{i}\phi_{i}(X)dX\\
 & =\sum_{i}n_{i}\tilde{\mu}_{i}\\
 & =\mathbf{n}^{\intercal}\tilde{\bmu}
\end{align*}

\end_inset

and the normalized mean is 
\begin_inset Formula 
\begin{align}
\bmu & =\frac{\tilde{\bmu}}{\mathbf{n}^{\intercal}\tilde{\bmu}}\label{eq:approx_dist_normalize}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Although it is not easy to find the weigths 
\begin_inset Formula $\mathcal{W}$
\end_inset

 using traning samples when the messages involve a data observation (we
 illustrate the reason for a special case in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:ssm_case"

\end_inset

), this message passing scheme that combines 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_dist_multilinear"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_dist_normalize"

\end_inset

 motivates a neural network structure that combines inputs multilinearly
 in the feature space.
 Given a task and an error signal, we can hopefully obtain the weights using
 backpropagation.
\end_layout

\begin_layout Subsection
Distribution regression to distribution (DRD)
\end_layout

\begin_layout Standard
In the previous sections, we discussed ideas about how to perform that was
 trained using samples but was intended for use on mean representations
 during test time.
 Is it possible to train also using mean representations? We can treat the
 mean representation of upstream messages 
\begin_inset Formula $\left\{ \bmu_{u}(X_{u})\right\} {}_{u\in ne(a)}$
\end_inset

 as input vectors, and simply learn a function that map them, together with
 features of observed data 
\begin_inset Formula $\bnu(y_{a})$
\end_inset

, to the next mean embedding.
 
\begin_inset Formula 
\begin{equation}
\bmu_{a}=g\left(\left\{ \bmu_{u}(X_{u})\right\} {}_{u\in ne(a)},y_{a}\right)=\left\langle g,\bnu(y_{a})\otimes\bigotimes_{u\in ne(a)}\bmu_{u}(X_{u})\right\rangle \label{eq:distribution_regression}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We call this approach distribution regression.
 The function 
\begin_inset Formula $h(\cdot)$
\end_inset

 is assumed to be linear in feature space.
 It is also worth noting that 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:distribution_regression"

\end_inset

 is very similar to 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:kernel_BP"

\end_inset

 in that the forms of message passing are identical, even though the messages
 combined were likelihoods (normalized by the unconditioned message) for
 kernel BP and mean representations for distribution regression.
 Unlike kernel belief propagation where the operators 
\begin_inset Formula $\cC$
\end_inset

's are estimated from samples in a way that respects the true underlying
 relationships between the variables, i.e.
 using KSR to propagate message to another latent node followed KBR that
 takes the message from the observation attached to that node, the regression
 in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:distribution_regression"

\end_inset

) does not care about the actual relationships between the variables, and
 also obviates the need for the assumption in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:approx_expectation"

\end_inset

) as long as the function 
\begin_inset Formula $h(\cdot)$
\end_inset

 is able to discover the relationships between the input and output messages.
 A similar situation has been studied by Szabo et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "szabo2016learning"

\end_inset

 in the infinite dimensional feature case where the inputs are distributions
 and output is a scalar or label of interest.
 
\end_layout

\begin_layout Standard
To learn the function 
\begin_inset Formula $g\left(\cdot\right)$
\end_inset

 on a directed graph, one can simulate the model by initializing the prior
 nodes with samples whose feature vectors are averaged to create the prior
 mean representations.
 Conditioned on each set of prior samples, different sets of samples need
 to be drawn in order to create enough training mean representations for
 regresion.
 One can iteratively learn the regressor 
\begin_inset Formula $h(\cdot)$
\end_inset

 by training a regressor to take mean representations of the parent nodes
 to predict feature vectors of samples at the child nodes under the squared
 loss.
 The predicted feature vectors can then be used as the mean representation
 for the child nodes in the next regression.
 Note that the set of data points for regression are in fact derived from
 the many sets of samples drawn from the graph.
\end_layout

\begin_layout Standard
Learning 
\begin_inset Formula $g\left(\cdot\right)$
\end_inset

 on an undirected graph has not been considered yet, the difficulty being
 that the there is no known prior distributions to start the recursive trainng.
 
\end_layout

\begin_layout Section
Message passing for state-space models
\begin_inset CommandInset label
LatexCommand label
name "sec:ssm_case"

\end_inset


\end_layout

\begin_layout Standard
Consider the particular type of inference problem described by the following
 set of distributions describing a state-space model 
\begin_inset Formula $p(z_{1}),p(z_{t}|z_{t-1}),p(z_{t}|x_{t})$
\end_inset

 where 
\begin_inset Formula $z_{t}$
\end_inset

 is latent and 
\begin_inset Formula $x_{t}$
\end_inset

 are observed.
 
\begin_inset Formula 
\begin{align*}
p_{\theta_{0}}(z_{0}) & =\exp\left[g(\theta_{0})T(z_{1})-\Phi(g(\theta_{0}))\right]\\
p_{\theta_{f}}(z_{t}|z_{t-1}) & =\exp\left[g(z_{t-1},\theta_{f})T(z_{t})-\Phi(g(z_{t-1},\theta_{f}))\right]\\
p(x_{t}|z_{t}) & =\exp\left[g(z_{t-1},\theta_{g})T(x_{t})-\Phi(g(z_{t},\theta_{g}))\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Filtering 
\end_layout

\begin_layout Standard
We denote by 
\begin_inset Formula $\bmu_{Z_{t}|x_{1:t}}=\E_{p(Z_{t}|x_{1:t})}\left[\bpsi(Z_{t})\right]$
\end_inset

 the mean representation of posterior 
\begin_inset Formula $p(Z_{t}|x_{1:t})$
\end_inset

 and 
\begin_inset Formula $\bphi(x_{t})$
\end_inset

 the feature of observation.
 Following the message passing schemes described above, we have the following
 ways of performing the filtering operation.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\text{KBR:} &  & \bmu_{Z_{t}|x_{1:t}}= & \left[C_{Z_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]\left[C_{X_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]^{-1}\bphi(x_{t})\\
\text{DRD:} &  & \bmu_{Z_{t}|x_{1:t}}= & \left\langle \cW,\bmu{}_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\right\rangle \\
 &  & \cW:\text{ \ } & \bmu{}_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\xmapsto{x_{1:t},z_{t}}\bpsi(z_{t})\\
\text{MMR:} &  & \tilde{\bmu}{}_{Z_{t}|x_{1:t}}= & \left\langle \cU,\bmu{}_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\right\rangle \\
 &  & \bmu{}_{Z_{t}|x_{1:t}}= & \frac{\tilde{\bmu}{}_{Z_{t}|x_{1:t}}}{\mathbf{n}^{\intercal}\tilde{\bmu}{}_{Z_{t}|x_{1:t}}}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For KBR, the parameters 
\begin_inset Formula $C_{Y_{t}X_{t}|Y_{t-1}}$
\end_inset

 and 
\begin_inset Formula $C_{X_{t}X_{t}|Y_{t-1}}$
\end_inset

 are estimated from samples drawn from the stationary distribution.
 For DRD, the regressor 
\begin_inset Formula $h_{t}$
\end_inset

 is trained at each time step starting from an arbitrary prior until the
 chains reach stationarity.
 When the posteriors only weakly depends on the first observation 
\begin_inset Formula $\bmu_{Y_{t}|x_{1:t}}\approx\bmu_{Y_{t}|x_{t-\tau::t}}$
\end_inset

 for some 
\begin_inset Formula $0<\tau<t$
\end_inset

 and for all 
\begin_inset Formula $t$
\end_inset

, we can assume that 
\begin_inset Formula $h_{t}$
\end_inset

 is stationary and thus can be used at all future time steps.
 For MMR, however, the weights cannot be obtained directly.
 To see why, we write the filtering operation in the mean representation
 of messsages as
\begin_inset Formula 
\begin{align}
\tilde{\bmu}{}_{Z_{t}|x_{1:t}} & =\int dz_{t}\bpsi(z_{t})\tilde{p}(z_{t}|x_{1:t})\label{eq:ssm-mme-def}\\
 & =\int dz_{t}\bpsi(z_{t})\int dz_{t-1}p(z_{t}|z_{t-1})p(x_{t}|z_{t})p(z_{t-1}|x_{1:t-1})\\
 & =\int dz_{t-1}\left[\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1})p(x_{t}|z_{t})\right]p(z_{t-1}|x_{1:t-1})\label{eq:ssm-mme-integral}\\
 & \stackrel{(1)}{\approx}\int dz_{t-1}\left[\left\langle \cU,\bphi(x_{t})\otimes\bpsi(z_{t})\right\rangle \right]p(z_{t-1}|x_{1:t-1})\label{ssm-mme}
\end{align}

\end_inset

where in step (1) we used the equivalent of Equation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:approx_expectation"

\end_inset

 to approximate the integral in the square brackets.
 In a simpler scenario where there is no data likelihood 
\begin_inset Formula $p(x_{t}|z_{t})$
\end_inset

), this integral is a conditional expectation and the weights 
\begin_inset Formula $\cW$
\end_inset

 becomes a regressor from 
\begin_inset Formula $z_{t-1}$
\end_inset

 to 
\begin_inset Formula $z_{t}$
\end_inset

.
 With the likelihood, this integral becomes an unnormalized expectation
 and cannot be approximated by regression.
 We discuss potential remedies in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:remedies-for-unnorm-exp"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection
DRD
\end_layout

\begin_layout Standard
DRD can be derived using a somewhat hypothetical path of computation.
 If we ignore the graphical structure and simply perform a multilinear regressio
n from history 
\begin_inset Formula $x_{1},x_{2},\dots x_{t-1}$
\end_inset

 to 
\begin_inset Formula $\bpsi(z_{t-1})$
\end_inset

, then we can obtain the previous filtering posterior directly 
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t-1}|x_{1:t-1}} & =\cC_{Z_{t-1}|X_{1:t-1}}\left[\bigotimes_{i=1}^{t-1}\bphi(x_{i})\right]
\end{align*}

\end_inset

from which we can write 
\begin_inset Formula 
\[
\bigotimes_{i=1}^{t-1}\bphi(x_{i})=\cC_{Z_{t-1}|X_{1:t-1}}^{\dagger}\bmu_{Z_{t-1}|x_{1:t-1}}
\]

\end_inset


\end_layout

\begin_layout Standard
Likewise, the current posterior can be obtained by 
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}|x_{1:t}} & =\cC_{Z_{t}|X_{1:t}}\left[\bigotimes_{i=1}^{t}\bphi(x_{i})\right]\\
 & =\cC_{Z_{t}|X_{1:t}}\left[\left[\bigotimes_{i=1}^{t-1}\bphi(x_{i})\right]\otimes\bphi(x_{t})\right]\\
 & =\cC_{Z_{t}|X_{1:t}}\left[\left[\cC_{Z_{t-1}|X_{1:t-1}}^{\dagger}\bmu_{Z_{t-1}|x_{1:t-1}}\right]\otimes\bphi(x_{t})\right]\\
 & =\cC_{Z_{t}|X_{1:t}}\cC_{Z_{t-1}|X_{1:t-1}}^{\dagger}\left[\bmu_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\right]\\
 & =\cC_{Z_{t},X_{1:t}}\cC_{Z_{t-1},X_{1:t-1}}^{\dagger}\left[\bmu_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\right]
\end{align*}

\end_inset

Thus, there exists a bilinear mapping from 
\begin_inset Formula $\bmu_{Z_{t-1}|x_{1:t-1}}$
\end_inset

 and 
\begin_inset Formula $\bphi(x_{t})$
\end_inset

 to 
\begin_inset Formula $\bmu_{Z_{t}|x_{1:t}}$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Combine DRD with sample-based learning
\end_layout

\begin_layout Standard
Notice that there is a factorisation of the filtering DDC
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}|x_{1:t}} & =\E_{Z_{t}|x_{1:t}}\left[\bpsi(Z_{t})\right]\\
 & =\E_{Z_{t-1}|x_{1:t}}\left[\E_{Z_{t}|Z_{t-1},X_{t}}\left[\bpsi(Z_{t})\right]\right]
\end{align*}

\end_inset

Or in operator form
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}|x_{1:t}} & =\left\langle \cC_{Z_{t}|Z_{t-1}x_{t}},\bmu_{Z_{t-1}|X_{1:t}}\right\rangle \\
 & =\left\langle \cC_{Z_{t}|Z_{t-1}X_{t}},\bmu_{Z_{t-1}|X_{1:t}}\otimes\bpsi(x_{t})\right\rangle 
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The inner expectation can be trained from samples, and 
\begin_inset Formula $\bmu_{Z_{t-1}|X_{1:t}}$
\end_inset

 can be computed from DRD of the form
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}-1|x_{1:t}} & =\left\langle \cW,\bmu{}_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\right\rangle \\
\cW:\text{ \ } & \bmu{}_{Z_{t-1}|x_{1:t-1}}\otimes\bphi(x_{t})\xmapsto{x_{1:t},z_{t-1}}\bpsi(z_{t-1})
\end{align*}

\end_inset

In fact, as we will need the one-step smoothing DDC 
\begin_inset Formula $\bmu_{Z_{t}-1|x_{1:t}}$
\end_inset

 for learning the generative model, this is the preferred filtering operation.
 
\end_layout

\begin_layout Subsubsection
Learning
\end_layout

\begin_layout Standard
The required gradients for updating the generative parameters are 
\begin_inset Formula $\E\left[\left\langle \nabla_{\theta_{f}}g\left(Z_{t-1},\theta\right),T\left(Z_{t}\right)\right\rangle \right]$
\end_inset

 and 
\begin_inset Formula $\E\left[\nabla_{\theta_{f}}\Phi\left(Z_{t-1},\theta\right)\right]$
\end_inset

 for the latent dynamics, and 
\begin_inset Formula $\E\left[\nabla_{\theta_{g}}g\left(Z_{t},\theta_{g}\right)\right]$
\end_inset

 and 
\begin_inset Formula $\E\left[\nabla_{\theta_{g}}\Phi\left(Z_{t},\theta_{g}\right)\right]$
\end_inset

 for the observation distribution, where all the expectations are taken
 under the filtering posterior conditioned on 
\begin_inset Formula $x_{1:t}$
\end_inset

.
 The update for 
\begin_inset Formula $\theta_{g}$
\end_inset

 can be obtained directly from 
\begin_inset Formula $\bmu_{Z_{t}|x_{1:t}}$
\end_inset

 combined with DDC function approximation, 
\begin_inset Formula $\E\left[\nabla_{\theta_{f}}\Phi\left(Z_{t-1},\theta\right)\right]$
\end_inset

 requires the one-step soothing posterior.
 
\end_layout

\begin_layout Standard
There are a few ways one could approach expectation of under the pairwise
 joint 
\begin_inset Formula $\E\left[\left\langle \nabla_{\theta_{f}}g\left(Z_{t-1},\theta\right),T\left(Z_{t}\right)\right\rangle \right]$
\end_inset

.
 
\end_layout

\begin_layout Paragraph
Feature-gradient regression
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\E\left[\left\langle \nabla_{\theta_{f}}g\left(Z_{t-1},\theta_{f}\right),T\left(Z_{t}\right)\right\rangle \right] & =\E_{Z_{t-1}|x_{1:t}}\left[\left\langle \nabla_{\theta_{f}}g\left(Z_{t-1},\theta_{f}\right),\E_{Z_{t}|Z_{t-1},x_{t}}\left[T\left(Z_{t}\right)\right]\right\rangle \right]\\
 & =\E_{Z_{t-1}|x_{1:t}}\left[\left\langle \nabla_{\theta_{f}}g\left(Z_{t-1},\theta_{f}\right),\left\langle \cU_{z_{t-1},x_{t}\mapsto T}\bphi,(x_{t})\otimes\bpsi(z_{t-1})\right\rangle \right\rangle \right]\\
 & =\left\langle \E_{Z_{t-1}|x_{1:t}}\left[\nabla_{\theta_{f}}g\left(Z_{t-1},\theta_{f}\right)\otimes\bpsi(z_{t-1})\right],\left\langle \cU_{z_{t-1},x_{t}\mapsto T},\bphi(x_{t})\right\rangle \right\rangle \\
 & =\left\langle \left\langle \cU_{z_{t-1}\mapsto\otimes},\bmu_{Z_{t-1}|x_{1:t}}\right\rangle ,\left\langle \cU_{z_{t-1},x_{t}\mapsto T},\bphi(x_{t})\right\rangle \right\rangle 
\end{align*}

\end_inset

where 
\begin_inset Formula 
\begin{align*}
\cU_{z_{t-1}\mapsto\otimes}: & \ \bpsi(z_{t-1})\xmapsto{z_{t-1}}\nabla_{\theta_{f}}g\left(Z_{t-1},\theta_{f}\right)\otimes\bpsi(z_{t-1})\\
\cU_{z_{t-1},x_{t}\mapsto T}: & \ \bpsi(z_{t-1}),\bphi(x_{t})\xmapsto{z_{t-1},z_{t,}x_{t}}T(z_{t})
\end{align*}

\end_inset

Two additional regressions are required, in which the regressor of 
\begin_inset Formula $\cU_{z_{t-1}\mapsto\otimes}$
\end_inset

 is trained to predict from feature to an outer product of this feature
 and gradients, and has deterministic target.
 On the other hand, the regressor of 
\begin_inset Formula $\cU_{z_{t-1},x_{t}\mapsto T}$
\end_inset

 involves only 
\begin_inset Formula $z_{t}$
\end_inset

, but has noisy target.
\end_layout

\begin_layout Paragraph
Direct regression
\begin_inset Formula 
\begin{align*}
\E\left[\left\langle \nabla_{\theta_{f}}g\left(Z_{t-1},\theta_{f}\right),T\left(Z_{t}\right)\right\rangle \right] & =\E_{Z_{t-1}|x_{1:t}}\left[\E_{Z_{t}|Z_{t-1},x_{t}}\left[\left\langle \nabla_{\theta_{f}}g\left(z_{t-1},\theta_{f}\right),T\left(z_{t}\right)\right\rangle \right]\right]\\
 & =\E_{Z_{t-1}|x_{1:t}}\left[\left\langle \cU_{z_{t-1},x_{t}\mapsto gT},\bpsi(z_{t-1})\otimes\bphi(x_{t})\right\rangle \right]\\
 & =\left\langle \cU_{z_{t-1},x_{t}\mapsto gT},\bmu_{Z_{t-1}|x_{1:t}}\otimes\bphi(x_{t})\right\rangle 
\end{align*}

\end_inset


\series medium
where 
\begin_inset Formula 
\[
\cU_{z_{t-1},x_{t}\mapsto gT}:\ \bpsi(z_{t-1}),\bphi(x_{t})\xmapsto{z_{t-1},z_{t,}x_{t}}\left\langle \nabla_{\theta_{f}}g\left(Z_{t-1},\theta_{f}\right),T\left(Z_{t}\right)\right\rangle 
\]

\end_inset


\end_layout

\begin_layout Standard
One additional regression is required, and like DRD, we may replace the
 
\begin_inset Formula $\bpsi(z_{t-1})$
\end_inset

 with 
\begin_inset Formula $\bmu_{Z_{t-1}|x_{1:t}}$
\end_inset

 from the sleep sequences.
 The regression target is noisy and involves two variables 
\begin_inset Formula $z_{t-1}$
\end_inset

and 
\begin_inset Formula $z_{t}$
\end_inset

 regardless of using messages on or samples of 
\begin_inset Formula $z_{t}$
\end_inset

 as inputs.
\end_layout

\begin_layout Subsection
Smoothing
\end_layout

\begin_layout Subsection
Similarities between DRD and KBR for filtering
\end_layout

\begin_layout Standard
Here, we focus on the novel DRD scheme and compare it with the KBR solution
 which is known to be consistent using infinitely many features.
 The function 
\begin_inset Formula $g$
\end_inset

, if multilinear in its input features, can be written as
\begin_inset Formula 
\begin{align}
g_{t} & =\underbrace{\E_{p(x_{1:t},z_{t})}\left[\bpsi(z_{t})\otimes\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right]}_{\cW_{\bpsi_{t}(\bphi_{t}\bmu_{t-1})}}\underbrace{\E_{p(x_{1:t})}\left[\left(\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right)\otimes\left(\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right)\right]^{-1}}_{\cW_{(\bphi_{t}\bmu_{t-1})(\bphi_{t}\bmu_{t-1})}^{-1}}\label{eq:ssm_drd_regressor}
\end{align}

\end_inset

Denoting the random observation history 
\begin_inset Formula $X_{1:t}$
\end_inset

 as 
\begin_inset Formula $H_{t}$
\end_inset

, the corresponding observations 
\begin_inset Formula $x_{1:t}$
\end_inset

 as 
\begin_inset Formula $h_{t}$
\end_inset

 and its feature as 
\begin_inset Formula $\bphi^{\otimes t}(h_{t})$
\end_inset

 
\begin_inset Foot
status open

\begin_layout Plain Layout
since the feature for the history up to time 
\begin_inset Formula $t$
\end_inset

 is the tensor product 
\begin_inset Formula $\bigotimes_{i=1}^{t}\bphi(x_{i}$
\end_inset

)
\end_layout

\end_inset

.
 The first expectation in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm_drd_regressor"

\end_inset

can be simplified as 
\begin_inset Formula 
\begin{align*}
\cW_{\bpsi_{t}(\bphi_{t}\bmu_{t-1})} & =\E_{p(h_{t-1})}\left[\E_{p(z_{t},x_{t}|h_{t-1})}\left[\bpsi(z_{t})\otimes\bphi(x_{t})\right]\otimes\bmu_{Z_{t-1}|h_{t-1}}\right]\\
 & =\E_{p(h_{t-1})}\left[\cC_{Z_{t}X_{t}|h_{t-1}}\otimes\bmu_{Z_{t-1}|h_{t-1}}\right]\\
 & =\E_{p(h_{t-1})}\left[\left\langle \cC_{Z_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \otimes\left\langle \cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \right]\\
 & =\E_{p(h_{t-1})}\left[\left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\otimes\bphi^{\otimes t-t}(h_{t-1})\right\rangle \right]\\
 & =\left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\E\left[\bphi^{\otimes t-1}(h_{t-1})\otimes\bphi^{\otimes t-1}(h_{t-1})\right]\right\rangle \\
 & =\left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes2}}\right\rangle 
\end{align*}

\end_inset

where 
\begin_inset Formula $\cC_{H_{t-1}^{\otimes n}}:=\E\left[\bigotimes_{i}^{n}\left[\bphi^{\otimes t-1}(h_{t-1})\right]\right]$
\end_inset

.
 The second expectation in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm_drd_regressor"

\end_inset

 can be simplified similarly
\begin_inset Foot
status open

\begin_layout Plain Layout
The tensor product, shortened as 
\begin_inset Formula $\left(\bphi\otimes\bmu\right)\otimes\left(\bphi\otimes\bmu\right)$
\end_inset

, is implemend using matrix operations.
 For the following, we use
\begin_inset Formula $\otimes$
\end_inset

 as Kronecker product and 
\begin_inset Formula $vec(\dot{\cdot)}$
\end_inset

 as the operation that concatenates the columns into a vector.
 The term inside the expectation is in fact
\begin_inset Formula 
\[
vec(\bphi\otimes\bmu^{T})vec^{T}(\bphi\otimes\bmu^{T})=(\bphi\bphi^{T})\otimes(\bmu\bmu^{T})
\]

\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\begin{align*}
\cW_{(\bphi_{t}\bmu_{t-1})(\bphi_{t}\bmu_{t-1})} & =\E_{p(h_{t-1})}\left[\cC_{X_{t}X_{t}|h_{t-1}}\otimes\left\langle \cC_{Z_{t-1}|h_{t-1}},\cC_{Z_{t-1}|h_{t-1}}\right\rangle \right]\\
 & =\E_{p(h_{t-1})}\left[\left\langle \cC_{X_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \otimes\left\langle \cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\otimes\bphi^{\otimes t-1}(h_{t-1})\right\rangle \right]\\
 & =\left\langle \cC_{X_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes3}}\right\rangle 
\end{align*}

\end_inset

which gives the recursion.
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}|x_{1:t}} & =\left\langle \left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes2}}\right\rangle \left\langle \cC_{X_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes3}}\right\rangle ^{-1},\bphi(x_{t})\otimes\bmu_{Z_{t-1}|x_{1:t-1}}\right\rangle \\
 & =\left\langle \left\langle \cC_{Z_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes2}}\right\rangle \left\langle \cC_{X_{t}X{}_{t}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}}\otimes\cC_{Z_{t-1}|H_{t-1}},\cC_{H_{t-1}^{\otimes3}}\right\rangle ^{-1}\cC_{Z_{t-1}|H_{t-1}},\bphi(x_{t})\otimes\bphi^{\otimes t-1}(h_{t-1})\right\rangle 
\end{align*}

\end_inset

Compare with KBR which can be written as 
\begin_inset Formula 
\begin{align*}
\bmu_{Z_{t}|x_{1:t}} & =\left[C_{Z_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]\left[C_{X_{t}X_{t}|Z_{t-1}}\bmu_{Z_{t-1}|x_{1:t-1}}\right]^{-1}\bphi(x_{t})\\
 & =\left\langle C_{Z_{t}X_{t}|Z_{t-1}}\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \left\langle C_{X_{t}X_{t}|Z_{t-1}}\cC_{Z_{t-1}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle ^{-1}\bphi(x_{t})\\
 & =\left\langle C_{Z_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle \left\langle C_{X_{t}X_{t}|H_{t-1}},\bphi^{\otimes t-1}(h_{t-1})\right\rangle ^{-1}\bphi(x_{t})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Both expressions involve the key covariance operators 
\begin_inset Formula $\cC_{Z_{t}X{}_{t}|H_{t-1}}$
\end_inset

 and 
\begin_inset Formula $\cC_{X_{t}X{}_{t}|H_{t-1}}$
\end_inset

.
 However, in the regression operator of DRD, there are also terms that involve
 the filtering posterior operator 
\begin_inset Formula $\cC_{Z_{t-1}|H_{t-1}}$
\end_inset

 and 2nd and 3rd moment representations of the history 
\begin_inset Formula $\cC_{H_{t-1}^{\otimes3}}$
\end_inset

, whereas in KBR no such terms are involved.
 Critically, there is no interaction between the two covariance operators
 and filtering posterior operator.
 Thus, intuitively, if the filtering operator is only approximate (due to
 regularisation), DRD is able to take into account any errors in the posterior.
 Higher order moments of the observation history may also help enhance the
 regression.
\end_layout

\begin_layout Subsection
Strategies for evaluating the integral inside 
\begin_inset CommandInset label
LatexCommand label
name "subsec:remedies-for-unnorm-exp"

\end_inset


\end_layout

\begin_layout Standard
Here we a discuss the issue of evaluating the integral in the square bracket
 of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm-mme-integral"

\end_inset

as part of process for learning a filter by MMR.
 Denote this expectation as 
\begin_inset Formula $E_{t}=\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1})p(x_{t}|z_{t})$
\end_inset

.
 Since it is not a expectation under a normalized density, training a regressor
 from 
\begin_inset Formula $z_{t-1}$
\end_inset

 and 
\begin_inset Formula $x_{t}$
\end_inset

 to predict 
\begin_inset Formula $z_{t}$
\end_inset

 will not yield the desired weights 
\begin_inset Formula $\cW$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "ssm-mme"

\end_inset

.
 This can be further verified by rewriting 
\begin_inset Formula $E_{t}$
\end_inset

 as follows, noting the fact that 
\begin_inset Formula $p(z_{t}|z_{t-1},x_{t})=\frac{p(z_{t}|z_{t-1})p(x_{t}|z_{t})}{p(x_{t}|z_{t-1})}$
\end_inset


\begin_inset Formula 
\begin{align*}
E_{t} & =\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1},x_{t})p(x_{t}|z_{t-1})\\
 & \approx\bmu_{Z_{t}|z_{t-1},x_{t}}p(x_{t}|z_{t-1})
\end{align*}

\end_inset

The regression method will produce the correct estimate of 
\begin_inset Formula $\bmu_{Z_{t}|z_{t-1},x_{t}}$
\end_inset

 but will miss a factor 
\begin_inset Formula $p(x_{t}|z_{t-1})$
\end_inset

 that depends on 
\begin_inset Formula $z_{t-1}$
\end_inset

 for 
\begin_inset Formula $E_{t}$
\end_inset

.
 The update equation for the next normalized mean embedding in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm-mme-def"

\end_inset

 becomes
\begin_inset Formula 
\[
\tilde{\bmu}{}_{Z_{t}|x_{1:t}}=\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}p(x_{t}|z_{t-1})p(z_{t-1}|x_{1:t-1})
\]

\end_inset

To take into account the normalizer, we can express the normalized mean
 embedding in several ways.
 
\begin_inset Formula 
\begin{align}
\bmu{}_{Z_{t}|x_{1:t}} & =\frac{1}{p(x_{t}|x_{1:t-1})}\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}p(x_{t}|z_{t-1})p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}\frac{p(x_{t}|z_{t-1})}{p(x_{t}|x_{1:t-1})}p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}\frac{p(x_{t},z_{t-1}|x_{1:t-1})}{p(x_{t}|x_{1:t-1})p(z_{t-1}|x_{1:t-1})}p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}\frac{p(z_{t-1}|x_{1:t})}{p(z_{t-1}|x_{1:t-1})}p(z_{t-1}|x_{1:t-1})\nonumber \\
 & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t}}p(z_{t-1}|x_{1:t})\label{eq:ssm-mme-recursion-lag-input}\\
 & \ne\left\langle \cC_{Z_{t}|Z_{t-1},x_{t}},\bmu_{z_{t-1}|x_{1:t-1}}\right\rangle \nonumber 
\end{align}

\end_inset

where 
\begin_inset Formula $\cC_{Z_{t}|Z_{t-1},x_{t}}$
\end_inset

 is a conditional operator such that 
\begin_inset Formula $\bmu_{Z_{t}|z_{t-1},x_{t}}=\left\langle \cC_{Z_{t}|Z_{t-1},x_{t}},\psi(z_{t-1})\right\rangle $
\end_inset

 and from the last line, we see that the desired input to the learned regressor
 would be the mean representation of 
\begin_inset Formula $p(z_{t-1}|x_{1:t})$
\end_inset

.
\end_layout

\begin_layout Subsubsection
Known likelihood function
\end_layout

\begin_layout Standard
If this likelihood function is known, we could approximate it with 
\begin_inset Formula $p(x_{t}|z_{t})\approx\left\langle p,\bphi(z_{t})\otimes\bpsi(x_{t})\right\rangle $
\end_inset

, and this integral becomes
\begin_inset Formula 
\begin{align*}
E_{t} & =\int dz_{t}\bpsi(z_{t})p(z_{t}|z_{t-1})p(x_{t}|z_{t})\\
 & =\int dz_{t}\bpsi(z_{t})\left\langle p,\bphi(z_{t})\otimes\bpsi(x_{t})\right\rangle p(z_{t}|z_{t-1})\\
 & =\left\langle p,\bphi(x_{t})\otimes C_{Z_{t}Z_{t}|z_{t-1}}\right\rangle 
\end{align*}

\end_inset

Nontheless, in general we would like to avoid approximating the likelihood
 function as it is inaccessible in many situations.
 
\end_layout

\begin_layout Subsubsection
Fixed-lag smoothing
\end_layout

\begin_layout Standard
Another solution can be seen by recursing on a slightly different object.
 Instead of 
\begin_inset Formula $p(z_{t}|x_{1:t})$
\end_inset

, if we are interested in a fix-lag version of the posterior given some
 future observations
\begin_inset Formula $p(z_{t}|x_{1:t+\tau})$
\end_inset

 for some 
\begin_inset Formula $\tau>0$
\end_inset

, then we may assume that 
\begin_inset Formula $p(z_{t}|x_{1:t+\tau})\approx p(z_{t}|x_{1:t+\tau-1})$
\end_inset

.
 From 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:ssm-mme-recursion-lag-input"

\end_inset

 it is easy to see 
\begin_inset Formula 
\begin{align*}
\bmu{}_{Z_{t}|x_{1:t+\tau}} & =\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t:t+\tau}}p(z_{t-1}|x_{1:t+\tau})\\
 & \approx\int dz_{t-1}\bmu_{Z_{t}|z_{t-1},x_{t:t+\tau}}p(z_{t-1}|x_{1:t+\tau-1})\\
 & =\left\langle \cC_{Z_{t}|Z_{t-1},x_{t:t+\tau}},\bmu_{Z_{t-1}|x_{1:t+\tau-1}}\right\rangle 
\end{align*}

\end_inset

which requires a regressor that takes 
\begin_inset Formula $z_{t-1}$
\end_inset

 and 
\begin_inset Formula $x_{t:t+\tau}$
\end_inset

 as input to predict 
\begin_inset Formula $z_{t}$
\end_inset

, and the predictions can be used as 
\begin_inset Formula $\bmu_{Z_{t}|z_{t-1},x_{t:t+\tau}}$
\end_inset

.
 There is a trade-off between how well this dropped condition approximation
 holds and how complicated the regressor have to be.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ref"
options "plain"

\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\renewcommand{\bphi}{\bm{\phi}}
{\bm{\phi}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\bmu}{\bm{\mu}}
{\bm{\mu}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\bSigma}{\bm{\Sigma}}
{\bm{\Sigma}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\bI}{\bm{I}}
{I}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\E}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\H}{\mathbb{H}}
{\mathbb{H}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\bpsi}{\bm{\psi}}
{\bm{\psi}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\cW}{\mathcal{W}}
{\mathcal{W}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\m}{\mathbf{m}}
{\mathbf{m}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\bxi}{\bm{\xi}}
{\bm{\xi}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\cH}{\mathcal{H}}
{\mathcal{H}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\cF}{\mathcal{F}}
{\mathcal{F}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\cG}{\mathcal{G}}
{\mathcal{G}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\cC}{\mathcal{C}}
{\mathcal{C}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\cU}{\mathcal{U}}
{\mathcal{U}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\cN}{\mathcal{N}}
{\mathcal{N}}
\end_inset


\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\renewcommand{\lotimes}{\Large{\otimes}}
{\Large{\otimes}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\bnu}{\bm{\nu}}
{\bm{\nu}}
\end_inset


\end_layout

\end_body
\end_document

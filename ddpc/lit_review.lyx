#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extreport
\begin_preamble
\setcitestyle{round}
% call the glossaries package
\usepackage[nonumberlist]{glossaries} 
\usepackage{hyperref}
\usepackage{cleveref}
% activate the glossaries building
\renewcommand*{\glstextformat}{\textbf}
\makeglossaries
\usepackage{atveryend}

% redefine the command that LyX puts out, to the glossaries input
\let\nomenclature\gls

% add your glossaries entries
\newglossaryentry{linux}{name={linux},description={an open-source unix implementation}}
\newglossaryentry{SSM}{name={SSM},description={State Space Model}}
\newglossaryentry{LGSSM}{name={LGSSM},description={Linear Gaussian State Space Model}}
\newglossaryentry{NPF}{name={NPF}, description={Neural Particle Filter by Kutschireiter et al. (2017)}}
\newglossaryentry{PPC}{name={PPC}, description={Probabilistic population code}}
\newglossaryentry{EFH}{name={EFH}, description={Exponential family harmonium}}
\newglossaryentry{rEFH}{name={rEFH}, description={Recurrent Exponential family harmonium for filtering by Makin et al. (2015)}}
\newglossaryentry{PPC-BR-EFH}{name={PPC-BR-EFH}, description={Probabilistic population code Bayes rule with exponential family harmonium by Sokoloski (2017)}}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams-chap-bytype
theorems-ams-extended-chap-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\rvfont}[1]{\boldsymbol{#1}}
{\boldsymbol{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vfont}[1]{\mathbf{#1}}
{\mathbf{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mfont}[1]{\mathbf{#1}}
{\mathrm{\boldsymbol{#1}}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vtheta}{\vfont{\theta}}
{\vfont{\theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vn}{\vfont n}
{\vfont n}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\vfont w}
{\vfont w}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vomega}{\boldsymbol{\omega}}
{\boldsymbol{\omega}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vz}{\vfont z}
{\vfont z}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vX}{\rvfont X}
{\rvfont X}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vZ}{\rvfont Z}
{\rvfont Z}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vN}{\rvfont N}
{\rvfont N}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vf}{\vfont f}
{\vfont f}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vg}{\vfont g}
{\vfont g}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vR}{\rvfont R}
{\rvfont R}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vpsi}{\boldsymbol{\psi}}
{\boldsymbol{\psi}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vzeta}{\boldsymbol{\zeta}}
{\boldsymbol{\zeta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vphi}{\boldsymbol{\phi}}
{\boldsymbol{\phi}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mtheta}{\mathbf{\Theta}}
{\mathbf{\Theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mW}{\mfont W}
{\mfont W}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mM}{\mfont M}
{\mfont M}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mSigma}{\mfont{\Sigma}}
{\mfont{\Sigma}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mV}{\mfont V}
{\mfont V}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\N}{\mathbb{N}}
{\mathbf{\mathbb{N}}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sM}{\mathcal{M}}
{\mathcal{M}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Dim}[1]{K_{#1}}
{K_{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sigm}{\operatorname{sigm}}
{\textrm{sigm}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Pois}{\operatorname{Pois}}
{\textrm{Pois}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Bern}{\operatorname{Bern}}
{\textrm{Bern}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\KL}{\operatorname{KL}}
{\textrm{KL}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\NN}{\operatorname{NN}}
{\textrm{NN}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vectorfont}[1]{\boldsymbol{#1}}
{\boldsymbol{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\setfont}[1]{\mathbb{#1}}
{\mathbb{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\matrixfont}[1]{\mathbf{#1}}
{\mathbf{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\graphfont}[1]{\mathcal{#1}}
{\mathcal{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vz}{\vectorfont z}
{\vectorfont z}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vx}{\vectorfont x}
{\vectorfont x}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vy}{\vectorfont y}
{\vectorfont y}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vz}{\vectorfont z}
{\vectorfont z}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vphi}{\vectorfont{\phi}}
{\vectorfont{\phi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\valpha}{\vectorfont{\alpha}}
{\vectorfont{\alpha}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vpsi}{\vectorfont{\psi}}
{\vectorfont{\psi}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vf}{\vectorfont f}
{\vectorfont f}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vtheta}{\vectorfont{\theta}}
{\vectorfont{\theta}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vgamma}{\vectorfont{\gamma}}
{\vectorfont{\gamma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vepsilon}{\vectorfont{\epsilon}}
{\vectorfont{\epsilon}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vb}{\vectorfont b}
{\vectorfont b}
\end_inset


\begin_inset FormulaMacro
\newcommand{\softplus}{\textrm{softplus}}
{\textrm{softplus}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vw}{\vectorfont w}
{\vectorfont w}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vm}{\vectorfont m}
{\vectorfont m}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vmu}{\vectorfont{\mu}}
{\vectorfont{\mu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vr}{\vectorfont r}
{\vectorfont r}
\end_inset


\begin_inset FormulaMacro
\newcommand{\va}{\vectorfont a}
{\vectorfont a}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vp}{\vectorfont p}
{\vectorfont p}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vh}{\vectorfont h}
{\vectorfont h}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vs}{\vectorfont s}
{\vectorfont s}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vv}{\vectorfont v}
{\vectorfont v}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mY}{\matrixfont Y}
{\matrixfont Y}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mI}{\matrixfont I}
{\matrixfont I}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\mSigma}{\matrixfont{\Sigma}}
{\matrixfont{\Sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mPhi}{\matrixfont{\Phi}}
{\matrixfont{\Phi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mPsi}{\matrixfont{\Psi}}
{\matrixfont{\Psi}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\mW}{\matrixfont W}
{\matrixfont W}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\gH}{\graphfont H}
{\graphfont H}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gN}{\graphfont N}
{\graphfont N}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gD}{\graphfont D}
{\graphfont D}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gU}{\graphfont U}
{\graphfont U}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gF}{\graphfont F}
{\graphfont F}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gG}{\graphfont G}
{\graphfont G}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gQ}{\graphfont Q}
{\graphfont Q}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gR}{\graphfont R}
{\graphfont R}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gX}{\graphfont X}
{\graphfont X}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sR}{\setfont R}
{\setfont R}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
{\mathbb{E}{}_{#1}\left[#2\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatorname*{\arg\min}}
{\textrm{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Dx}{\Delta_{\vtheta}(\vx)}
{\Delta_{\vtheta}(\vx)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Dxt}{\Delta_{\vtheta_{t}}(\vx)}
{\Delta_{\vtheta_{t}}(\vx)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\KL}{\mathrm{KL}}
{\mathrm{KL}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\NN}{\mathrm{NN}}
{\mathrm{NN}}
\end_inset


\end_layout

\begin_layout Chapter
Literature review
\end_layout

\begin_layout Section
Approximate inference and learning 
\end_layout

\begin_layout Subsection
Variational learning
\end_layout

\begin_layout Standard
Consider a generative model described by a directed graph 
\begin_inset Formula $\vZ_{0}\rightarrow\vZ_{2}\rightarrow\cdots\rightarrow\vZ_{L}$
\end_inset

, where 
\begin_inset Formula $\vZ_{0}\sim p_{\theta_{0}}\left(\vz_{0}\right)$
\end_inset

 and each arrow suggests a parametric conditional distribution 
\begin_inset Formula $p_{\theta_{l}}(\vZ_{l}|\vZ_{l-1}),l\in[1\dots,L]$
\end_inset

.
 In addition, denote 
\begin_inset Formula $\vX=\vZ_{L}$
\end_inset

, and 
\begin_inset Formula $\vZ=\left\{ \vZ_{0},\cdots\vZ_{L-1}\right\} $
\end_inset

.
 The goal is to fit the parameters 
\begin_inset Formula $\theta=\{\theta_{l}\}_{l=1}^{L}$
\end_inset

 in order to maximize the likelihood of some observed data 
\begin_inset Formula $\vx=\left\{ \vx_{n}\right\} _{n=1}^{N}\sim\Pi(\vx)$
\end_inset

 i.i.d from some unknown true distribution 
\begin_inset Formula $\Pi(\vx)$
\end_inset

.
 Although the set of conditional distributions implies a normalized joint
 distribution 
\begin_inset Formula $p_{\theta}(\vZ,\vX)=p_{\theta_{0}}\left(\vZ_{0}\right)\prod_{l=1}^{L}p_{\theta_{l}}\left(\vZ_{l}|\vZ_{l-1}\right)$
\end_inset

 , evaluating the likelihood of data 
\begin_inset Formula $\mathcal{L}(\theta):=\log p_{\theta}(\vx)$
\end_inset

 involves the sum 
\begin_inset Formula $\sum_{\vZ}p_{\theta}(\vZ,\vX)$
\end_inset

, which is computationally intractable.
 Instead, we evaluate the free energy
\begin_inset Formula 
\begin{align*}
\mathcal{F}(\theta,q): & =\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX)-\log(q(\vZ|\vX))}\\
 & =\mathcal{L}(\theta)-\KL\left[q(\vZ|\vX)\|p(\vZ\|\vX)\right]\\
 & \le\mathcal{L}(\theta)
\end{align*}

\end_inset

where 
\begin_inset Formula $q(\vZ|\vX)$
\end_inset

 is the variational distribution that can depend on 
\begin_inset Formula $\vX$
\end_inset

.
 It can be shown that the bound is tight if and only if 
\begin_inset Formula $\KL\left[\log q\left(\vZ|\vX\right)\|\log p_{\theta}\left(\vZ|\vX\right)\right]=0$
\end_inset

.
 Thus, maximizing 
\begin_inset Formula $\mathcal{F}(\theta,q)$
\end_inset

 involves an iterative process that in each iteration alternates between
\end_layout

\begin_layout Enumerate
(M-step) maximizing 
\begin_inset Formula $\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX))}$
\end_inset


\end_layout

\begin_layout Enumerate
(E-step) minimizing 
\begin_inset Formula $\KL\left[\log q\left(\vZ|\vX\right)\|\log p_{\theta}\left(\vZ|\vX\right)\right]$
\end_inset


\end_layout

\begin_layout Standard
By defining the joint distribution 
\begin_inset Formula $q_{\phi}(\vZ,\vX):=q_{\phi}(\vZ|\vX)\Pi(\vX)$
\end_inset

, the M-step is also equivalent to
\begin_inset Formula 
\begin{equation}
\min_{\theta}\KL\left[q_{\phi}(\vZ,\vX)\|p_{\theta}(\vZ,\vX)\right]\label{eq:M_step_joint_KL}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Helmholtz machine
\end_layout

\begin_layout Standard
The wake-sleep algorithm by 
\begin_inset CommandInset citation
LatexCommand citet
key "HintonEtAl1995"
literal "true"

\end_inset

, designed for the classical Helmholtz machine proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "DayanEtAl1995"
literal "true"

\end_inset

, performs the M-step by samples from an approximate posterior 
\begin_inset Formula $q_{\phi}(\vZ|\vX)$
\end_inset

, and the posterior samples are obtained from an inferential model using
 fixed inferential parameters 
\begin_inset Formula $\phi$
\end_inset

.
 Because the generative model is Markov, the approximate (and true) posteriors
 factorizes
\begin_inset Formula 
\[
q_{\phi}(\vZ|\vX)=\prod_{l=0}^{L-1}q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})
\]

\end_inset


\end_layout

\begin_layout Standard
Thus, posterior samples can be obtained by a sequence of conditional sampling:
 given an input 
\begin_inset Formula $\vx$
\end_inset

, draws 
\begin_inset Formula $\vZ_{L-1}$
\end_inset

 given 
\begin_inset Formula $\vx$
\end_inset

, and then draws 
\begin_inset Formula $\vZ_{L-2}$
\end_inset

 conditioned on each a posterior sample 
\begin_inset Formula $\vz_{L-1}|\vx$
\end_inset

, and so on.
 Each factor 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})$
\end_inset

 is implemented as a distribution with tractable sampling routine and flexible
 parameterisation, such as an exponential family distribution with parameters
 that are output of neural networks.
 For example, if 
\begin_inset Formula $\vZ$
\end_inset

 is binary, then we may choose conditionally independent posterior 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vz_{l+1})=\prod_{d}q_{\phi_{l,d}}(Z_{l,d}|\vz_{l+1})$
\end_inset

, 
\begin_inset Formula $q_{\phi_{l}}(Z_{l,d}|\vz_{l+1})=\Bern(Z_{l,d}|\NN(\vz_{l+1};\phi_{l,d}))$
\end_inset

, which in addition assumes conditional independence.
 To train 
\begin_inset Formula $\phi_{l}$
\end_inset

, generative samples 
\begin_inset Formula $\left\{ \vz_{l},\vz_{l+1}\right\} $
\end_inset

are drawn from 
\begin_inset Formula $p_{\theta}(\vZ_{l},\vZ_{l+1})$
\end_inset

 by ancestral sampling, and the objective is to maximize the likelihood
 of 
\begin_inset Formula $\vz_{l}$
\end_inset

 given 
\begin_inset Formula $\vz_{l+1}$
\end_inset

, averaged over different 
\begin_inset Formula $\vz_{l+1}$
\end_inset


\begin_inset Formula 
\[
\max_{\phi_{l}}\E{p_{\theta}(\vZ_{l},\vZ_{l+1})}{\log q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})}
\]

\end_inset

When the same procedure is used for all layers, it can be shown that this
 process is equivalent to minimizing the average reverse KL divergence 
\begin_inset Formula 
\[
\min_{\phi}\E{p_{\theta}(\vX)}{\KL\left[p_{\theta}(\vZ|\vX)\|q_{\phi}(\vZ|\vX)\right]}
\]

\end_inset

Or equivalently, by defining the joint distribution 
\begin_inset Formula $q_{\phi}(\vZ,\vX):=q_{\phi}(\vZ|\vX)\Pi(\vX)$
\end_inset


\begin_inset Formula 
\[
\min_{\phi}\KL\left[p_{\theta}(\vZ,\vX)\|q_{\phi}(\vZ,\vX)\right]
\]

\end_inset

This step of using generated sample to train the inferential model is known
 as the 
\begin_inset Quotes eld
\end_inset

sleep phase
\begin_inset Quotes erd
\end_inset

 .
 Note that the objective been minimized here is different from the KL in
 the usual E-step.
\end_layout

\begin_layout Standard
In the 
\begin_inset Quotes eld
\end_inset

wake phase
\begin_inset Quotes erd
\end_inset

, the inferential model takes real observations 
\begin_inset Formula $\vX\sim\Pi(\vx)$
\end_inset

 and produces posterior samples from 
\begin_inset Formula $q_{\phi}(\vZ|\vX)$
\end_inset

.
 These samples are used to perform the usual M-step, which corresponds to
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:M_step_joint_KL"

\end_inset

.
 Therefore, the wake-sleep algorithm does not optimize a consistent divergence,
 although if 
\begin_inset Formula $p_{\theta}(\vZ,\vX)=q_{\phi}(\vZ,\vX)$
\end_inset

, both losses are at global minimum 0.
\end_layout

\begin_layout Standard
Particular instances of Helmholtz machine are biologically attractive.
 When 
\begin_inset Formula $p_{\theta_{l}}(\vZ_{l}|\vZ_{l-1})$
\end_inset

 and 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})$
\end_inset

 is in the exponential family, conditionally independent, and each factor
 is parameterized by its conditional mean in the form of the canonical link
 function on 
\begin_inset Formula $\vz_{l+1}$
\end_inset

 (a generalized linear model), then the gradient of 
\begin_inset Formula $\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX))}$
\end_inset

 w.r.t 
\begin_inset Formula $\theta$
\end_inset

 and the gradient of 
\begin_inset Formula $\E{p(\vZ,\vX)}{\log q_{\phi}(\vZ|\vX)}$
\end_inset

 w.r.t 
\begin_inset Formula $\phi$
\end_inset

 only depend on local samples, so learning in the generative and inferential
 model can be done using the biologically plausible learning rule.
 This is the case for the sigmoid belief net 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand citep
key "DayanEtAl1995"
literal "true"

\end_inset

 and generalisations in 
\begin_inset CommandInset citation
LatexCommand citep
key "DayanHinton1996"
literal "true"

\end_inset

.
\end_layout

\begin_layout Section
Neural representation of uncertainty
\end_layout

\begin_layout Subsection
Probabilistic population code (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
The probabilistic population code was proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "MaPouget2006Bayesian,BeckEtAl2007"
literal "false"

\end_inset

 as a way for the brain to represent probability distributions.
 Here, I reproduce and comment on the PPC theory.
 Given a stimulus 
\begin_inset Formula $x\in\gX$
\end_inset

 (a scalar for simplicity), neurons with tuning functions 
\begin_inset Formula $\vphi(x):=\left[\phi_{m}(x)\right]_{m}^{M}$
\end_inset

 respond with stochastic activities 
\begin_inset Formula $\vr:=\left[r_{m}\right]_{m=1}^{M},r_{m}\in\mathcal{R}$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
not to be confused with the real domain 
\begin_inset Formula $\mathbb{R}$
\end_inset


\end_layout

\end_inset

 according to a likelihood function 
\begin_inset CommandInset citation
LatexCommand citep
key "MaPouget2006Bayesian,BeckEtAl2007"
literal "false"

\end_inset


\begin_inset Formula 
\begin{equation}
p(\vr|x)=\frac{1}{Z(x)}\nu(\vr)\exp\left[\vh(x)\cdot\vr\right],\label{eq:PPC_lik}
\end{equation}

\end_inset

where 
\begin_inset Formula $\vh(x)$
\end_inset

 is known as the response kernel.
 A typical example of 
\begin_inset Formula $\vh(x)$
\end_inset

 is the Poisson-like neurons for which Gaussian-shaped tuning curves
\begin_inset Formula 
\begin{equation}
\phi_{m}(x)=A\exp\left[-\frac{1}{2\sigma_{\text{tc}}}(x-x_{m})^{2}\right]\label{eq:PPC_gaussian_tc}
\end{equation}

\end_inset

where 
\begin_inset Formula $x_{m}$
\end_inset

 is the preferred stimulus for the 
\begin_inset Formula $m$
\end_inset

'th neuron, 
\begin_inset Formula $\sigma_{\text{tc}}$
\end_inset

 is the tuning curve width and 
\begin_inset Formula $A$
\end_inset

 is the maximum firing rate.
 Under the uniform prior
\begin_inset Formula 
\begin{equation}
p(x)\propto1,\label{eq:PPC_uniform_prior}
\end{equation}

\end_inset

the likelihood implies the following posterior distribution, referred to
 as PPC encoded by 
\begin_inset Formula $\vr$
\end_inset

 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p_{\text{PPC}}(x|\vr)\propto\frac{1}{Z(x)}\exp\left[\vh(x)\cdot\vr\right].\label{eq:PPC_post}
\end{equation}

\end_inset

This is an exponential family distribution with 
\begin_inset Formula $\vh(x)$
\end_inset

 being the sufficient statistics, and 
\begin_inset Formula $\vr$
\end_inset

 is the natural parameter.
 Note that, although the tuning curves may form a rich set of basis functions,
 the response kernel 
\begin_inset Formula $\vh(x)$
\end_inset

 may not.
 For instance, Gaussian-shaped 
\begin_inset Formula $\vphi(x)$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_gaussian_tc"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is a rich basis on 
\begin_inset Formula $\gX$
\end_inset

, it only implies a Gaussian 
\begin_inset Formula $p_{\text{PPC}}(x|\vr)$
\end_inset

 if 
\begin_inset Formula $Z(x)$
\end_inset

 is a constant, since 
\begin_inset Formula $\vh(x)$
\end_inset

 is quadratic in 
\begin_inset Formula $x$
\end_inset

.
 Based on the encoding construction 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_lik"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and the induced posterior 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

, I make the following remarks: 
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Uncertainty encoded by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 
\end_layout

\end_inset

The stimulus 
\begin_inset Formula $x$
\end_inset

 in PPC construction is deterministic with no uncertainty.
 The uncertainty in 
\begin_inset Formula $p_{\text{PPC}}(x|\vr)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is induced by decoding through the likelihood 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_lik"
plural "false"
caps "false"
noprefix "false"

\end_inset

, incorporating a uniform prior 
\begin_inset Formula $p(s)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 as log linear codes
\end_layout

\end_inset

The form of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is a log linear encoding of distribution with basis functions 
\begin_inset Formula $\vh(x)$
\end_inset

 and linear weight 
\begin_inset Formula $\vr$
\end_inset

, with an additional base measure 
\begin_inset Formula $\frac{1}{Z(x)}$
\end_inset

.
 In this view, the encoded uncertainty simply comes depends on 
\begin_inset Formula $\vh(x)$
\end_inset

 and 
\begin_inset Formula $Z(x)$
\end_inset

 and is unrelated to the likelihood 
\begin_inset Formula $p(\vr|x)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 as sub-optimal decoding
\end_layout

\end_inset

We may take 
\begin_inset Formula $p_{\text{PPC}}(x|\vr)$
\end_inset

 as Bayesian decoding of uncertainty in 
\begin_inset Formula $x$
\end_inset

 from a particular firing pattern 
\begin_inset Formula $\vr$
\end_inset

 using a wrong prior.
 While the likelihood 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_lik"
plural "false"
caps "false"
noprefix "false"

\end_inset

 depends on the property of neurons and is thus known to the brain, the
 prior is unknown.
 Plus, the prior is usually not uniform even for low-level features, such
 as visual orientation (oblique effect), not to mention higher dimensional
 stimuli, such as images.
 Therefore, 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_uniform_prior"
plural "false"
caps "false"
noprefix "false"

\end_inset

 does not hold in general and decoding is sub-optimal, meaning that the
 decoded uncertainty does not reflect the real distribution of 
\begin_inset Formula $s$
\end_inset

 in the environment that can cause 
\begin_inset Formula $\vr$
\end_inset

 
\begin_inset CommandInset label
LatexCommand label
name "rem:PPC_suboptimal_decoding"

\end_inset

.
 
\end_layout

\begin_layout Standard
A generic base measure 
\begin_inset Formula $\frac{1}{Z(x)}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

 may complicate downsteram computations.
 This can be alleviated with further assumptions on tuning functions 
\begin_inset Formula $\vphi(x)$
\end_inset

, as explained in the Remark below.
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Weak dependence on 
\begin_inset Formula $Z$
\end_inset


\end_layout

\end_inset

 For Poisson neurons with likelihood 
\begin_inset Formula $\text{Pois}\left(r_{m}|\phi_{m}(x)\right)$
\end_inset

, under the uniform prior assumption 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_uniform_prior"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the distribution encoded by PPC is 
\begin_inset Formula 
\begin{equation}
p_{\text{PPC}}(x|\vr)\propto p(\vr|x)=\prod_{n=1}^{N}\text{Pois}\left(r_{m}|\phi_{m}(x)\right)\propto\frac{1}{\exp\left[\sum_{m=1}^{M}\phi_{m}(x)\right]}\exp\left[\vr\cdot\vh(\vx)\right].\label{eq:PPC_poisson}
\end{equation}

\end_inset

Under the condition that the total firing rate is roughly independent of
 
\begin_inset Formula $x$
\end_inset


\begin_inset Formula 
\begin{equation}
\sum_{m=1}^{M}\phi_{m}(x)\approx C(\vphi),\label{eq:PPC_total_firing_rate}
\end{equation}

\end_inset

the inverse base measure 
\begin_inset Formula $Z(x)=\exp\left(\sum_{m=1}^{M}\phi_{m}(x)\right)$
\end_inset

 can be assumed constant for any 
\begin_inset Formula $x$
\end_inset

.
 This is possible if 
\begin_inset Formula $\vphi$
\end_inset

 is densely arranged on 
\begin_inset Formula $\gX$
\end_inset

 and translation-invarant, neurons.
 If we further assume 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_gaussian_tc"
plural "false"
caps "false"
noprefix "false"

\end_inset

, then 
\begin_inset Formula $p_{\text{PPC}}(x|\vr)$
\end_inset

 is a Gaussian distribution.
\begin_inset CommandInset label
LatexCommand label
name "rem:weak_z_dependence"

\end_inset


\end_layout

\begin_layout Standard
Under the interpretation of Remark 
\begin_inset CommandInset ref
LatexCommand ref
reference "rem:PPC_suboptimal_decoding"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $p_{\text{PPC}}$
\end_inset

 is invariant to some nuisance variables 
\begin_inset Formula $c$
\end_inset

 that enter the likelihood as
\begin_inset Formula 
\begin{equation}
p(\vr|x,c)=\frac{1}{Z(x)}\nu_{n}(\vr,c)\exp\left[\vh(x)\cdot\vr\right],\label{eq:PPC_nuisance_lik}
\end{equation}

\end_inset

where the base measure 
\begin_inset Formula $\nu_{n}(\vr,c)$
\end_inset

 also depends on 
\begin_inset Formula $c$
\end_inset

.
 Then decoding by the Bayes rule gives 
\begin_inset Formula 
\[
p_{\text{PPC,n}}(x|\vr,c)=\frac{p(\vr|x,c)p(x|c)}{p(\vr|c)}\propto\frac{p(x|c)}{Z(x)}\exp\left[\vh(x)\cdot\vr\right].
\]

\end_inset

An example is when 
\begin_inset Formula $c$
\end_inset

 is the contrast of a visual stimuli and scales the tuning functions to
 
\begin_inset Formula $c\vphi(x)$
\end_inset

.
 Under the assumption that 
\begin_inset Formula $x$
\end_inset

 is both uniform and independent of 
\begin_inset Formula $c$
\end_inset

, 
\begin_inset Formula $p_{\text{PPC,n}}(x|\vr,c)=p_{\text{PPC}}(x|\vr)$
\end_inset

 and is thus independent of 
\begin_inset Formula $c$
\end_inset

.
 Almost all 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature consider contrast as the only nuisance variable.
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Possible nuisance variables
\end_layout

\end_inset

 The way nuisance variables can affect firing through the base measure in
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_nuisance_lik"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is limited, as it does now allow a generic dependence of tuning on 
\begin_inset Formula $c$
\end_inset

 through the response kernel 
\begin_inset Formula $\vh(\cdot)$
\end_inset

.
 In tasks where the nuisance variable is, for instance, spatial frequency
 or phase, the respose kernel 
\begin_inset Formula $\vh(\cdot)$
\end_inset

 depends on 
\begin_inset Formula $c$
\end_inset

 in more complicated ways that cannot be expressed through 
\begin_inset Formula $\nu_{n}$
\end_inset

 only.
 Also, for 
\begin_inset Formula $p^{\text{PPC,n}}(x|\vr,c)=p_{\text{PPC}}(x|\vr)$
\end_inset

 to hold, the condition that 
\begin_inset Formula $p(x|c)$
\end_inset

 is uniform and independent of 
\begin_inset Formula $c$
\end_inset

 is even stronger than 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_uniform_prior"
plural "false"
caps "false"
noprefix "false"

\end_inset

, since 
\begin_inset Formula $c$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 are unlikely to be independent in the environment.
\begin_inset CommandInset label
LatexCommand label
name "rem:PPC_nuisance_variables"

\end_inset


\end_layout

\begin_layout Standard
I now move on to discuss how 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 has been used for computation.
 In 
\series bold
cue combination
\series default
, consider two neural populations with responses described by 
\begin_inset Formula 
\begin{align*}
p(\vr_{a},\vr_{b}|x) & =\frac{1}{Z_{ab}(x)}\nu_{ab}(\vr_{a},\vr_{b})\exp\left[\vh_{a}(x)\cdot\vr_{a}+\vh_{b}(x)\cdot\vr_{b}\right].
\end{align*}

\end_inset

Under uniform 
\begin_inset Formula $p(x)$
\end_inset

 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_uniform_prior"
plural "false"
caps "false"
noprefix "false"

\end_inset

, this likelihood implies a posterior
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p_{\text{PPC}}(x|\vr_{a},\vr_{b})\propto\frac{1}{Z_{ab}(x)}\exp\left[\vh_{a}(x)\cdot\vr_{a}+\vh_{b}(x)\cdot\vr_{b}\right].
\]

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\vr_{a}$
\end_inset

 and 
\begin_inset Formula $\vr_{b}$
\end_inset

 may not be independent conditioned on 
\begin_inset Formula $s$
\end_inset

.
 For cue combination, it is desirable for the messages from 
\begin_inset Formula $\vr_{a}$
\end_inset

 and 
\begin_inset Formula $\vr_{b}$
\end_inset

 into a downstream activity 
\begin_inset Formula $\vr_{c}$
\end_inset

 such that 
\begin_inset Formula $p(x|\vr_{c})=p(x|\vr_{a},\vr_{b})$
\end_inset

.
 Suppose that this third population has response kernel 
\begin_inset Formula $\vh_{c}(s)$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2007"
literal "false"

\end_inset

 shows that message combination is possible if 
\begin_inset Formula $\vh_{c}(x)\cdot\vr_{c}=\vh_{a}(x)\cdot\vr_{a}+\vh_{b}(x)\cdot\vr_{b}$
\end_inset

.
 One way to ensure this holds is to construct the response kernels of the
 two upstream populations by
\begin_inset Formula 
\begin{equation}
\vh_{d}(x)=\mW_{d}\vh_{c}(x),\quad d\in\left\{ a,b\right\} .\label{eq:PPC_shared_basis}
\end{equation}

\end_inset

This allows cue combinations to be performed easily by
\begin_inset Formula 
\begin{equation}
\vr_{c}=\mW_{a}^{\intercal}\vr_{a}+\mW_{b}^{\intercal}\vr_{b}.\label{eq:PPC_cue_combination}
\end{equation}

\end_inset

The decoded posterior is then
\begin_inset Formula 
\[
p_{\text{PPC}}(x|\vr_{c})\propto\frac{1}{Z_{ab}(x)}\exp\left[\vh_{c}(x)\cdot\vr_{c}\right]=p_{\text{PPC}}(x|\vr_{a},\vr_{b})
\]

\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 for cue combination
\end_layout

\end_inset

 The shared basis assumption in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_shared_basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 allows 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 to implement cue combination with simple addition of the codes.
 However, it also implies that response properties of upstream neural population
s 
\begin_inset Formula $a$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 is constrained by a downstream neuron 
\begin_inset Formula $c$
\end_inset

.
 This seems counter-intuitive, as one would hope that the reverse would
 hold.
 For example, V4 neurons have wider tuning widths than V1 neurons, which
 is likely a consequence of V4 pooling responses from V1.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "BeckPouget2011Marginalization"
literal "false"

\end_inset

 considered 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 implementation of 
\series bold
adding of two random variables
\series default
 
\begin_inset Formula $x_{3}=x_{1}+x_{2}$
\end_inset

.
 This computation is performed during coordinate transformation.
 For instance, transforming eye-centred coordinates 
\begin_inset Formula $x_{1}$
\end_inset

 to head-centred coordinates 
\begin_inset Formula $x_{3}$
\end_inset

 by adding the eye position 
\begin_inset Formula $x_{2}$
\end_inset

.
 The authors assumed that all variables are Gaussian distributed, 
\begin_inset Formula $x_{i}\sim\gN(\mu_{i},\Sigma_{i})$
\end_inset

, which can be encoded by PPC using neurons with Gaussian-shaped tuning
 curves 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_gaussian_tc"
plural "false"
caps "false"
noprefix "false"

\end_inset

, giving a corresponding response kernel 
\begin_inset Formula $\vh_{d}(x)=\left(\frac{-x^{2}}{2\vp_{d}}+x\vm_{d}\right)^{\intercal}$
\end_inset

, where 
\begin_inset Formula $d\in\left\{ a,b,c\right\} $
\end_inset

 and 
\begin_inset Formula $p_{d,m}\in\sR_{+}$
\end_inset

 and 
\begin_inset Formula $b_{d,m}\in\sR$
\end_inset

 depends on 
\begin_inset Formula $\sigma_{\text{tc}}$
\end_inset

 and 
\begin_inset Formula $x_{m}$
\end_inset

.
 In particular, 
\begin_inset Formula $\vp_{d}$
\end_inset

 and 
\begin_inset Formula $\vm_{d}$
\end_inset

 decodes from 
\begin_inset Formula $\vr_{d}$
\end_inset

 the precision and the mean multiplied by the precision, respectively.
 
\end_layout

\begin_layout Standard
The sum of two Gaussians is another Gaussian with mean and variance that
 are the respective sums of the input means and precisions.
 This relationship is expressed in terms of 
\begin_inset Formula $\vr$
\end_inset

's as 
\begin_inset Formula 
\begin{equation}
\frac{\vm_{c}\cdot\vr_{c}}{\vp_{c}\cdot\vr_{c}}=\frac{\vm_{a}\cdot\vr_{a}}{\vp_{a}\cdot\vr_{a}}+\frac{\vm_{b}\cdot\vr_{b}}{\vp_{b}\cdot\vr_{b}}\quad\frac{1}{\vp_{c}\cdot\vr_{c}}=\frac{1}{\vp_{a}\cdot\vr_{a}}+\frac{1}{\vp_{b}\cdot\vr_{b}}.\label{eq:PPC_gaussian_addition}
\end{equation}

\end_inset

This is a under-determined system, as it only constrains projections of
 
\begin_inset Formula $\vr_{c}$
\end_inset

 along 
\begin_inset Formula $\vp_{c}$
\end_inset

 and 
\begin_inset Formula $\vm_{c}$
\end_inset

.
 One proposed solution is obtained by first finding orthonormal basis 
\begin_inset Formula $\vp_{c}^{\dagger}$
\end_inset

 and 
\begin_inset Formula $\vm_{c}^{\dagger}$
\end_inset

 such that 
\begin_inset Formula $\vp_{c}^{\dagger}\cdot\vp_{c}=\vm_{c}^{\dagger}\cdot\vm_{c}=1;\vp_{c}^{\dagger}\cdot\vm_{c}=\vm_{c}^{\dagger}\cdot\vp_{c}=0$
\end_inset

, and then express 
\begin_inset Formula $\vr_{c}$
\end_inset

 using these basis.
 
\begin_inset Formula 
\begin{equation}
\vr_{c}=\frac{\left[\left(\vm_{a}\cdot\vr_{a}\right)\left(\vp_{b}\cdot\vr_{b}\right)+\left(\vm_{b}\cdot\vr_{b}\right)\left(\vp_{a}\cdot\vr_{a}\right)\right]}{\vp_{a}\cdot\vr_{a}+\vp_{b}\cdot\vr_{b}}\vm_{c}^{\dagger}+\frac{\left(\vp_{a}\cdot\vr_{a}\right)\left(\vp_{b}\cdot\vr_{b}\right)}{\vp_{a}\cdot\vr_{a}+\vp_{b}\cdot\vr_{b}}\vp_{c}^{\dagger}\label{eq:PPC_divisive_subspace}
\end{equation}

\end_inset

which can be written in computationally inefficient but biologically plausible
 form by expanding the dot products 
\begin_inset Formula 
\begin{equation}
r_{c,k}=\frac{\sum_{ij}w_{ijk}r_{a,i}r_{b,j}}{\vp_{a}\cdot\vr_{a}+\vp_{b}\cdot\vr_{b}},\quad w_{ijk}=\left(m_{a,i}v_{b,j}+m_{b,i}v_{a,j}\right)m_{c,k}^{\dagger}+v_{a,i}v_{b,j}v_{c,k}^{\dagger}.\label{eq:PPC_divisive}
\end{equation}

\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Constrained space of 
\begin_inset Formula $\vr_{c}$
\end_inset


\end_layout

\end_inset

 The simpler expression 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive_subspace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 suggests that 
\begin_inset Formula $\vr_{c}$
\end_inset

 only lives on the plane defined by 
\begin_inset Formula $\vm_{c}^{\dagger}$
\end_inset

 and 
\begin_inset Formula $\vp_{c}^{\dagger}$
\end_inset

, which is a rather constrained space for neural activities.
\end_layout

\begin_layout Standard
The authors noted that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is a quadratic operation followed by divisive normalisation.
 The latter that is thought to underlie many cortical computation 
\begin_inset CommandInset citation
LatexCommand citet
key "CarandiniHeeger2012Normalization"
literal "false"

\end_inset

.
 To test how well these two computations generalise to other nonlinear tasks
 with non-Gaussian variables, they are deployed in the neural networks trained
 to computing PPC posteriors 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

.
 In each task, the parameters of the neural networks are trained by stochastic
 gradient descent to minimise the KL divergence between the true posterior
 given the neural activities and the PPC posterior decoded from the neural
 network.
 The PPC posterior is assumed to be a standard exponential family distribution
 (Gaussian, Bernoulli or von-Mises) depending on the nature of the output
 variable, with natural parameters being linearly decoded from the neural
 network's output.
 
\end_layout

\begin_layout Standard
Across tasks, it was found that a network with both quadratic operation
 and divisive normalisation can form a PPC posterior that is very close
 to the true posterior in terms of KL divergence, and was closer than networks
 without either quadratic operation or normalisation, or with ReLU nonlinearity
 instead.
 In addition, normalisation nonlinearity allows good PPC posteriors to be
 linearly decodable independent of the stimulus contrast.
 
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Small KL divergence
\end_layout

\end_inset

 Although the divisive normalisation is derived from the summation of Gaussian
 variables, it is shown empirically to be important for a few nonlinear
 problems with non-Gaussian variables.
 However, the true posterior can be well approximated by, if not equal to,
 the assumed standard exponential family distribution for each task.
 A stronger test case would be a task where the posterior is skewed or multimoda
l.
 
\begin_inset CommandInset label
LatexCommand label
name "rem:PPC_simple_posteriors"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Contrast-invariance
\end_layout

\end_inset

 The target true posterior distribution ignores the contrast by definition;
 therefore, in principle, any neural network with sufficient capacity should
 be able to learn to ignore contrast and form equally good PPC posteriors
 with a linear readout.
 The observation that divisive normalisation helps decoding regardless of
 contrast suggests that normalisation helps with contrast invariance, which
 is rather intuitive as contrast only affects the scale of the neural activity
 in all tasks.
 The experiments also suggest that quadratic operation may give the network
 more capacity than other operations compared.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Advantage of divisive normalisation
\end_layout

\end_inset

 The result (in odor recognition) that the network with divisive normalisation
 outperforms other alternatives in contrast-invariant linear decodability
 (small KL and contrast-invariance) indicates stronger approximating power
 of this neural network architecture.
 Indeed, this operation has advantages in other more challenging tasks on
 images 
\begin_inset CommandInset citation
LatexCommand citet
key "BalleSimoncelli2016Density,BalleSimoncelli2017End,LaparraSimoncelli2017Perceptually"
literal "false"

\end_inset

.
 However, one cannot conclude solely from these two observations that divisive
 normalisation is crucial for good performance on the tasks, or for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 representation and computation of variables therein.
\end_layout

\begin_layout Standard
So far, we have seen exact PPC algorithms for cue combination 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_cue_combination"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and linear addition 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which are easy to implement by two different types of neural circuits.
 However, the assumptions of shared basis in cue combination 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_shared_basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

, Gaussianity 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_gaussian_addition"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and constrained activity 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive_subspace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in addition are quite strong.
 I argue the reason for these limitations is that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 encodes distribution by natural parameters, which complicates computation.
 
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 for generic computations
\end_layout

\end_inset

 Computation over distributions represented in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 amounts to mapping a set of input natural parameters to an output natural
 parameters.
 This requires either knowing the form of the true output distribution,
 such as when adding two Gaussian variables, or making an assumption for
 such a distribution.
 For exponential family distributions, the form is specified by a sufficient
 statistics function.
 (Note that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_shared_basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in addition implicitly sets the sufficient statistics functions for the
 three distributions.) However, it is, in general, challenging to know the
 set of possible natural parameters from a given sufficient statistics function
 
\begin_inset CommandInset citation
LatexCommand citep
after "Chap. 2"
key "WainwrightJordan2008"
literal "false"

\end_inset

.
 Thus, almost all computations considered in the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature has the output distribution being one of the standard exponential
 family distributions for which the set of possible natural parameters are
 known.
 
\begin_inset CommandInset label
LatexCommand label
name "rem:PPC_generic_computations"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Deriving 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 algorithms
\end_layout

\end_inset

 To compute the downstream 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 in closed-form, the relationship between the input and output natural parameter
s must be known, such as 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_gaussian_addition"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for addition, or can be approximated, such as 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_shared_basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in cue combination.
 This is usually possible by having a likelihood 
\begin_inset Formula $p(\vr|x)$
\end_inset

 such that 
\begin_inset Formula $p_{\text{PPC}}(x|\vr)$
\end_inset

 is tractable for downstream manipulations.
 In most cases, the likelihood is assumed to be Poisson with Gaussian-shaped
 tuning curves, inducing a Gaussian 
\begin_inset Formula $p_{\text{PPC}}(x|\vr)$
\end_inset

 that facilitates mathematical derivations in natural parameters (Remark
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "rem:weak_z_dependence"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Implementing 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 in circuitry
\end_layout

\end_inset

 A Gaussian 
\begin_inset Formula $p_{\text{PPC}}(x|\vr)$
\end_inset

 may not facilitate simple and consistent neural circuit implementations.
 The expressions of output natural parameters derived are different for
 different tasks; implementing these expressions thus necessarily requires
 task-specific circuitry.
 In some cases, some operations are not known to be biologically plausible
 and requires further approximations.
 (Recall that the plausible quadratic operation and divisive normalisation
 are derived in addition of Gaussian variables.)
\begin_inset CommandInset label
LatexCommand label
name "rem:PPC_generic_implementations"

\end_inset


\end_layout

\begin_layout Standard
I now briefly go through PPC implementations of three additional tasks to
 illustrate Remarks 
\begin_inset CommandInset ref
LatexCommand ref
reference "rem:PPC_generic_computations"
plural "false"
caps "false"
noprefix "false"

\end_inset

 to 
\begin_inset CommandInset ref
LatexCommand ref
reference "rem:PPC_generic_implementations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Common to all the three tasks, the desired posterior is binary for which
 the natural parameter is known: the log odds ratio.
 This trivial case requires no posterior assumptions.
\end_layout

\begin_layout Standard
In 
\series bold
visual search 
\series default

\begin_inset CommandInset citation
LatexCommand citep
key "MaPouget2011Behavior"
literal "false"

\end_inset

, the stimulus consists of 
\begin_inset Formula $N$
\end_inset

 oriented bars presented at 
\begin_inset Formula $N$
\end_inset

 locations on a screen.
 On each trial, with probability 0.5, one of the bars is chosen to be the
 target (
\begin_inset Formula $T=1$
\end_inset

) with a fixed orientation 
\begin_inset Formula $s_{T}$
\end_inset

 across trials.
 All other bars are distractors with orientations i.i.d.
 as 
\begin_inset Formula $p_{d}(s)$
\end_inset

 on each trial.
 The task is to report whether the target exists among the 
\begin_inset Formula $N$
\end_inset

 bars.
 Suppose that each stimulus 
\begin_inset Formula $s$
\end_inset

 generates a neural pattern 
\begin_inset Formula $\vr_{n}$
\end_inset

 with tuning kernel 
\begin_inset Formula $\vh_{n}$
\end_inset

 for the 
\begin_inset Formula $n$
\end_inset

'th location.
 The 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 of the desired posterior distribution is 
\begin_inset Formula 
\[
d=\log\frac{p\left(T=1|\left\{ \vr_{n}\right\} _{n=1}^{N}\right)}{p\left(T=0|\left\{ \vr_{n}\right\} _{n=1}^{N}\right)},
\]

\end_inset

which can be expressed in terms of 
\begin_inset Formula $\left\{ \vr_{n}\right\} _{n=1}^{N}$
\end_inset

 as
\begin_inset Formula 
\begin{equation}
d=\log\frac{1}{N}\sum_{n=1}^{N}e^{d_{n}},\quad d_{n}=\vh_{n}(s_{T})\cdot\vr_{n}-\log\int_{0}^{\pi}\exp\left[\vh_{n}(x_{n})\cdot\vr_{n}\right]dp_{d}(x_{n}).\label{eq:PPC_visual_search}
\end{equation}

\end_inset

Intuitively, the decision variable 
\begin_inset Formula $d$
\end_inset

 is the log of the average probability ratio that the target 
\begin_inset Formula $s_{T}$
\end_inset

 is present at the 
\begin_inset Formula $n$
\end_inset

'the location.
 The logarithm in the expression for 
\begin_inset Formula $d_{n}$
\end_inset

 is biologically implausible.
 If 
\begin_inset Formula $p_{d}(x_{n})$
\end_inset

 is a delta function at 
\begin_inset Formula $x_{D}$
\end_inset

, then 
\begin_inset Formula $d_{n}=\left[\vh_{n}(x_{T})-\vh_{n}(x_{D})\right]\cdot\vr_{n}$
\end_inset

, and 
\begin_inset Formula $e^{d}$
\end_inset

 can be implemented easily by neural circuitry.
 However, for a generic 
\begin_inset Formula $p_{d}(x_{n})$
\end_inset

, the integral is intractable, and it is not known how neural circuits can
 approximate it.
 
\end_layout

\begin_layout Standard
In 
\series bold
visual categorisation
\series default
, the stimuli are Gabor filters generated with orientations drawn from a
 mixture of two Gaussians 
\begin_inset Formula $p(x|C=1)=\gN\left(x|0,\sigma_{1}^{2}\right)$
\end_inset

 and 
\begin_inset Formula $p(x|C=2)=\gN\left(x|0,\sigma_{2}^{2}\right)$
\end_inset

, where 
\begin_inset Formula $\sigma_{1}<\sigma_{2}$
\end_inset

.
 Subjects need to decide from which Gaussian the presented orientation is
 drawn.
 The fact that the two Gaussians are largely overlapping makes this task
 difficult and suitable for probing uncertainty representation.
 Assuming that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_total_firing_rate"
plural "false"
caps "false"
noprefix "false"

\end_inset

 holds and that neurons are Poisson with Gaussian-shaped tuning curves,
 
\begin_inset Formula $\vh(x)=\left(\frac{-x^{2}}{2}\vv+x\vm\right)^{\intercal}$
\end_inset

, the relevant PPC code can be shown to be 
\begin_inset Formula 
\begin{equation}
d=\frac{p(C=1|\vr)}{p(C=2|\vr)}=\frac{1}{2}\log\frac{1+\sigma_{2}^{2}\vv\cdot\vr}{1+\sigma_{1}^{2}\vv\cdot\vr}-\frac{\left(\sigma_{2}^{2}-\sigma_{1}^{2}\right)\left(\vm\cdot\vr\right)^{2}}{2\left(1+\sigma_{1}^{2}\vv\cdot\vr\right)\left(1+\sigma_{2}^{2}\vv\cdot\vr\right)}.\label{eq:PPC_visual_categorisation}
\end{equation}

\end_inset

The implausible logarithm appears in this expression.
\end_layout

\begin_layout Standard
In 
\series bold
causal inference 
\series default

\begin_inset CommandInset citation
LatexCommand citep
key "MaRahmati2013Towards"
literal "false"

\end_inset

, subjects receive two stimuli 
\begin_inset Formula $x_{1},x_{2}\in\sR^{2}$
\end_inset

 and need to decide whether they have a common cause, a task that resembles
 whether subjects show the ventriloquist effect 
\begin_inset CommandInset citation
LatexCommand citep
key "HowardTempleton1966Human"
literal "false"

\end_inset

.
 In half of the trials, the two stimuli are generated by a common source
 
\begin_inset Formula $x_{1}=x_{2}=x$
\end_inset

, producing neural responses 
\begin_inset Formula $\vr_{1}$
\end_inset

 and 
\begin_inset Formula $\vr_{2}$
\end_inset

 with likelihood 
\begin_inset Formula 
\[
p(\vr_{1},\vr_{2}|C=1)=\int p_{r}(\vr_{1}|x)p_{r}(\vr_{2}|x)p_{x}(x)ds,
\]

\end_inset

 where 
\begin_inset Formula $C=1$
\end_inset

 indicates common cause, 
\begin_inset Formula $p_{r}$
\end_inset

 is the likelihood of neural response, and 
\begin_inset Formula $p_{x}$
\end_inset

 is the stimulus distribution.
 In the other half of the trials, the two stimuli are generated by two independe
nt causes 
\begin_inset Formula $x_{1},x_{2}\stackrel{iid}{\sim}p_{x}(x)$
\end_inset

 with likelihood 
\begin_inset Formula 
\[
p(\vr_{1},\vr_{2}|C=0)=\left(\int p_{r}(\vr_{1}|x_{1})p_{x}(x_{1})dx_{1}\right)\left(\int p_{r}(\vr_{2}|x_{2})p_{x}(x_{2})dx_{2}\right).
\]

\end_inset

The integrals in these likelihoods are intractable for generic 
\begin_inset Formula $p_{x}$
\end_inset

.
 However, by assuming again that 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_total_firing_rate"
plural "false"
caps "false"
noprefix "false"

\end_inset

 holds and that 
\begin_inset Formula $p_{r}(\vr|x)$
\end_inset

 is Poisson with Gaussian-shaped tuning curves as in Remark 
\begin_inset CommandInset ref
LatexCommand eqref
reference "rem:weak_z_dependence"
plural "false"
caps "false"
noprefix "false"

\end_inset

, 
\begin_inset Formula $p_{r}(x|\vr)$
\end_inset

 is an unnormalised Gaussian, and those likelihoods can be computed in closed-fo
rm.
 The relevant log odds is 
\begin_inset Formula 
\[
d=\log\frac{p(C=1|\vr_{1},\vr_{2})}{p(C=0|\vr_{1},\vr_{2})},
\]

\end_inset

which expands to 
\begin_inset Formula 
\[
d=\frac{z_{11}z_{21}}{z_{12}+z_{22}+J_{s}}-\frac{1}{2}\left[\frac{z_{22}z_{11}^{2}}{\left(z_{12}+J_{s}\right)\left(z_{12}+z_{22}+J_{s}\right)}-\frac{z_{12}z_{21}^{2}}{\left(z_{22}+J_{s}\right)\left(z_{12}+z_{22}+J_{s}\right)}+\log\left(1+\frac{z_{12}z_{22}}{J_{s}(z_{12}+z_{22}+J_{s})}\right)\right],
\]

\end_inset

where 
\begin_inset Formula $J_{s}$
\end_inset

 depends on the tuning width 
\begin_inset Formula $\sigma_{\text{tc}}$
\end_inset

, and 
\begin_inset Formula $z$
\end_inset

's are linear projections of 
\begin_inset Formula $\vr$
\end_inset

 along directions that depends on the tuning curves parameter.
 The implausible logarithm appears again.
 As the authors noted 
\begin_inset CommandInset citation
LatexCommand citep
after "Section 6"
key "MaRahmati2013Towards"
literal "false"

\end_inset

, the neural implementation of this equation is possible but very complicated.
\end_layout

\begin_layout Standard
In summary, for the three binary decision tasks, the expressions for PPC
 posterior are different for the tasks, suggesting drastically different
 neural circuitry for tasks involving very simple stimulus distributions.
 Also, all expressions involve logarithm, a strange nonlinearity for neural
 circuits to perform.
 
\end_layout

\begin_layout Subsubsection
Neural filtering algorithms
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "WilsonFinkel2009"
literal "true"

\end_inset

 modified the line attractor network for head direction cells initially
 proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang1996"
literal "true"

\end_inset

 to implement the Kalman filter; in the case of constant, noiseless innovation
 (fixed drifting speed) and small observation noise, this network is shown
 to approximate the Kalman filtering equations for the mean and variance
 for 1-dimensional linear Gaussian system; for larger noises, the experiments
 show that it is able to adapt to change points much quicker than the Kalman
 filter.
 The neurons show higher firing rate when the underlying latent variable
 has higher precision, consistent with recordings
\begin_inset CommandInset citation
LatexCommand citet
key "SclarFreeman1982"
literal "true"

\end_inset

.
 In order to implement the dynamics, the speed in the latent variable enters
 through a parameter in the weight matrices connecting all neurons, which
 is not very biologically plausible, although some justifications were provided.
 
\end_layout

\begin_layout Standard
The neural particle filter (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

)
\begin_inset CommandInset citation
LatexCommand citep
key "KutschireiterEtAl2017"
literal "true"

\end_inset

 is a sample-based filtering algorithm for continuous time 
\begin_inset ERT
status open

\begin_layout Plain Layout

gls{SSM}
\end_layout

\end_inset

:
\begin_inset Formula 
\begin{align*}
d\vz_{t} & =\vf(\vz_{t})+\mSigma_{z}^{1/2}d\vomega_{t}^{(z)}\\
d\vx_{t} & =\vg(\vz_{t})+\mSigma_{x}^{1/2}d\vomega_{t}^{(x)}
\end{align*}

\end_inset

where 
\begin_inset Formula $\vomega_{t}^{(\cdot)}$
\end_inset

 is the standard Brownian motion.
 The filtered posterior is represented by a set of unweighted samples 
\begin_inset Formula $\left\{ \vz_{t}^{(s)}\right\} _{s}$
\end_inset

, and the evolution of each particle is motivated by the Kushner equation
 given below, denoting 
\begin_inset Formula $\E{p(\vz_{t}|\vx_{1:t})}{\cdot}$
\end_inset

 by 
\begin_inset Formula $\langle\cdot\rangle$
\end_inset

 and let 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 be an arbitrary function on 
\begin_inset Formula $\vz$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
d\langle\phi\rangle=\langle\vf\rangle dt+\text{cov}(\phi,\vg)\mSigma_{x}^{-1}\left(d\vx_{t}-\left\langle \vg\right\rangle dt\right)\label{eq:NPF_kushner}
\end{equation}

\end_inset

.
 When 
\begin_inset Formula $\vg$
\end_inset

 is linear and 
\begin_inset Formula $\phi(\vz)=\vz$
\end_inset

, it recovers the SDE governing the posterior mean in continuous-time Kalman
 filtering.
 Although this equation is exact, we still do not know the higher moments
 
\begin_inset Formula $\text{cov}(\phi,\vg)$
\end_inset

 and thus direct implementation is impossible (closure problem).
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 proposes to remedy this issue with two design choices.
 First, each particle 
\begin_inset Formula $\vz_{t}^{(s)}$
\end_inset

 evolves according to 
\begin_inset Formula 
\begin{equation}
d\vz_{t}^{(s)}=\vf(\vz_{t}^{(s)})dt+\mW_{t}\left(d\vx_{t}-\left\langle \vg\right\rangle dt\right)+\mSigma_{x}^{1/2}d\vomega_{t}^{(z)}\label{eq:NPF_sample_evolution}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mW_{t}=\hat{\text{cov}}(\vz,\vg)\mSigma_{x}^{-1}$
\end_inset

 with 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 begin the empirically estimated using all samples.
 We can see 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 is similar to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_kushner"

\end_inset

 when 
\begin_inset Formula $\phi(\vz)=\vz$
\end_inset

 except for the last noise term.
 Therefore, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 tries to avoid the closure problem by employing a population of samples
 that follow the trajectory for the mean, replacing 
\begin_inset Formula $\text{cov}(\vz,\vg)$
\end_inset

 with the sample estimate.
 
\end_layout

\begin_layout Standard
By avoiding the need for particle weights as in common particle filters,
 it is made biologically plausible and a neural circuits implementation
 was proposed.
 By training the network weights using maximum likelihood, the model was
 able to learn the parameters of the observations density.
 The latent dynamics was not considered as joint samples of adjacent time
 steps were not considered.
 The experiments show that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 avoided the 
\begin_inset Quotes eld
\end_inset

curse of dimensionality
\begin_inset Quotes erd
\end_inset

, requires much fewer particles than the conventional bootstrap particle
 filter for higher-dimensional problems.
\end_layout

\begin_layout Standard
It remains a question as to why the 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 worked at all, as it is not obvious why (approximately) following the trajector
y of the mean allows the particles to be distributed correctly.
 Intuitively, the first term on the RHS of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 is the dynamics contribution, the second term is the correction from observatio
n.
 
\begin_inset Formula $\mW_{t}$
\end_inset

 would be the correct gain if 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 were exact, and the amount of correction is determined by the posterior
 variance.
 If the third noise term is absent and the particles are initially correctly
 distributed, then all the particles would move closer to the mean after
 infinitesimal amount of filtering time, and 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 is then an underestimate of the true covariance, resulting in a smaller
 correction for future time.
 Since there is a noise term, 
\begin_inset Formula $\left\{ \vz_{t}^{(s)}\right\} _{s}$
\end_inset

 are pushed away from the mean which increases 
\begin_inset Formula $\text{cov}(\phi,\vg)$
\end_inset

 and hence the correction.
 The correct amount of 
\begin_inset Quotes eld
\end_inset

push
\begin_inset Quotes erd
\end_inset

 is the uncertainty induced by data and likely correlated, reflecting explaining
-away, instead of the independent prior noise.
 Thus, it may be the case that the full distribution is different from the
 true posterior, but the posterior mean estimated from these particles are
 close to the correct value.
 The authors did not show full distribution of the posteriors.
\end_layout

\begin_layout Paragraph
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 implementation of the Kalman filter 
\end_layout

\begin_layout Paragraph
Recurrent exponential family harmonium (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
Hard-wiring the neurons to implement/approximate optimal statistical computation
s, as done in some 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature, 
\begin_inset CommandInset citation
LatexCommand citep
key "BeckEtAl2007,BeckEtAl2011"
literal "true"

\end_inset

, is not biologically plausible, as the brain does not receive such fine-grained
 supervisions to construct the wiring.
 
\end_layout

\begin_layout Standard
Viewing inference and learning as a density estimation operation, 
\begin_inset CommandInset citation
LatexCommand citet
key "MakinEtAl2013"
literal "true"

\end_inset

 proposed to use the exponential family harmonium 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, a restricted Boltzmann machine-like fully-connected graphical model, to
 implement a 
\emph on
model
\emph default
 joint distribution of observation 
\begin_inset Formula $\vX$
\end_inset

 and latent code 
\begin_inset Formula $\vR$
\end_inset

, 
\begin_inset Formula $q(\vx,\vr)$
\end_inset

.
 
\begin_inset Formula 
\begin{align*}
q(\vr|\vx) & =\prod_{i=1}^{\Dim R}\Bern\left[r_{i}|\sigm\left(\mM_{RX}\vx+\vtheta_{R}\right)\right]\\
q(\vx|\vr) & =\prod_{i=1}^{\Dim X}\Pois\left[x_{i}|\exp\left(\mM_{RX}^{\intercal}\vr+\vtheta_{X}\right)\right]
\end{align*}

\end_inset

The choices of 
\begin_inset Formula $\sigm$
\end_inset

 and 
\begin_inset Formula $\exp$
\end_inset

 are the canonical link functions for Bernoulli and Poisson, respectively,
 that allow modulation of the natural parameters through linear transformation
 of the conditioning variable.
 The latent code is a sample drawn from 
\begin_inset Formula $\vR|\vx$
\end_inset

, or just its mean rate.
 In fact, the observation 
\begin_inset Formula $\vX$
\end_inset

 is spike count generated by Poisson neurons with Gaussian tuning functions
 over some unobserved variable 
\begin_inset Formula $\vZ$
\end_inset

, yielding a 
\emph on
generative
\emph default
 joint
\emph on
 
\emph default
distribution 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

.
 Learning entails maximizing the spike count log likelihood 
\begin_inset Formula $\E{p(\vx)}{\log q(\vx)}$
\end_inset

 using the standard contrastive divergence for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, similar to one more commonly known for training restricted Boltzmann machines.
 
\end_layout

\begin_layout Standard
Now it is clear that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 is a density estimation approach to model 
\begin_inset Formula $p(\vx)$
\end_inset

.
 Different from other density estimation approach to model natural stimuli
 
\begin_inset CommandInset citation
LatexCommand citep
key "Lewicki2002,LewickiOlshausen1999,LewickiSejnowski2000"
literal "true"

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 is a fully connected graph rather than a directed graph, and inference
 produces conditionally independent 
\emph on
code
\emph default
 
\begin_inset Formula $\vr$
\end_inset

 rather than the latent variables 
\begin_inset Formula $\vr$
\end_inset

.
 However, the latent code 
\begin_inset Formula $\vr$
\end_inset

 is some abstract feature to describe 
\begin_inset Formula $\vx$
\end_inset

 and is not associated directly with the latent variables 
\begin_inset Formula $\vZ$
\end_inset

 in the generative model, and thus the uncertainty in the posterior cannot
 be interrogated straightforwardly.
 Indeed, to evaluate the performance of this model by decoding, 
\begin_inset Formula $\vr$
\end_inset

 is first used to evaluate the conditional expected spike count 
\begin_inset Formula $\E{q(\vx|\vr)}{\vx}$
\end_inset

 under the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 model, assuming there is no loss of information; then, the mean and covariance
 of 
\begin_inset Formula $\vz$
\end_inset

 are approximately decoded according to the generative joint 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

, pretending
\begin_inset Formula $\E{q(\vx|\vr)}{\vx}$
\end_inset

 is evaluated under 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

.
 Using this approach, one can in principle decoded posterior distribution
 that are more complicated than a Gaussian using, for example, histograms.
\end_layout

\begin_layout Standard
Later on, the same authors 
\begin_inset CommandInset citation
LatexCommand citet
key "MakinEtAl2015"
literal "true"

\end_inset

 applied the same principle to build a recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 for filtering.
 In this model, the latent code from the previous time step 
\begin_inset Formula $\vR_{t-1}$
\end_inset

 and the spike observation 
\begin_inset Formula $\vX_{t}$
\end_inset

 are concatenated, which is used to form an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 with a latent code distribution 
\begin_inset Formula $\vR_{t}$
\end_inset

.
 The latent code for next time step is then the conditional sample 
\begin_inset Formula $\vr_{t}\sim\vR_{t}|\vr_{t-1},\vx_{t}$
\end_inset

, and the same construction of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 continues, giving the recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

.
 The learning algorithm is the 1-step contrastive divergence performed at
 each time step, which maximises the log likelihood of 
\begin_inset Formula $\left[\vr_{t-1},\vx_{t}\right]$
\end_inset

, treating the previous latent code 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 as an 
\emph on
observation
\emph default
.
 
\end_layout

\begin_layout Standard
Although the learning rule is local and thus biologically plausible, the
 underlying cost function is unclear; as the authors also pointed out, the
 justification for treating 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 as an observation is missing.
 By copying 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 to the next time step, no latent dynamics is explicitly modelled and needs
 to be learned by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 at each time step.
 As the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 also needs to incorporate new evidence, it is conceivable that the class
 of transition dynamics and emission may be restrictive.
 Indeed, the experiments conducted all had linear Gaussian latent dynamics.
\end_layout

\begin_layout Paragraph
PPC Bayes rule and EFH (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset

 proposed a neural filtering algorithm that combines 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "WellingEtAl2005"
literal "true"

\end_inset

.
 By extensive use of properties of the exponential family distributions,
 the theory suggests that evidence from observation can be more or less
 exactly incorporated into the belief of the latent, that is, Bayes rule
 can be implemented exactly.
 Here, I refer to this approach as the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule with 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

).
 Forming the prior for the next time step from the previous belief requires
 knowledge about the transition dynamics, which is not explicitly modeled;
 nonetheless, the mapping of natural parameter of the posterior 
\begin_inset Formula $p(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

 to that of the predictive distribution 
\begin_inset Formula $p(\vz_{t}|\vx_{1:t-1})$
\end_inset

 was modeled as a neural network function.
 Although the neural network needs to be trained by the non-plausible back-propa
gation, and only Gaussian posterior is allowed in practice, the theory itself
 is interesting and is thoroughly reviewed here.
 
\end_layout

\begin_layout Standard
This framework has two parts.
 In the first part, a relationship between sensory input 
\begin_inset Formula $\rvfont Z$
\end_inset

 and Poisson spike count 
\begin_inset Formula $\rvfont X$
\end_inset

 is constructed so that the prior and posterior of 
\begin_inset Formula $\rvfont Z$
\end_inset

 live in the same exponential family; the second part describes how the
 prior and posterior can be encoded by the two different neural populations,
 using natural parameters
\begin_inset Formula $\vtheta_{t}$
\end_inset

, and the belief update on these natural parameters.
 In short, the purpose for conjugacy is to maintain consistency of neural
 representation as PPC for the latent variable.
\end_layout

\begin_layout Subparagraph
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\end_layout

\begin_layout Standard
In first part, a conjugacy between prior and posterior 
\begin_inset Formula $z$
\end_inset

 (consider a scalar latent for simplicity) with Poisson likelihood 
\begin_inset Formula $p(\vx|z)$
\end_inset

 is established.
 Let the prior of sensory input 
\begin_inset Formula $z$
\end_inset

 follow some generic exponential family distribution with sufficient statistics
 
\begin_inset Formula $\vpsi(z)$
\end_inset

 
\begin_inset Formula 
\begin{equation}
p(z)=\frac{1}{Z_{Z}(\vtheta_{Z})}\nu_{Z}(z)\exp\left[\vtheta_{Z}\cdot\vpsi(z)\right]\label{eq:NBR_prior}
\end{equation}

\end_inset

denote the set of exponential family distributions with sufficient statistics
 
\begin_inset Formula $\vpsi$
\end_inset

 to be 
\begin_inset Formula $\sM_{\psi}$
\end_inset

.
 These inputs generate neural responses 
\begin_inset Formula $N$
\end_inset

 that is assumed to be independent Poisson conditioned on a particular 
\begin_inset Formula $Z=z$
\end_inset

 with Gaussian-shaped tuning curves 
\begin_inset Formula 
\[
f_{i}(z)=\gamma\exp\left(-\frac{\left(z-z_{i}^{0}\right)^{2}}{2\sigma^{2}}\right),\ i\in\N_{\Dim X},
\]

\end_inset

where homogeneous gain 
\begin_inset Formula $\gamma$
\end_inset

 and tuning width 
\begin_inset Formula $\sigma$
\end_inset

.
 The likelihood of a given firing pattern 
\begin_inset Formula $\vx$
\end_inset

 is
\begin_inset Formula 
\begin{align}
p(\vx|z) & =\prod_{i=1}^{\Dim X}\exp\left[-f_{i}(z)\right]\frac{f_{i}(z)^{x_{i}}}{x_{i}!}\nonumber \\
 & =\frac{1}{\exp\left(\sum_{i=1}^{\Dim X}f_{i}(z)\right)\prod_{i=1}^{\Dim X}(x_{i}!)}\exp\left[\sum_{i=1}^{\Dim X}\log f_{i}(z)x_{i}\right]\nonumber \\
 & =\frac{1}{\exp\left(\Phi_{X}(z)\right)}\nu_{X}(\vx)\exp\left[\vpsi(z)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right]\label{eq:NBR_poisson}
\end{align}

\end_inset

where 
\begin_inset Formula $\vpsi(z)$
\end_inset

, 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 are related to the tuning functions 
\begin_inset Formula $f_{i}$
\end_inset


\begin_inset Formula 
\begin{align*}
\nu_{X}(\vx) & =1/\prod_{i=1}^{\Dim X}x_{i}!\\
\vpsi(z) & =\left[z;z^{2}\right]\\
\mM_{ZX} & =\left[\frac{z^{0\intercal}}{\sigma^{2}};-\frac{\mathbf{\mathbf{1}}^{\intercal}}{2\sigma^{2}}\right]\\
\vtheta_{X} & =\left[\log\gamma-\frac{\left(z^{0}\right)^{2}}{2\sigma^{2}}\right],
\end{align*}

\end_inset

and 
\begin_inset Formula $\Phi_{X}(z)$
\end_inset

 is the log normaliser of 
\begin_inset Formula $p(\vx|z)$
\end_inset


\begin_inset Formula 
\begin{equation}
\Phi_{X}(z)=\sum_{i=1}^{\Dim X}f_{i}(z)=\log\sum_{\vx}\nu_{X}(\vx)\exp\left(\vpsi(z)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right)\label{eq:NBR_poisson_logZ}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 denotes the the weights for 
\emph on
interaction
\emph default
 of 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

 .
 Together with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_prior"

\end_inset

, 
\begin_inset Formula $p(\vx|z)$
\end_inset

 defines a joint distribution between 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

.
 
\end_layout

\begin_layout Standard
Now we would like the posterior 
\begin_inset Formula $p(z|\vx)$
\end_inset

 to be conjugate, meaning that it is also in 
\begin_inset Formula $\sM_{\psi}$
\end_inset

.
 The author proposes a form of density function of
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $\vN$
\end_inset

 that is an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 permitting Poisson 
\begin_inset Formula $p(\vx|z)$
\end_inset

 
\begin_inset Formula 
\[
kp_{0}(z,\vx)\propto\nu_{Z}(z)\nu_{X}(\vx)\exp\left[\vtheta_{Z}\cdot\vpsi(z)+\vpsi(z)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right]
\]

\end_inset

which captures the relationship between the observation and neural spiking.
 Using 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_poisson_logZ"

\end_inset

, we can sum out 
\begin_inset Formula $\vx$
\end_inset

 and obtain (unnormalised) 
\begin_inset Formula $p_{0}(z)$
\end_inset

,
\begin_inset Formula 
\begin{align}
p_{0}(z) & \propto\nu_{Z}(z)\exp\left[\vtheta_{Z}\cdot\vpsi(z)+\Phi_{X}(z)\right]\label{eq:NBR_true_prior}\\
p_{0}(\vx) & \propto\nu_{X}(\vx)\exp\left[\vtheta_{X}\cdot\vx+\log Z_{Z}(\vtheta_{Z}+\mM_{ZX}\cdot\vx)\right].\label{eq:NBR_true_lik}
\end{align}

\end_inset

The implied posterior distribution is
\begin_inset Formula 
\begin{equation}
p_{0}(z|\vx)\propto\nu_{Z}(z)\exp\left(\vpsi(z)\cdot\left[\mM_{ZX}\cdot\vx+\vtheta_{Z}\right]\right)\label{eq:NBR_true_posterior}
\end{equation}

\end_inset

and 
\begin_inset Formula $p_{0}(z|\vx)\in\sM$
\end_inset

.
 Thus, 
\begin_inset Formula $p_{0}(z)$
\end_inset

 is almost in 
\begin_inset Formula $\sM_{\psi}$
\end_inset

 but off by 
\begin_inset Formula $\Phi_{X}(z)=\sum_{i=1}^{\Dim X}f_{i}(z)$
\end_inset

 in the log density.
 If one assumes that this sum, which is the total firing rate for any given
 
\begin_inset Formula $z$
\end_inset

, is a constant, 
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{\Dim X}f_{i}(z)=\lambda\label{eq:NBR_const_total_rate}
\end{equation}

\end_inset

then 
\begin_inset Formula $p_{0}\left(z,\vx\right)=p\left(z,\vx\right)$
\end_inset

, and we obtain a model that has the following properties:
\end_layout

\begin_layout Enumerate
the prior 
\begin_inset Formula $p(z)\in\sM_{\psi}$
\end_inset


\end_layout

\begin_layout Enumerate
the likelihood 
\begin_inset Formula $p(\vx|z)$
\end_inset

 is independent Poisson with mean rate depending on Gaussian tuning functions
 
\begin_inset Formula $f_{i}$
\end_inset


\end_layout

\begin_layout Enumerate
the posterior 
\begin_inset Formula $p(z|\vx)\in\sM_{\psi}$
\end_inset


\end_layout

\begin_layout Standard
Given these properties, the next question is how to encode these distributions
 into neural populations.
 Following PPC, and assuming that 
\begin_inset Formula $\sum_{i=1}^{\Dim X}f_{i}(z)$
\end_inset

 is a constant, two separate populations with rates 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 encode the prior 
\begin_inset Formula $p_{0}(z)$
\end_inset

 and posterior 
\begin_inset Formula $p_{0}(z|\vx)$
\end_inset

, respectively, about 
\begin_inset Formula $z$
\end_inset

.
 
\begin_inset Formula 
\begin{align}
q(z) & \propto\exp\left[\vpsi(z)\cdot\mW_{Z}\cdot\vr_{Z}\right]\label{eq:NBR_ppc_prior}\\
q(z|\vx) & \propto\exp\left[\vpsi(z)\cdot\mW_{Z|X}\cdot\vr_{Z|X}\right]\label{eq:NBR_ppc_posterior}
\end{align}

\end_inset

This encoding allows more neurons than parameters to encode a distribution,
 and thus allows noise in the representation.
 For exact Bayes rule, it is desirable for 
\begin_inset Formula $q(z|\vr_{X})=p(z)$
\end_inset

, 
\begin_inset Formula $q(z|\vr_{Z|X})=p(z|\vx)$
\end_inset

 and 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 to be a function of 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

.
 By matching the natural parameters between 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_prior"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior"

\end_inset

 (taking into account 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

), 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_posterior"

\end_inset

and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_posterior"

\end_inset

, we have
\begin_inset Formula 
\begin{align}
\mW_{Z}\cdot\vr_{Z} & =\vtheta_{Z}\label{eq:NBR_ppc_prior_np}\\
\mW_{Z|X}\cdot\vr_{Z|X} & =\vtheta_{Z}+\mM_{ZX}\cdot\vx
\end{align}

\end_inset

which gives 
\begin_inset Formula 
\[
\mW_{Z|X}\cdot\vr_{Z|X}=\mW_{Z}\cdot\vr_{Z}+\mM_{ZX}\cdot\vx.
\]

\end_inset

To express 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 in terms of 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

, the author further assumes (akin to 
\begin_inset CommandInset citation
LatexCommand citep
key "BeckEtAl2007"
literal "true"

\end_inset

) that 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\mW_{Z}$
\end_inset

 are related to 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 through linear recoding
\begin_inset Formula 
\begin{align}
\mM_{ZX} & =\mW_{Z|X}\mfont V_{X}\nonumber \\
\mW_{Z} & =\mW_{Z|X}\mfont V{}_{Z}\label{eq:NBR_W_map}
\end{align}

\end_inset

Finally, the encoding for the posterior 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 can be expressed in terms of prior encoding 
\begin_inset Formula $\vr_{X}$
\end_inset

 and the number of spike
\begin_inset Formula $\vx$
\end_inset


\begin_inset Formula 
\[
\vr_{Z|X}=\mfont V_{X}\vx+\mfont V{}_{Z}\vr_{Z}
\]

\end_inset

which is the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\begin_inset Foot
status open

\begin_layout Plain Layout
referred to as neural Bayes rule by 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset


\end_layout

\end_inset

.
 If the same encoding is used for prior and posterior, meaning 
\begin_inset Formula $\mW_{Z}=\mW_{Z|X}$
\end_inset

, then 
\begin_inset Formula $\mfont V{}_{Z}$
\end_inset

 is the identity.
 Compared to 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 framework proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2007"
literal "true"

\end_inset

, the present formulation differs in that
\end_layout

\begin_layout Enumerate
the prior 
\begin_inset Formula $p(z)$
\end_inset

 is explicitly represented by a neural population
\end_layout

\begin_layout Enumerate
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

 are connected by a joint distribution in the form of an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We note here that the observations 
\begin_inset Formula $\vx$
\end_inset

 is implied by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 joint and hence lives in the exponential family, which can be restrictive.
 Also, the assumption for constant firing rate 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

 may not hold for tuning curves that are asymmetric (e.g.
 sigmoidal), and holds as a result of using symmetric tuning functions and
 Poisson neurons; indeed, as with previous work on 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

, Gaussian tuning curves 
\begin_inset Formula $f_{i}$
\end_inset

 are used in all experiments, which means that the prior and posteriors
 are all Gaussian, and 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_poisson"

\end_inset

 are determined by parameters in 
\begin_inset Formula $f_{i}$
\end_inset

.
 While it may be the case that other choices of tuning curve allows more
 flexible prior and posterior distributions, this is largely unexplored
 in the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature.
 
\end_layout

\begin_layout Subparagraph
Forward inference and learning using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\end_layout

\begin_layout Standard
In filtering, we would like to recursively infer the distribution of latent
 state 
\begin_inset Formula $\vz_{t}$
\end_inset

 given all Poisson observation 
\begin_inset Formula $\vx_{1:t}$
\end_inset

, whose belief is represented by rate 
\begin_inset Formula $\vr_{t|t}:=\vr_{Z_{t}|X_{1:t}}$
\end_inset

.
 The unknown latent dynamics should also be updated to incoming statistics.
\end_layout

\begin_layout Standard
For inference, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule facilitates evidence incorporation that updates 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

 to 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t})$
\end_inset

 at each time 
\begin_inset Formula $t$
\end_inset

, but the step from the previous posterior 
\begin_inset Formula $q(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

 to the predictive distribution 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

 is missing; that is, in terms of PPC, how to map from 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 to 
\begin_inset Formula $\vr_{t|t-1}$
\end_inset

.
 If the true latent process is Markovian, then there exists a mapping from
 
\begin_inset Formula $q(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

 to 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

, which is a multiplication by the true transition dynamics 
\begin_inset Formula $p(\vz_{t}|\vz_{t-1})$
\end_inset

 followed by marginalisation of 
\begin_inset Formula $\vz_{t-1}$
\end_inset

.
 It may be possible to realise these operations in the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 code, so 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset

 proposes to train a generic neural network 
\begin_inset Formula $\phi(\cdot;\mW_{\phi})$
\end_inset

, parameterised by 
\begin_inset Formula $\mW_{\phi}$
\end_inset

, to perform approximate this mapping.
 
\begin_inset Formula $\mW_{\phi}$
\end_inset

 can then be seen as a generative parameter that approximates the unknown
 true dynamics, which may be nonlinear and non-Gaussian.
 The predictive distribution still needs to be well approximated by a Gaussian
 as required by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule.
\end_layout

\begin_layout Standard
For learning, first note that some parameters are restricted.
 Given the Gaussian tuning curve restrictions, 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 are determined by tuning parameters of 
\begin_inset Formula $f_{i}$
\end_inset

.
 The authors set 
\begin_inset Formula $\mW_{Z}=\mW_{Z|X}$
\end_inset

, so
\begin_inset Formula $\mfont V{}_{Z}$
\end_inset

 is the identity matrix.
 So the only free parameters are 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 and 
\begin_inset Formula $\mW_{\phi}$
\end_inset

.
 In the experiment of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset

, the author fixes 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 to one of the two choices:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mW_{Z|X}=\mM_{ZX}$
\end_inset

 (naive code), implying projections along the centres of the Gaussian tuning
 functions; 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 with rows that are orthonormal to each other and also to the 1-vector (orthonor
mal code), as laid out in 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2011"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The objective for online learning is the conditional likelihood 
\begin_inset Formula $q(\vx_{t}|\vx_{1:t-1})$
\end_inset

, which is similar to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior_np"

\end_inset

 but with prior natural parameter 
\begin_inset Formula $\vtheta_{Z}$
\end_inset

 encoded by 
\begin_inset Formula $\vr_{t|t-1}=\phi(\vr_{t-1|t-1};\mW_{\phi})=:\phi_{t-1}(\mW_{\phi})$
\end_inset

 using 
\begin_inset Formula $\mW_{Z}$
\end_inset

.
 Substituting this into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior_np"

\end_inset

 and then into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_lik"

\end_inset

, and adding the time indices, the likelihood is 
\begin_inset Formula 
\begin{align*}
q(\vx_{t}|\vx_{1:t-1};\mW_{\phi}) & \propto\nu(\vx_{t})\exp\left[\vtheta_{X}\cdot\vx_{t}+Z_{Z}(\mW_{Z}\cdot\vr_{t|t-1}+\mM_{ZX}\cdot\vx_{t})\right]
\end{align*}

\end_inset

Using contrastive divergence-like approach for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, and the fact that 
\begin_inset Formula $\nabla Z_{Z}(\theta)=\E{p(Z)}{\vpsi(\vz)}$
\end_inset

 when 
\begin_inset Formula $\theta$
\end_inset

 is the natural parameter of 
\begin_inset Formula $p(Z)$
\end_inset

, it can be shown that the gradient for 
\begin_inset Formula $\mW_{\phi}$
\end_inset

 is
\begin_inset Formula 
\begin{equation}
\nabla_{\mW_{\phi}}\log q(\vx_{t}|\vx_{1:t-1};\mW_{\phi})=\left[\E{q(\vz_{t}|x_{1:t})}{\vpsi(\vz_{t})}-\E{q(\vz_{t}|x_{1:t-1})}{\vpsi(\vz_{t})}\right]\cdot\mW_{Z}\cdot\nabla_{\mW_{\phi}}\vr_{t|t-1}\label{eq:NBR_learning}
\end{equation}

\end_inset


\begin_inset Formula $\nabla_{\mW_{\phi}}\vr_{t|t-1}$
\end_inset

 has long-range dependencies as 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 is applied recursively.
 However, if inference is optimal, 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 contains all stimulus information, since the generative process is Markov.
 Thus, the author uses the approximation 
\begin_inset Formula $\nabla_{\mW_{\phi}}\vr_{t|t-1}\approx\nabla_{\mW_{\phi}}\phi(\vr_{t-1|t-1};\mW_{\phi})$
\end_inset

 and treats 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 as a constant, which avoids back-propagation through time.
 The first expectation can be computed exactly as it is Gaussian.
 The two expectations can be evaluated either by 
\end_layout

\begin_layout Enumerate
Gibbs sampling from 
\begin_inset Formula $p_{0}(\vz_{t},\vx_{t}|\vx_{1:t-1})$
\end_inset


\end_layout

\begin_layout Enumerate
direct computation of mean parameters from 
\begin_inset Formula $\vr_{t|t-1}$
\end_inset

 assuming 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

 is Gaussian.
\end_layout

\begin_layout Standard
Overall, this framework can approximate unknown latent dynamics in the latent
 space, and the conditioning can be done exactly using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule.
 However, realisation of the generic 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 theory is restricted to Gaussian prior and posteriors by the use of Gaussian-sh
aped tuning curves, which restricts the range of possible transition dynamics
 that can be well approximated.
 Interestingly, it is reported that when treating 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 as a constant in order to avoid backpropagation through time, the model
 cannot discover second-order latent variables, such as velocity when observing
 the position of an oscillatory signal.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 is related to the recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "MakinEtAl2015"
literal "true"

\end_inset

 but are fundamentally different.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 takes a density density estimation approach and treats the latent code
 
\begin_inset Formula $\vr_{t}$
\end_inset

 as features, but not an explicit encoding of the latent variables 
\begin_inset Formula $\vz_{t}$
\end_inset

 as in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 which encodes natural parameter of the posterior, a 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 allows arbitrary transition dynamics to be learned by back propagation;
 whereas 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 does not explicitly learn any transition dynamics, but the overall learning
 uses the more biologically plausible contrastive divergence rule.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 maximizes the incremental log likelihood of 
\begin_inset Formula $\log p(\vx_{t}|\vx_{1:t-1})$
\end_inset

, whereas 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 does not optimize any principled cost function.
 
\end_layout

\begin_layout Standard
The only part common to both approaches is the choice of likelihood via
 Gaussian tuning curves.
 This choice may be important for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 to admit the approximation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

, but less so for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

, as the density estimation formulation is agnostic to the generative model,
 as long as the support of observation distribution in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 include the data.
\end_layout

\begin_layout Paragraph
General machine learning algorithms
\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\z}{\mathbf{z}}
{\mathbf{z}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "MarinoEtAl2018"
literal "true"

\end_inset

 studied a more general non-Markovian state-space model the joint distribution
 factorises as 
\begin_inset Formula $p(\x_{\le T},\z_{\le T}):=\prod_{t=1}^{T}p\left(\x_{t},\z_{t}|\x_{<t},\z_{<t}\right)=\prod_{t=1}^{T}p\left(\x_{t}|\x_{<t},\z_{\le t}\right)p(\z_{t}|\x_{<t},\z_{<t})$
\end_inset

.
 Though it complicates full inference, the authors realise that the free
 energy, when factorised over time, can be optimized for each time step
 in the forward direction, permitting online filtering and learning.
 In addition, at each time step, filtering involves an inner loop of amortized
 inference at each time step 
\begin_inset Formula $t$
\end_inset

 where gradient updates were performed on the free energy term at time 
\begin_inset Formula $t$
\end_inset

, taking the posterior at 
\begin_inset Formula $t-1$
\end_inset

 as the prior.
 
\end_layout

\begin_layout Standard
The Kalman VAE 
\begin_inset CommandInset citation
LatexCommand citet
key "FraccaroEtAl2017"
literal "true"

\end_inset

 incorporates a standard VAE into a chain of Linear Gaussian state space
 model (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

) in which the linear matrices evolve over time.
 In the generative model, an intermediate sequence is first generated from
 the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

, using potentially time-dependent transition matrices.
 These intermediate latents are then fed into a nonlinear neural network
 to obtain observations.
 In order to allow for nonlinear dynamics, the linear transition matrices
 were functions of time and intermediate outputs.
 More specifically, let the intermediate latent at time 
\begin_inset Formula $t$
\end_inset

 be 
\begin_inset Formula $a_{t}$
\end_inset

, the dynamics matrix in the latent 
\begin_inset Formula $A=\sum_{k=1}^{K}\alpha_{t}^{(k)}\left(a_{0:t-1}\right)A^{\left(k\right)}$
\end_inset

, where 
\begin_inset Formula $\alpha_{t}^{(k)}$
\end_inset

 is a series of coefficients generated by recurrent neural network.
 These coefficients are used to linearly interpolate between 
\begin_inset Formula $K$
\end_inset

 linear modes (
\begin_inset Formula $\alpha_{t}^{(k)}\in[0,1]$
\end_inset

,
\begin_inset Formula $\sum_{k=1}^{K}\alpha_{t}^{(k)}=1$
\end_inset

).
 The control and observation matrices are constructed in the same fashion.
 Thus, these dynamics are functions of the samples generated from reparameterize
d Gaussian distributions, enabling learning by backpropagation.
 During inference, the message sent from each observation is a Gaussian
 distribution produced from a standard VAE.
 Filtering and smoothing are then analytically tractable, taking samples
 from the observation-only posterior as the usual observation for the LGSSM.
 Thus, although the latent variables follow linear dynamics, the linear
 matrices change over time and can depend on the observation history, giving
 the generative model the capacity of producing complex dynamics while maintaini
ng inferential tractability, provided that the observations were encoded
 into a Gaussian distribution.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LGSSM"
description " "
literal "true"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printglossaries
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ref"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document

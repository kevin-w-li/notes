#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\setcitestyle{round}
% call the glossaries package
\usepackage[savewrites]{glossaries} 
\usepackage{hyperref}
% activate the glossaries building
\renewcommand*{\glstextformat}{\textbf}
\makeglossaries
\usepackage{atveryend}

% redefine the command that LyX puts out, to the glossaries input
\let\nomenclature\gls

% add your glossaries entries
\newglossaryentry{linux}{name={linux},description={an open-source unix implementation}}
\newglossaryentry{LGSSM}{name={LGSSM},description={Linear Gaussian State Space Model}}
\newglossaryentry{NPF}{name={NPF}, description={Neural Particle Filter by Kutschireiter et al. (2017)}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section
Literature review
\end_layout

\begin_layout Subsection
Neural algorithms
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "WilsonFinkel2009"

\end_inset

 modified the line attractor network for head direction cells initially
 proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang1996"

\end_inset

 to implement the Kalman filter; in the case of constant, noiseless innovation
 (fixed drifting speed) and small observation noise, this network is shown
 to approximate the Kalman filtering equations for the mean and variance
 for 1-dimensional linear Gaussian system; for larger noises, the experiments
 show that it is able to adapt to change points much quicker than the Kalman
 filter.
 The neurons show higher firing rate when the underlying latent variable
 has higher precision, consistent with recordings
\begin_inset CommandInset citation
LatexCommand citet
key "SclarFreeman1982"

\end_inset

.
 In order to implement the dynamics, the speed in the latent variable enters
 through a parameter in the weight matrices connecting all neurons, which
 is not very biologically plausible, although some justifications were provided.
 
\end_layout

\begin_layout Standard
The neural particle filter (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

)
\begin_inset CommandInset citation
LatexCommand citep
key "KutschireiterEtAl2017"

\end_inset

 follows the sampling hypothesis and is an ansatz derived from the filtering
 equation 
\begin_inset CommandInset citation
LatexCommand citep
key "Kushner1964"

\end_inset

 for continuous-time SDE.
 Each particle evolves according to dynamics which depends on the prediction
 error similar to the dynamics of the posterior mean in the Kalman filter,
 with the gain computed using the ensemble of particles.
 By avoiding the need for particle weights as in common particle filters,
 it is made biologically plausible and a neural circuits implementation
 was proposed.
 It is interesting that the evolution of each particle according to the
 mean resulted in good posterior estimates collectively for the ensemble.
 In their experiments, inference was shown to estimate the posterior mean
 well.
 By training the network weights using maximum likelihood, the model was
 able to learn the parameters of the observations density.
 The latent dynamics was not considered as joint samples of adjacent time
 steps were not considered.
 The experiments show that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 avoided the 
\begin_inset Quotes eld
\end_inset

curse of dimensionality
\begin_inset Quotes erd
\end_inset

, but its latent dynamics were independent, although the linear mixing of
 observation may create correlation.
 
\end_layout

\begin_layout Subsection
General inference algorithms
\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\z}{\mathbf{z}}
{\mathbf{z}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "MarinoEtAl2018"

\end_inset

 studied a more general non-Markovian state-space model the joint distribution
 factorises as 
\begin_inset Formula $p(\x_{\le T},\z_{\le T}):=\prod_{t=1}^{T}p\left(\x_{t},\z_{t}|\x_{<t},\z_{<t}\right)=\prod_{t=1}^{T}p\left(\x_{t}|\x_{<t},\z_{\le t}\right)p(\z_{t}|\x_{<t},\z_{<t})$
\end_inset

.
 Though it complicates full inference, the authors realise that the free
 energy, when factorised over time, can be optimized for each time step
 in the forward direction, permitting online filtering and learning.
 In addition, at each time step, filtering involves an inner loop of amortized
 inference at each time step 
\begin_inset Formula $t$
\end_inset

 where gradient updates were performed on the free energy term at time 
\begin_inset Formula $t$
\end_inset

, taking the posterior at 
\begin_inset Formula $t-1$
\end_inset

 as the prior.
 
\end_layout

\begin_layout Standard
The Kalman VAE 
\begin_inset CommandInset citation
LatexCommand citet
key "FraccaroEtAl2017"

\end_inset

 incorporates a standard VAE into a chain of Linear Gaussian state space
 model (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

) in which the linear matrices evolve over time.
 In the generative model, an intermediate sequence is first generated from
 the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

, using potentially time-dependent transition matrices.
 These intermediate latents are then fed into a nonlinear neural network
 to obtain observations.
 In order to allow for nonlinear dynamics, the linear transition matrices
 were functions of time and intermediate outputs.
 More specifically, let the intermediate latent at time 
\begin_inset Formula $t$
\end_inset

 be 
\begin_inset Formula $a_{t}$
\end_inset

, the dynamics matrix in the latent 
\begin_inset Formula $A=\sum_{k=1}^{K}\alpha_{t}^{(k)}\left(a_{0:t-1}\right)A^{\left(k\right)}$
\end_inset

, where 
\begin_inset Formula $\alpha_{t}^{(k)}$
\end_inset

 is a series of coefficients generated by recurrent neural network.
 These coefficients are used to linearly interpolate between 
\begin_inset Formula $K$
\end_inset

 linear modes (
\begin_inset Formula $\alpha_{t}^{(k)}\in[0,1]$
\end_inset

,
\begin_inset Formula $\sum_{k=1}^{K}\alpha_{t}^{(k)}=1$
\end_inset

).
 The control and observation matrices are constructed in the same fashion.
 Thus, these dynamics are functions of the samples generated from reparameterize
d Gaussian distributions, enabling learning by backpropagation.
 During inference, the message sent from each observation is a Gaussian
 distribution produced from a standard VAE.
 Filtering and smoothing are then analytically tractable, taking samples
 from the observation-only posterior as the usual observation for the LGSSM.
 Thus, although the latent variables follow linear dynamics, the linear
 matrices change over time and can depend on the observation history, giving
 the generative model the capacity of producing complex dynamics while maintaini
ng inferential tractability, provided that the observations were encoded
 into a Gaussian distribution.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LGSSM"
description " "

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printglossaries
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ref"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document

#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\setcitestyle{round}
% call the glossaries package
\usepackage[nonumberlist]{glossaries} 
\usepackage{hyperref}
% activate the glossaries building
\renewcommand*{\glstextformat}{\textbf}
\makeglossaries
\usepackage{atveryend}

% redefine the command that LyX puts out, to the glossaries input
\let\nomenclature\gls

% add your glossaries entries
\newglossaryentry{linux}{name={linux},description={an open-source unix implementation}}
\newglossaryentry{SSM}{name={SSM},description={State Space Model}}
\newglossaryentry{LGSSM}{name={LGSSM},description={Linear Gaussian State Space Model}}
\newglossaryentry{NPF}{name={NPF}, description={Neural Particle Filter by Kutschireiter et al. (2017)}}
\newglossaryentry{PPC}{name={PPC}, description={Probabilistic population code}}
\newglossaryentry{EFH}{name={EFH}, description={Exponential family harmonium}}
\newglossaryentry{rEFH}{name={rEFH}, description={Recurrent Exponential family harmonium for filtering by Makin et al. (2015)}}
\newglossaryentry{PPC-BR-EFH}{name={PPC-BR-EFH}, description={Probabilistic population code Bayes rule with exponential family harmonium by Sokoloski (2017)}}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\rvfont}[1]{\boldsymbol{#1}}
{\boldsymbol{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vfont}[1]{\mathbf{#1}}
{\mathbf{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mfont}[1]{\mathbf{#1}}
{\mathrm{\boldsymbol{#1}}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vtheta}{\vfont{\theta}}
{\vfont{\theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vn}{\vfont n}
{\vfont n}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\vfont w}
{\vfont w}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vomega}{\boldsymbol{\omega}}
{\boldsymbol{\omega}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vz}{\vfont z}
{\vfont z}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vs}{\vfont s}
{\vfont s}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vX}{\rvfont X}
{\rvfont X}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vZ}{\rvfont Z}
{\rvfont Z}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vN}{\rvfont N}
{\rvfont N}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vf}{\vfont f}
{\vfont f}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vg}{\vfont g}
{\vfont g}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vr}{\vfont r}
{\vfont r}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vR}{\rvfont R}
{\rvfont R}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vpsi}{\boldsymbol{\psi}}
{\boldsymbol{\psi}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vzeta}{\boldsymbol{\zeta}}
{\boldsymbol{\zeta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vphi}{\boldsymbol{\phi}}
{\boldsymbol{\phi}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mtheta}{\mathbf{\Theta}}
{\mathbf{\Theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mW}{\mfont W}
{\mfont W}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mM}{\mfont M}
{\mfont M}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mSigma}{\mfont{\Sigma}}
{\mfont{\Sigma}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mV}{\mfont V}
{\mfont V}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\N}{\mathbb{N}}
{\mathbf{\mathbb{N}}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sM}{\mathcal{M}}
{\mathcal{M}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Dim}[1]{K_{#1}}
{K_{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sigm}{\operatorname{sigm}}
{\textrm{sigm}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Pois}{\operatorname{Pois}}
{\textrm{Pois}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Bern}{\operatorname{Bern}}
{\textrm{Bern}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\KL}{\operatorname{KL}}
{\textrm{KL}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\NN}{\operatorname{NN}}
{\textrm{NN}}
\end_inset


\end_layout

\begin_layout Section
Literature review
\end_layout

\begin_layout Subsection
Variational learning
\end_layout

\begin_layout Standard
Consider a generative model described by a directed graph 
\begin_inset Formula $\vZ_{0}\rightarrow\vZ_{2}\rightarrow\cdots\rightarrow\vZ_{L}$
\end_inset

, where 
\begin_inset Formula $\vZ_{0}\sim p_{\theta_{0}}\left(\vz_{0}\right)$
\end_inset

 and each arrow suggests a parametric conditional distribution 
\begin_inset Formula $p_{\theta_{l}}(\vZ_{l}|\vZ_{l-1}),l\in[1\dots,L]$
\end_inset

.
 In addition, denote 
\begin_inset Formula $\vX=\vZ_{L}$
\end_inset

, and 
\begin_inset Formula $\vZ=\left\{ \vZ_{0},\cdots\vZ_{L-1}\right\} $
\end_inset

.
 The goal is to fit the parameters 
\begin_inset Formula $\theta=\{\theta_{l}\}_{l=1}^{L}$
\end_inset

 in order to maximize the likelihood of some observed data 
\begin_inset Formula $\vx=\left\{ \vx_{n}\right\} _{n=1}^{N}\sim\Pi(\vx)$
\end_inset

 i.i.d from some unknown true distribution 
\begin_inset Formula $\Pi(\vx)$
\end_inset

.
 Although the set of conditional distributions implies a normalized joint
 distribution 
\begin_inset Formula $p_{\theta}(\vZ,\vX)=p_{\theta_{0}}\left(\vZ_{0}\right)\prod_{l=1}^{L}p_{\theta_{l}}\left(\vZ_{l}|\vZ_{l-1}\right)$
\end_inset

 , evaluating the likelihood of data 
\begin_inset Formula $\mathcal{L}(\theta):=\log p_{\theta}(\vx)$
\end_inset

 involves the sum 
\begin_inset Formula $\sum_{\vZ}p_{\theta}(\vZ,\vX)$
\end_inset

, which is computationally intractable.
 Instead, we evaluate the free energy
\begin_inset Formula 
\begin{align*}
\mathcal{F}(\theta,q): & =\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX)-\log(q(\vZ|\vX))}\\
 & =\mathcal{L}(\theta)-\KL\left[q(\vZ|\vX)\|p(\vZ\|\vX)\right]\\
 & =\le\mathcal{L}(\theta)
\end{align*}

\end_inset

where 
\begin_inset Formula $q(\vZ|\vX)$
\end_inset

 is the variational distribution that can depend on 
\begin_inset Formula $\vX$
\end_inset

.
 It can be shown that the bound is tight if and only if 
\begin_inset Formula $\KL\left[\log q\left(\vZ|\vX\right)\|\log p_{\theta}\left(\vZ|\vX\right)\right]=0$
\end_inset

.
 Thus, maximizing 
\begin_inset Formula $\mathcal{F}(\theta,q)$
\end_inset

 involves an iterative process that in each iteration alternates between
\end_layout

\begin_layout Enumerate
(M-step) maximizing 
\begin_inset Formula $\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX))}$
\end_inset


\end_layout

\begin_layout Enumerate
(E-step) minimizing 
\begin_inset Formula $\KL\left[\log q\left(\vZ|\vX\right)\|\log p_{\theta}\left(\vZ|\vX\right)\right]$
\end_inset


\end_layout

\begin_layout Standard
By defining the joint distribution 
\begin_inset Formula $q_{\phi}(\vZ,\vX):=q_{\phi}(\vZ|\vX)\Pi(\vX)$
\end_inset

, the M-step is also equivalent to
\begin_inset Formula 
\begin{equation}
\min_{\theta}\KL\left[q_{\phi}(\vZ,\vX)\|p_{\theta}(\vZ,\vX)\right]\label{eq:M_step_joint_KL}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Helmholtz machine
\end_layout

\begin_layout Standard
The wake-sleep algorithm by 
\begin_inset CommandInset citation
LatexCommand citet
key "HintonEtAl1995"

\end_inset

, designed for the classical Helmholtz machine proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "DayanEtAl1995"

\end_inset

, performs the M-step by samples from an approximate posterior 
\begin_inset Formula $q_{\phi}(\vZ|\vX)$
\end_inset

, and the posterior samples are obtained from an inferential model using
 fixed inferential parameters 
\begin_inset Formula $\phi$
\end_inset

.
 Because the generative model is Markov, the approximate (and true) posteriors
 factorizes
\begin_inset Formula 
\[
q_{\phi}(\vZ|\vX)=\prod_{l=0}^{L-1}q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})
\]

\end_inset


\end_layout

\begin_layout Standard
Thus, posterior samples can be obtained by a sequence of conditional sampling:
 given an input 
\begin_inset Formula $\vx$
\end_inset

, draws 
\begin_inset Formula $\vZ_{L-1}$
\end_inset

 given 
\begin_inset Formula $\vx$
\end_inset

, and then draws 
\begin_inset Formula $\vZ_{L-2}$
\end_inset

 conditioned on each a posterior sample 
\begin_inset Formula $\vz_{L-1}|\vx$
\end_inset

, and so on.
 Each factor 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})$
\end_inset

 is implemented as a distribution with tractable sampling routine and flexible
 parameterisation, such as an exponential family distribution with parameters
 that are output of neural networks.
 For example, if 
\begin_inset Formula $\vZ$
\end_inset

 is binary, then we may choose conditionally independent posterior 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vz_{l+1})=\prod_{d}q_{\phi_{l,d}}(Z_{l,d}|\vz_{l+1})$
\end_inset

, 
\begin_inset Formula $q_{\phi_{l}}(Z_{l,d}|\vz_{l+1})=\Bern(Z_{l,d}|\NN(\vz_{l+1};\phi_{l,d}))$
\end_inset

, which in addition assumes conditional independence.
 To train 
\begin_inset Formula $\phi_{l}$
\end_inset

, generative samples 
\begin_inset Formula $\left\{ \vz_{l},\vz_{l+1}\right\} $
\end_inset

are drawn from 
\begin_inset Formula $p_{\theta}(\vZ_{l},\vZ_{l+1})$
\end_inset

 by ancestral sampling, and the objective is to maximize the likelihood
 of 
\begin_inset Formula $\vz_{l}$
\end_inset

 given 
\begin_inset Formula $\vz_{l+1}$
\end_inset

, averaged over different 
\begin_inset Formula $\vz_{l+1}$
\end_inset


\begin_inset Formula 
\[
\max_{\phi_{l}}\E{p_{\theta}(\vZ_{l},\vZ_{l+1})}{\log q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})}
\]

\end_inset

When the same procedure is used for all layers, it can be shown that this
 process is equivalent to minimizing the average reverse KL divergence 
\begin_inset Formula 
\[
\min_{\phi}\E{p_{\theta}(\vX)}{\KL\left[p_{\theta}(\vZ|\vX)\|q_{\phi}(\vZ|\vX)\right]}
\]

\end_inset

Or equivalently, by defining the joint distribution 
\begin_inset Formula $q_{\phi}(\vZ,\vX):=q_{\phi}(\vZ|\vX)\Pi(\vX)$
\end_inset


\begin_inset Formula 
\[
\min_{\phi}\KL\left[p_{\theta}(\vZ,\vX)\|q_{\phi}(\vZ,\vX)\right]
\]

\end_inset

This step of using generated sample to train the inferential model is known
 as the 
\begin_inset Quotes eld
\end_inset

sleep phase
\begin_inset Quotes erd
\end_inset

 .
 Note that the objective been minimized here is different from the KL in
 the usual E-step.
\end_layout

\begin_layout Standard
In the 
\begin_inset Quotes eld
\end_inset

wake phase
\begin_inset Quotes erd
\end_inset

, the inferential model takes real observations 
\begin_inset Formula $\vX\sim\Pi(\vx)$
\end_inset

 and produces posterior samples from 
\begin_inset Formula $q_{\phi}(\vZ|\vX)$
\end_inset

.
 These samples are used to perform the usual M-step, which corresponds to
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:M_step_joint_KL"

\end_inset

.
 Therefore, the wake-sleep algorithm does not optimize a consistent divergence,
 although if 
\begin_inset Formula $p_{\theta}(\vZ,\vX)=q_{\phi}(\vZ,\vX)$
\end_inset

, both losses are at global minimum 0.
\end_layout

\begin_layout Standard
Particular instances of Helmholtz machine are biologically attractive.
 When 
\begin_inset Formula $p_{\theta_{l}}(\vZ_{l}|\vZ_{l-1})$
\end_inset

 and 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})$
\end_inset

 is in the exponential family, conditionally independent, and each factor
 is parameterized by its conditional mean in the form of the canonical link
 function on 
\begin_inset Formula $\vz_{l+1}$
\end_inset

 (a generalized linear model), then the gradient of 
\begin_inset Formula $\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX))}$
\end_inset

 w.r.t 
\begin_inset Formula $\theta$
\end_inset

 and the gradient of 
\begin_inset Formula $\E{p(\vZ,\vX)}{\log q_{\phi}(\vZ|\vX)}$
\end_inset

 w.r.t 
\begin_inset Formula $\phi$
\end_inset

 only depend on local samples, so learning in the generative and inferential
 model can be done using the biologically plausible learning rule.
 This is the case for the sigmoid belief net 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand citep
key "DayanEtAl1995"

\end_inset

 and generalisations in 
\begin_inset CommandInset citation
LatexCommand citep
key "DayanHinton1996"

\end_inset

.
\end_layout

\begin_layout Subsection
Neural filtering algorithms
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "WilsonFinkel2009"

\end_inset

 modified the line attractor network for head direction cells initially
 proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang1996"

\end_inset

 to implement the Kalman filter; in the case of constant, noiseless innovation
 (fixed drifting speed) and small observation noise, this network is shown
 to approximate the Kalman filtering equations for the mean and variance
 for 1-dimensional linear Gaussian system; for larger noises, the experiments
 show that it is able to adapt to change points much quicker than the Kalman
 filter.
 The neurons show higher firing rate when the underlying latent variable
 has higher precision, consistent with recordings
\begin_inset CommandInset citation
LatexCommand citet
key "SclarFreeman1982"

\end_inset

.
 In order to implement the dynamics, the speed in the latent variable enters
 through a parameter in the weight matrices connecting all neurons, which
 is not very biologically plausible, although some justifications were provided.
 
\end_layout

\begin_layout Standard
The neural particle filter (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

)
\begin_inset CommandInset citation
LatexCommand citep
key "KutschireiterEtAl2017"

\end_inset

 is a sample-based filtering algorithm for continuous time 
\begin_inset ERT
status open

\begin_layout Plain Layout

gls{SSM}
\end_layout

\end_inset

:
\begin_inset Formula 
\begin{align*}
d\vz_{t} & =\vf(\vz_{t})+\mSigma_{z}^{1/2}d\vomega_{t}^{(z)}\\
d\vx_{t} & =\vg(\vz_{t})+\mSigma_{x}^{1/2}d\vomega_{t}^{(x)}
\end{align*}

\end_inset

where 
\begin_inset Formula $\vomega_{t}^{(\cdot)}$
\end_inset

 is the standard Brownian motion.
 The filtered posterior is represented by a set of unweighted samples 
\begin_inset Formula $\left\{ \vz_{t}^{(s)}\right\} _{s}$
\end_inset

, and the evolution of each particle is motivated by the Kushner equation
 given below, denoting 
\begin_inset Formula $\E{p(\vz_{t}|\vx_{1:t})}{\cdot}$
\end_inset

 by 
\begin_inset Formula $\langle\cdot\rangle$
\end_inset

 and let 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 be an arbitrary function on 
\begin_inset Formula $\vz$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
d\langle\phi\rangle=\langle\vf\rangle dt+\text{cov}(\phi,\vg)\mSigma_{x}^{-1}\left(d\vx_{t}-\left\langle \vg\right\rangle dt\right)\label{eq:NPF_kushner}
\end{equation}

\end_inset

.
 When 
\begin_inset Formula $\vg$
\end_inset

 is linear and 
\begin_inset Formula $\phi(\vz)=\vz$
\end_inset

, it recovers the SDE governing the posterior mean in continuous-time Kalman
 filtering.
 Although this equation is exact, we still do not know the higher moments
 
\begin_inset Formula $\text{cov}(\phi,\vg)$
\end_inset

 and thus direct implementation is impossible (closure problem).
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 proposes to remedy this issue with two design choices.
 First, each particle 
\begin_inset Formula $\vz_{t}^{(s)}$
\end_inset

 evolves according to 
\begin_inset Formula 
\begin{equation}
d\vz_{t}^{(s)}=\vf(\vz_{t}^{(s)})dt+\mW_{t}\left(d\vx_{t}-\left\langle \vg\right\rangle dt\right)+\mSigma_{x}^{1/2}d\vomega_{t}^{(z)}\label{eq:NPF_sample_evolution}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mW_{t}=\hat{\text{cov}}(\vz,\vg)\mSigma_{x}^{-1}$
\end_inset

 with 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 begin the empirically estimated using all samples.
 We can see 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 is similar to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_kushner"

\end_inset

 when 
\begin_inset Formula $\phi(\vz)=\vz$
\end_inset

 except for the last noise term.
 Therefore, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 tries to avoid the closure problem by employing a population of samples
 that follow the trajectory for the mean, replacing 
\begin_inset Formula $\text{cov}(\vz,\vg)$
\end_inset

 with the sample estimate.
 
\end_layout

\begin_layout Standard
By avoiding the need for particle weights as in common particle filters,
 it is made biologically plausible and a neural circuits implementation
 was proposed.
 By training the network weights using maximum likelihood, the model was
 able to learn the parameters of the observations density.
 The latent dynamics was not considered as joint samples of adjacent time
 steps were not considered.
 The experiments show that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 avoided the 
\begin_inset Quotes eld
\end_inset

curse of dimensionality
\begin_inset Quotes erd
\end_inset

, requires much fewer particles than the conventional bootstrap particle
 filter for higher-dimensional problems.
\end_layout

\begin_layout Standard
It remains a question as to why the 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 worked at all, as it is not obvious why (approximately) following the trajector
y of the mean allows the particles to be distributed correctly.
 Intuitively, the first term on the RHS of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 is the dynamics contribution, the second term is the correction from observatio
n.
 
\begin_inset Formula $\mW_{t}$
\end_inset

 would be the correct gain if 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 were exact, and the amount of correction is determined by the posterior
 variance.
 If the third noise term is absent and the particles are initially correctly
 distributed, then all the particles would move closer to the mean after
 infinitesimal amount of filtering time, and 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 is then an underestimate of the true covariance, resulting in a smaller
 correction for future time.
 Since there is a noise term, 
\begin_inset Formula $\left\{ \vz_{t}^{(s)}\right\} _{s}$
\end_inset

 are pushed away from the mean which increases 
\begin_inset Formula $\text{cov}(\phi,\vg)$
\end_inset

 and hence the correction.
 The correct amount of 
\begin_inset Quotes eld
\end_inset

push
\begin_inset Quotes erd
\end_inset

 is the uncertainty induced by data and likely correlated, reflecting explaining
-away, instead of the independent prior noise.
 Thus, it may be the case that the full distribution is different from the
 true posterior, but the posterior mean estimated from these particles are
 close to the correct value.
 The authors did not show full distribution of the posteriors.
\end_layout

\begin_layout Subsubsection
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 implementation of the Kalman filter 
\end_layout

\begin_layout Subsubsection
Recurrent exponential family harmonium (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
Hard-wiring the neurons to implement/approximate optimal statistical computation
s, as done in some 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature, 
\begin_inset CommandInset citation
LatexCommand citep
key "BeckEtAl2007,BeckEtAl2011"

\end_inset

, is not biologically plausible, as the brain does not receive such fine-grained
 supervisions to construct the wiring.
 
\end_layout

\begin_layout Standard
Viewing inference and learning as a density estimation operation, 
\begin_inset CommandInset citation
LatexCommand citet
key "MakinEtAl2013"

\end_inset

 proposed to use the exponential family harmonium 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, a restricted Boltzmann machine-like fully-connected graphical model, to
 implement a 
\emph on
model
\emph default
 joint distribution of observation 
\begin_inset Formula $\vX$
\end_inset

 and latent code 
\begin_inset Formula $\vR$
\end_inset

, 
\begin_inset Formula $q(\vx,\vr)$
\end_inset

.
 
\begin_inset Formula 
\begin{align*}
q(\vr|\vx) & =\prod_{i=1}^{\Dim R}\Bern\left[r_{i}|\sigm\left(\mM_{RX}\vx+\vtheta_{R}\right)\right]\\
q(\vx|\vr) & =\prod_{i=1}^{\Dim X}\Pois\left[x_{i}|\exp\left(\mM_{RX}^{\intercal}\vr+\vtheta_{X}\right)\right]
\end{align*}

\end_inset

The choices of 
\begin_inset Formula $\sigm$
\end_inset

 and 
\begin_inset Formula $\exp$
\end_inset

 are the canonical link functions for Bernoulli and Poisson, respectively,
 that allow modulation of the natural parameters through linear transformation
 of the conditioning variable.
 The latent code is a sample drawn from 
\begin_inset Formula $\vR|\vx$
\end_inset

, or just its mean rate.
 In fact, the observation 
\begin_inset Formula $\vX$
\end_inset

 is spike count generated by Poisson neurons with Gaussian tuning functions
 over some unobserved variable 
\begin_inset Formula $\vZ$
\end_inset

, yielding a 
\emph on
generative
\emph default
 joint
\emph on
 
\emph default
distribution 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

.
 Learning entails maximizing the spike count log likelihood 
\begin_inset Formula $\E{p(\vx)}{\log q(\vx)}$
\end_inset

 using the standard contrastive divergence for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, similar to one more commonly known for training restricted Boltzmann machines.
 
\end_layout

\begin_layout Standard
Now it is clear that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 is a density estimation approach to model 
\begin_inset Formula $p(\vx)$
\end_inset

.
 Different from other density estimation approach to model natural stimuli
 
\begin_inset CommandInset citation
LatexCommand citep
key "Lewicki2002,LewickiOlshausen1999,LewickiSejnowski2000"

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 is a fully connected graph rather than a directed graph, and inference
 produces conditionally independent 
\emph on
code
\emph default
 
\begin_inset Formula $\vr$
\end_inset

 rather than the latent variables 
\begin_inset Formula $\vr$
\end_inset

.
 However, the latent code 
\begin_inset Formula $\vr$
\end_inset

 is some abstract feature to describe 
\begin_inset Formula $\vx$
\end_inset

 and is not associated directly with the latent variables 
\begin_inset Formula $\vZ$
\end_inset

 in the generative model, and thus the uncertainty in the posterior cannot
 be interrogated straightforwardly.
 Indeed, to evaluate the performance of this model by decoding, 
\begin_inset Formula $\vr$
\end_inset

 is first used to evaluate the conditional expected spike count 
\begin_inset Formula $\E{q(\vx|\vr)}{\vx}$
\end_inset

 under the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 model, assuming there is no loss of information; then, the mean and covariance
 of 
\begin_inset Formula $\vz$
\end_inset

 are approximately decoded according to the generative joint 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

, pretending
\begin_inset Formula $\E{q(\vx|\vr)}{\vx}$
\end_inset

 is evaluated under 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

.
 Using this approach, one can in principle decoded posterior distribution
 that are more complicated than a Gaussian using, for example, histograms.
\end_layout

\begin_layout Standard
Later on, the same authors 
\begin_inset CommandInset citation
LatexCommand citet
key "MakinEtAl2015"

\end_inset

 applied the same principle to build a recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 for filtering.
 In this model, the latent code from the previous time step 
\begin_inset Formula $\vR_{t-1}$
\end_inset

 and the spike observation 
\begin_inset Formula $\vX_{t}$
\end_inset

 are concatenated, which is used to form an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 with a latent code distribution 
\begin_inset Formula $\vR_{t}$
\end_inset

.
 The latent code for next time step is then the conditional sample 
\begin_inset Formula $\vr_{t}\sim\vR_{t}|\vr_{t-1},\vx_{t}$
\end_inset

, and the same construction of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 continues, giving the recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

.
 The learning algorithm is the 1-step contrastive divergence performed at
 each time step, which maximises the log likelihood of 
\begin_inset Formula $\left[\vr_{t-1},\vx_{t}\right]$
\end_inset

, treating the previous latent code 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 as an 
\emph on
observation
\emph default
.
 
\end_layout

\begin_layout Standard
Although the learning rule is local and thus biologically plausible, the
 underlying cost function is unclear; as the authors also pointed out, the
 justification for treating 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 as an observation is missing.
 By copying 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 to the next time step, no latent dynamics is explicitly modelled and needs
 to be learned by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 at each time step.
 As the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 also needs to incorporate new evidence, it is conceivable that the class
 of transition dynamics and emission may be restrictive.
 Indeed, the experiments conducted all had linear Gaussian latent dynamics.
\end_layout

\begin_layout Subsubsection
PPC Bayes rule and EFH (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"

\end_inset

 proposed a neural filtering algorithm that combines 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "WellingEtAl2005"

\end_inset

.
 By extensive use of properties of the exponential family distributions,
 the theory suggests that evidence from observation can be more or less
 exactly incorporated into the belief of the latent, that is, Bayes rule
 can be implemented exactly.
 Here, I refer to this approach as the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule with 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

).
 Forming the prior for the next time step from the previous belief requires
 knowledge about the transition dynamics, which is not explicitly modeled;
 nonetheless, the mapping of natural parameter of the posterior 
\begin_inset Formula $p(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

 to that of the predictive distribution 
\begin_inset Formula $p(\vz_{t}|\vx_{1:t-1})$
\end_inset

 was modeled as a neural network function.
 Although the neural network needs to be trained by the non-plausible back-propa
gation, and only Gaussian posterior is allowed in practice, the theory itself
 is interesting and is thoroughly reviewed here.
 
\end_layout

\begin_layout Standard
This framework has two parts.
 In the first part, a relationship between sensory input 
\begin_inset Formula $\rvfont Z$
\end_inset

 and Poisson spike count 
\begin_inset Formula $\rvfont X$
\end_inset

 is constructed so that the prior and posterior of 
\begin_inset Formula $\rvfont Z$
\end_inset

 live in the same exponential family; the second part describes how the
 prior and posterior can be encoded by the two different neural populations,
 using natural parameters
\begin_inset Formula $\vtheta_{t}$
\end_inset

, and the belief update on these natural parameters.
 In short, the purpose for conjugacy is to maintain consistency of neural
 representation as PPC for the latent variable.
\end_layout

\begin_layout Paragraph
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\end_layout

\begin_layout Standard
In first part, a conjugacy between prior and posterior 
\begin_inset Formula $\vz$
\end_inset

 with Poisson likelihood 
\begin_inset Formula $p(\vx|\vz)$
\end_inset

 is established.
 Let the prior of sensory input 
\begin_inset Formula $\vZ$
\end_inset

 follow some generic exponential family distribution with sufficient statistics
 
\begin_inset Formula $\vpsi(\vz)$
\end_inset

 
\begin_inset Formula 
\begin{equation}
p(\vz)=\frac{1}{Z_{Z}(\vtheta_{Z})}\nu_{Z}(\vz)\exp\left[\vtheta_{Z}\cdot\vpsi(\vz)\right]\label{eq:NBR_prior}
\end{equation}

\end_inset

denote the set of exponential family distributions with sufficient statistics
 
\begin_inset Formula $\vpsi$
\end_inset

 to be 
\begin_inset Formula $\sM_{\psi}$
\end_inset

.
 These inputs generate neural responses 
\begin_inset Formula $N$
\end_inset

 that is assumed to be independent Poisson conditioned on a particular 
\begin_inset Formula $\vZ=\vz$
\end_inset

 with Gaussian-shaped tuning curves 
\begin_inset Formula $f_{i}(\vz)=\gamma\exp\left(-\frac{\left(z_{i}-z_{i}^{0}\right)^{2}}{2\sigma^{2}}\right),\ i\in\N_{\Dim X}$
\end_inset

, where homogeneous gain 
\begin_inset Formula $\gamma$
\end_inset

 and tuning width 
\begin_inset Formula $\sigma$
\end_inset

.
 The likelihood of a given firing pattern 
\begin_inset Formula $\vx$
\end_inset

 is
\begin_inset Formula 
\begin{align}
p(\vx|\vz) & =\prod_{i=1}^{\Dim X}\exp\left[-f_{i}(\vz)\right]\frac{f_{i}(\vz)^{x_{i}}}{x_{i}!}\nonumber \\
 & =\frac{1}{\exp\left(\sum_{i=1}^{\Dim X}f_{i}(\vz)\right)\prod_{i=1}^{\Dim X}(x_{i}!)}\exp\left[\sum_{i=1}^{\Dim X}\log f_{i}(\vz)x_{i}\right]\nonumber \\
 & =\frac{1}{\exp\left(\Phi_{X}(\vz)\right)}\nu_{X}(\vx)\exp\left[\vpsi(\vz)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right]\label{eq:NBR_poisson}
\end{align}

\end_inset

where 
\begin_inset Formula $\vpsi(\vz)$
\end_inset

, 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 are related to the tuning functions 
\begin_inset Formula $f_{i}$
\end_inset


\begin_inset Formula 
\begin{align*}
\nu_{X}(\vx) & =1/\prod_{i=1}^{\Dim X}x_{i}!\\
\vpsi(\vz) & =\left[\vz;\vz^{2}\right]\\
\mM_{ZX} & =\left[\frac{I\vz^{0}}{\sigma^{2}};-\frac{I}{2\sigma^{2}}\right]\\
\vtheta_{X} & =\left[\log\gamma-\frac{\left(\vz^{0}\right)^{2}}{2\sigma^{2}}\right]
\end{align*}

\end_inset

, and 
\begin_inset Formula $\Phi_{X}(\vz)$
\end_inset

 is the log normaliser of 
\begin_inset Formula $p(\vx|\vz)$
\end_inset


\begin_inset Formula 
\begin{equation}
\Phi_{X}(\vz)=\sum_{i=1}^{\Dim X}f_{i}(\vz)=\log\sum_{\vx}\nu_{X}(\vx)\exp\left(\vpsi(\vz)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right)\label{eq:NBR_poisson_logZ}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 denotes the the weights for 
\emph on
interaction
\emph default
 of 
\begin_inset Formula $\vz$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

 .
 Together with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_prior"

\end_inset

, 
\begin_inset Formula $p(\vx|\vz)$
\end_inset

 defines a joint distribution between 
\begin_inset Formula $\vZ$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

.
 
\end_layout

\begin_layout Standard
Now we would like the posterior 
\begin_inset Formula $p(\vz|\vx)$
\end_inset

 to be conjugate, meaning that it is also in 
\begin_inset Formula $\sM_{\psi}$
\end_inset

.
 The author proposes a form of density function of
\begin_inset Formula $\vZ$
\end_inset

 and 
\begin_inset Formula $\vN$
\end_inset

 that is an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 permitting Poisson 
\begin_inset Formula $p(\vz|\vx)$
\end_inset

 
\begin_inset Formula 
\[
p_{0}(\vz,\vx)\propto\nu_{Z}(\vz)\nu_{X}(\vx)\exp\left[\vtheta_{Z}\cdot\vpsi(\vz)+\vpsi(\vz)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right]
\]

\end_inset

which captures the relationship between the observation and neural spiking.
 Using 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_poisson_logZ"

\end_inset

, we can sum out 
\begin_inset Formula $\vx$
\end_inset

 and obtain (unnormalised) 
\begin_inset Formula $p_{0}(\vz)$
\end_inset

,
\begin_inset Formula 
\begin{align}
p_{0}(\vz) & \propto\nu_{Z}(\vz)\exp\left[\vtheta_{Z}\cdot\vpsi(\vz)+\Phi_{Z}(\vz)\right]\label{eq:NBR_true_prior}\\
p_{0}(\vx) & \propto\nu_{X}(\vx)\exp\left[\vtheta_{X}\cdot\vx+\log Z_{Z}(\vtheta_{Z}+\mM_{ZX}\cdot\vx)\right]\label{eq:NBR_true_lik}
\end{align}

\end_inset

.The implied posterior distribution is
\begin_inset Formula 
\begin{equation}
p_{0}(\vz|\vx)\propto\nu_{Z}(\vz)\exp\left(\vpsi(\vz)\cdot\left[\mM_{ZX}\cdot\vx+\vtheta_{Z}\right]\right)\label{eq:NBR_true_posterior}
\end{equation}

\end_inset

and 
\begin_inset Formula $p_{0}(\vz|\vx)\in\sM$
\end_inset

.
 Thus, 
\begin_inset Formula $p_{0}(\vz)$
\end_inset

 is almost in 
\begin_inset Formula $\sM_{\psi}$
\end_inset

 but off by 
\begin_inset Formula $\Phi_{Z}(\vz)=\sum_{i=1}^{\Dim X}f_{i}(\vz)$
\end_inset

 in the log density.
 If one assumes that this sum, which is the total firing rate for any given
 stimulus 
\begin_inset Formula $z$
\end_inset

, is a constant, 
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{\Dim X}f_{i}(\vz)=\lambda\label{eq:NBR_const_total_rate}
\end{equation}

\end_inset

then 
\begin_inset Formula $p_{0}\left(\vz,\vx\right)=p\left(\vz,\vx\right)$
\end_inset

, and we obtain a model that has the following properties:
\end_layout

\begin_layout Enumerate
the prior 
\begin_inset Formula $p(\vz)\in\sM_{\psi}$
\end_inset


\end_layout

\begin_layout Enumerate
the likelihood 
\begin_inset Formula $p(\vx|\vz)$
\end_inset

 is independent Poisson with mean rate depending on Gaussian tuning functions
 
\begin_inset Formula $f_{i}$
\end_inset


\end_layout

\begin_layout Enumerate
the posterior 
\begin_inset Formula $p(\vz|\vx)\in\sM_{\psi}$
\end_inset


\end_layout

\begin_layout Standard
Given these properties, the next question is how to encode these distributions
 into neural populations.
 Following PPC, and assuming that 
\begin_inset Formula $\sum_{i=1}^{\Dim X}f_{i}(\vz)$
\end_inset

 is a constant, two separate populations with rates 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 encode the prior 
\begin_inset Formula $p_{0}(\vz)$
\end_inset

 and posterior 
\begin_inset Formula $p_{0}(\vz|\vx)$
\end_inset

, respectively, about 
\begin_inset Formula $\vz$
\end_inset

.
 
\begin_inset Formula 
\begin{align}
q(\vz|\vr_{X}) & \propto\exp\left[\vpsi(\vz)\cdot\mW_{Z}\cdot\vr_{Z}\right]\label{eq:NBR_ppc_prior}\\
q(\vz|\vr_{Z|X}) & \propto\exp\left[\vpsi(\vz)\cdot\mW_{Z|X}\cdot\vr_{Z|X}\right]\label{eq:NBR_ppc_posterior}
\end{align}

\end_inset

It is desirable for 
\begin_inset Formula $q(\vz|\vr_{X})=p(\vz)$
\end_inset

, 
\begin_inset Formula $q(\vz|\vr_{Z|X})=p(\vz|\vx)$
\end_inset

 and 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 to be a function of 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

, the neural representations for the prior and observation.
 By matching the natural parameters between 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_prior"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior"

\end_inset

 (taking into account 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

), 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_posterior"

\end_inset

and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_posterior"

\end_inset

, we have
\begin_inset Formula 
\begin{align}
\mW_{Z}\cdot\vr_{Z} & =\vtheta_{Z}\label{eq:NBR_ppc_prior_np}\\
\mW_{Z|X}\cdot\vr_{Z|X} & =\mM_{ZX}\cdot\vx+\vtheta_{Z}
\end{align}

\end_inset

which gives 
\begin_inset Formula 
\[
\mW_{Z|X}\cdot\vr_{Z|X}=\mM_{ZX}\cdot\vx+\mW_{Z}\cdot\vr_{Z}
\]

\end_inset

.To express 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 in terms of 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

, the author further assumes (akin to 
\begin_inset CommandInset citation
LatexCommand citep
key "BeckEtAl2007"

\end_inset

) that 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\mW_{Z}$
\end_inset

 are related to 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 through linear recoding
\begin_inset Formula 
\begin{align}
\mM_{ZX} & =\mW_{Z|X}\mfont V{}_{Z|X\to ZX}\nonumber \\
\mW_{Z} & =\mW_{Z|X}\mfont V{}_{Z|X\to Z}\label{eq:NBR_W_map}
\end{align}

\end_inset

Finally, the encoding for the posterior 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 can be expressed in terms of prior encoding 
\begin_inset Formula $\vr_{X}$
\end_inset

 and the number of spike
\begin_inset Formula $\vx$
\end_inset


\begin_inset Formula 
\[
\vr_{Z|X}=\mfont V{}_{Z|X\to ZX}\vx+\mfont V{}_{Z|X\to Z}\vr_{Z}
\]

\end_inset

which is the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\begin_inset Foot
status open

\begin_layout Plain Layout
referred to as neural Bayes rule by 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"

\end_inset


\end_layout

\end_inset

.
 Compared to 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 framework proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2007"

\end_inset

, the present formulation differs in that
\end_layout

\begin_layout Enumerate
the prior 
\begin_inset Formula $p(\vz)$
\end_inset

 is not flat for decoding and is explicitly represented by a neural population
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\vz$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

 are connected by a joint distribution in the form of an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We note here that the observations 
\begin_inset Formula $\vz$
\end_inset

 is implied by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 joint and hence lives in the exponential family, which can be restrictive.
 Also, the assumption for constant firing rate 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

 may not hold for tuning curves that are asymmetric (e.g.
 sigmoidal), and holds as a result of using symmetric tuning functions and
 Poisson neurons; indeed, as with previous work on 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

, Gaussian tuning curves 
\begin_inset Formula $f_{i}$
\end_inset

 are used in all experiments, which means that 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_poisson"

\end_inset

 are determined by parameters in 
\begin_inset Formula $f_{i}$
\end_inset

.
 Nonetheless, using this construction, belief update of the latent can be
 achieved by recursively using the natural parameter of the posterior at
 time 
\begin_inset Formula $t$
\end_inset

 as the that of the prior at 
\begin_inset Formula $t+1$
\end_inset

.
\end_layout

\begin_layout Paragraph
Forward inference and learning using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\end_layout

\begin_layout Standard
In filtering, we would like to infer the latent state 
\begin_inset Formula $\vz_{t}$
\end_inset

 given all Poisson observation 
\begin_inset Formula $\vx_{1:t}$
\end_inset

, whose belief is represented by rate 
\begin_inset Formula $\vr_{t|t}:=\vr_{Z_{t}|X_{1:t}}$
\end_inset

; moreover, the parameters of the internal model that relates 
\begin_inset Formula $\vz_{t}$
\end_inset

, 
\begin_inset Formula $\vx_{t}$
\end_inset

 and rates 
\begin_inset Formula $\vr$
\end_inset

's should be adjusted to statistics of incoming data.
 
\end_layout

\begin_layout Standard
For inference, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule facilitates evidence incorporation that updates 
\begin_inset Formula $p(\vz_{t}|\vx_{1:t-1})$
\end_inset

 to 
\begin_inset Formula $p(\vz_{t}|\vx_{1:t})$
\end_inset

 at each time 
\begin_inset Formula $t$
\end_inset

, but it does not tell how to compute the predictive distribution 
\begin_inset Formula $p(\vz_{t}|\vx_{1:t-1})$
\end_inset

 from the previous posterior 
\begin_inset Formula $p(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

; that is, in terms of PPC, how to map from 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 to 
\begin_inset Formula $\vr_{t|t-1}$
\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand citep
key "BeckEtAl2011"

\end_inset

, the dynamics in 
\begin_inset Formula $\vx_{t}$
\end_inset

 considered is a continuous time stationary Markov process, the solution
 derived there is specific to the linear drift.
 Instead, 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"

\end_inset

 proposes to train a generic neural network 
\begin_inset Formula $\phi(\cdot;\mW_{\phi})$
\end_inset

 , parameterised by 
\begin_inset Formula $\mW_{\phi}$
\end_inset

, which is learned on data to optimize the likelihood objective (see below).
 Thus, 
\begin_inset Formula $\mW_{\phi}$
\end_inset

 is a generative parameter of 
\begin_inset Formula $p_{\phi}(\vz_{t}|\vz_{1:t-1})$
\end_inset

 which approximates an arbitrary true dyanmics 
\begin_inset Formula $p(\vz_{t}|\vz_{1:t-1})$
\end_inset

.
\end_layout

\begin_layout Standard
For learning, it turns out that some parameters are restricted by practical
 use of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule.
 The authors restrict 
\begin_inset Formula $\mW_{Z}=\mW_{Z|X}$
\end_inset

, so
\begin_inset Formula $\mfont V{}_{Z|X\to Z}$
\end_inset

 is the identity matrix.
 In addition, given the Gaussian tuning curve restrictions, 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 are determined by tuning parameters of 
\begin_inset Formula $f_{i}$
\end_inset

.
 So the only free parameters are 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 and the tuning parameters, and the recoder 
\begin_inset Formula $\mfont V{}_{Z|X\to M}$
\end_inset

 is constrained.
 Ideally, one can optimize all these free parameters and 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

, as well as 
\begin_inset Formula $\mW_{\phi}$
\end_inset

.
 In the experiment of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"

\end_inset

, the author fixes 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 and only trains 
\begin_inset Formula $\mW_{\phi}$
\end_inset

.
\end_layout

\begin_layout Standard
The objective for online learning is the conditional likelihood 
\begin_inset Formula $p(\vx_{t}|\vx_{1:t-1})$
\end_inset

, which is similar to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior_np"

\end_inset

 but with a different natural parameter 
\begin_inset Formula $\vtheta_{Z}$
\end_inset

 which is encoded by 
\begin_inset Formula $\vr_{t|t-1}=\phi(\vr_{t-1|t-1};\mW_{\phi})=:\phi_{t-1}(\mW_{\phi})$
\end_inset

 using 
\begin_inset Formula $\mW_{Z}$
\end_inset

.
 Substituting this into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior_np"

\end_inset

 and then into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_lik"

\end_inset

, and adding the time indices, the log likelihood is 
\begin_inset Formula 
\begin{align*}
p_{0}(\vx_{t}|\vx_{1:t-1};\mW_{\phi}) & \propto\nu(\vx_{t})\exp\left[\vtheta_{X}\cdot\vx_{t}+Z_{Z}(\mW_{Z}\cdot\phi_{t-1}(\mW_{\phi})+\mM_{ZX}\cdot\vx_{t})\right]
\end{align*}

\end_inset

Using contrastive divergence-like approach for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, and the fact that 
\begin_inset Formula $\nabla Z_{Z}(\theta)=\E{p(Z)}{\vpsi(\vz)}$
\end_inset

 when 
\begin_inset Formula $\theta$
\end_inset

 is the natural parameter of 
\begin_inset Formula $p(Z)$
\end_inset

, it can be shown that the gradient for 
\begin_inset Formula $\mW_{\phi}$
\end_inset

 is
\begin_inset Formula 
\begin{equation}
\nabla_{\mW_{\phi}}\log p_{0}(\vx_{t}|\vx_{1:t-1};\mW_{\phi})=\left[\E{q(\vz_{t}|x_{1:t})}{\vpsi(\vz_{t})}-\E{q(\vz_{t}|x_{1:t-1})}{\vpsi(\vz_{t})}\right]\cdot\mW_{Z}\cdot\nabla_{\mW_{\phi}}\vr_{t|t-1}\label{eq:NBR_learning}
\end{equation}

\end_inset


\begin_inset Formula $\nabla_{\mW_{\phi}}\vr_{t|t-1}$
\end_inset

 has long-range dependencies as 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 is applied recursively.
 However, if inference is optimal, 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 contains all stimulus information, since the generative process is Markov.
 Thus, the author uses the approximation 
\begin_inset Formula $\nabla_{\mW_{\phi}}\vr_{t|t-1}\approx\nabla_{\mW_{\phi}}\phi(\vr_{t-1|t-1};\mW_{\phi})$
\end_inset

 and treats 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 as a constant, which avoids back-propagation through time.
 The two expectations can be evaluated either by 
\end_layout

\begin_layout Enumerate
Gibbs sampling, assuming Gaussian posterior 
\begin_inset Formula $p_{0}(\vz_{t}|\vx_{t},\vx_{1:t-1})$
\end_inset

 induced by Gaussian tuning curves, or 
\end_layout

\begin_layout Enumerate
direct computation of Gaussian mean parameters for both terms, assuming
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

.
\end_layout

\begin_layout Standard
In the experiments, the author tested the performance of learning the transition
 dynamics by evaluating the log likelihood of data.
 In addition to testing the two ways of approximating the expectations in
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_learning"

\end_inset

, two choices of 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 was also compared,
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mW_{Z|X}=\mM_{ZX}$
\end_inset

 (naive code), which implies that the first row, which projects 
\begin_inset Formula $\psi_{1}(\vz)=\vz$
\end_inset

, is proportional to the centres of the Gaussian tuning functions; and the
 second row, which projects 
\begin_inset Formula $\psi_{2}(\vz)=\vz\vz^{\intercal}$
\end_inset

, is a constant vector; 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 with rows that are orthonormal to each other and also to the 1-vector (orthonor
mal code), as laid out in 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2011"

\end_inset


\end_layout

\begin_layout Standard
Then it may not be so surprising that, the restrictive encoding of uncertainty
 (second moment) in the naive code may be very sub-optimal, which is what
 the author observed in experiments.
 Interestingly, it is reported that when treating 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 as a constant in order to avoid backpropagation through time, the model
 cannot discover second-order latent variables, such as velocity when observing
 the position of an oscillatory signal.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 is related to the recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "MakinEtAl2015"

\end_inset

 but are fundamentally different.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 takes a density density estimation approach and treats the latent code
 
\begin_inset Formula $\vr_{t}$
\end_inset

 as features, but not an explicit encoding of the latent variables 
\begin_inset Formula $\vz_{t}$
\end_inset

 as in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 which does encode natural parameter of the posterior, a 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 allows arbitrary transition dynamics to learned by back propagation; whereas
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 does not explicitly learn any transition dynamics, but the overall learning
 uses the more biologically plausible contrastive divergence rule.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 maximizes the incremental log likelihood of 
\begin_inset Formula $\log p(\vx_{t}|\vx_{1:t-1})$
\end_inset

, whereas 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 does not optimize any principled cost function.
 
\end_layout

\begin_layout Standard
The only part common to both approaches is the choice of likelihood via
 Gaussian-tuned Poisson neurons, which is subsumed by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 framework.
 Nonetheless, this choice may be important for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 to admit the approximation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

, but less so for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

, as the density estimation formulation is agnostic to the generative model,
 as long as the support of observation distribution in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 include the data support.
\end_layout

\begin_layout Subsubsection
General machine learning algorithms
\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\z}{\mathbf{z}}
{\mathbf{z}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "MarinoEtAl2018"

\end_inset

 studied a more general non-Markovian state-space model the joint distribution
 factorises as 
\begin_inset Formula $p(\x_{\le T},\z_{\le T}):=\prod_{t=1}^{T}p\left(\x_{t},\z_{t}|\x_{<t},\z_{<t}\right)=\prod_{t=1}^{T}p\left(\x_{t}|\x_{<t},\z_{\le t}\right)p(\z_{t}|\x_{<t},\z_{<t})$
\end_inset

.
 Though it complicates full inference, the authors realise that the free
 energy, when factorised over time, can be optimized for each time step
 in the forward direction, permitting online filtering and learning.
 In addition, at each time step, filtering involves an inner loop of amortized
 inference at each time step 
\begin_inset Formula $t$
\end_inset

 where gradient updates were performed on the free energy term at time 
\begin_inset Formula $t$
\end_inset

, taking the posterior at 
\begin_inset Formula $t-1$
\end_inset

 as the prior.
 
\end_layout

\begin_layout Standard
The Kalman VAE 
\begin_inset CommandInset citation
LatexCommand citet
key "FraccaroEtAl2017"

\end_inset

 incorporates a standard VAE into a chain of Linear Gaussian state space
 model (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

) in which the linear matrices evolve over time.
 In the generative model, an intermediate sequence is first generated from
 the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

, using potentially time-dependent transition matrices.
 These intermediate latents are then fed into a nonlinear neural network
 to obtain observations.
 In order to allow for nonlinear dynamics, the linear transition matrices
 were functions of time and intermediate outputs.
 More specifically, let the intermediate latent at time 
\begin_inset Formula $t$
\end_inset

 be 
\begin_inset Formula $a_{t}$
\end_inset

, the dynamics matrix in the latent 
\begin_inset Formula $A=\sum_{k=1}^{K}\alpha_{t}^{(k)}\left(a_{0:t-1}\right)A^{\left(k\right)}$
\end_inset

, where 
\begin_inset Formula $\alpha_{t}^{(k)}$
\end_inset

 is a series of coefficients generated by recurrent neural network.
 These coefficients are used to linearly interpolate between 
\begin_inset Formula $K$
\end_inset

 linear modes (
\begin_inset Formula $\alpha_{t}^{(k)}\in[0,1]$
\end_inset

,
\begin_inset Formula $\sum_{k=1}^{K}\alpha_{t}^{(k)}=1$
\end_inset

).
 The control and observation matrices are constructed in the same fashion.
 Thus, these dynamics are functions of the samples generated from reparameterize
d Gaussian distributions, enabling learning by backpropagation.
 During inference, the message sent from each observation is a Gaussian
 distribution produced from a standard VAE.
 Filtering and smoothing are then analytically tractable, taking samples
 from the observation-only posterior as the usual observation for the LGSSM.
 Thus, although the latent variables follow linear dynamics, the linear
 matrices change over time and can depend on the observation history, giving
 the generative model the capacity of producing complex dynamics while maintaini
ng inferential tractability, provided that the observations were encoded
 into a Gaussian distribution.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LGSSM"
description " "

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printglossaries
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ref"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document

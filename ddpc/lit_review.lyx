#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass extreport
\begin_preamble
\setcitestyle{round}
% call the glossaries package
\usepackage[nonumberlist]{glossaries} 
\usepackage{hyperref}
\usepackage{cleveref}
% activate the glossaries building
\renewcommand*{\glstextformat}{\textbf}
\makeglossaries
\usepackage{atveryend}

% redefine the command that LyX puts out, to the glossaries input
\let\nomenclature\gls

% add your glossaries entries
\newglossaryentry{linux}{name={linux},description={an open-source unix implementation}}
\newglossaryentry{SSM}{name={SSM},description={State Space Model}}
\newglossaryentry{LGSSM}{name={LGSSM},description={Linear Gaussian State Space Model}}
\newglossaryentry{NPF}{name={NPF}, description={Neural Particle Filter by Kutschireiter et al. (2017)}}
\newglossaryentry{PPC}{name={PPC}, description={Probabilistic population code}}
\newglossaryentry{EFH}{name={EFH}, description={Exponential family harmonium}}
\newglossaryentry{rEFH}{name={rEFH}, description={Recurrent Exponential family harmonium for filtering by Makin et al. (2015)}}
\newglossaryentry{PPC-BR-EFH}{name={PPC-BR-EFH}, description={Probabilistic population code Bayes rule with exponential family harmonium by Sokoloski (2017)}}
\end_preamble
\use_default_options true
\begin_modules
theorems-ams-chap-bytype
theorems-ams-extended-chap-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command bibtex
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\rvfont}[1]{\boldsymbol{#1}}
{\boldsymbol{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vfont}[1]{\mathbf{#1}}
{\mathbf{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mfont}[1]{\mathbf{#1}}
{\mathrm{\boldsymbol{#1}}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vtheta}{\vfont{\theta}}
{\vfont{\theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vn}{\vfont n}
{\vfont n}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\vfont w}
{\vfont w}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vomega}{\boldsymbol{\omega}}
{\boldsymbol{\omega}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vz}{\vfont z}
{\vfont z}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vs}{\vfont s}
{\vfont s}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vX}{\rvfont X}
{\rvfont X}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vZ}{\rvfont Z}
{\rvfont Z}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vN}{\rvfont N}
{\rvfont N}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vf}{\vfont f}
{\vfont f}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vg}{\vfont g}
{\vfont g}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vR}{\rvfont R}
{\rvfont R}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vx}{\vfont x}
{\vfont x}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vpsi}{\boldsymbol{\psi}}
{\boldsymbol{\psi}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vzeta}{\boldsymbol{\zeta}}
{\boldsymbol{\zeta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vphi}{\boldsymbol{\phi}}
{\boldsymbol{\phi}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mtheta}{\mathbf{\Theta}}
{\mathbf{\Theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mW}{\mfont W}
{\mfont W}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mM}{\mfont M}
{\mfont M}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mSigma}{\mfont{\Sigma}}
{\mfont{\Sigma}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mV}{\mfont V}
{\mfont V}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\N}{\mathbb{N}}
{\mathbf{\mathbb{N}}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sM}{\mathcal{M}}
{\mathcal{M}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Dim}[1]{K_{#1}}
{K_{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sigm}{\operatorname{sigm}}
{\textrm{sigm}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Pois}{\operatorname{Pois}}
{\textrm{Pois}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Bern}{\operatorname{Bern}}
{\textrm{Bern}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\KL}{\operatorname{KL}}
{\textrm{KL}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\NN}{\operatorname{NN}}
{\textrm{NN}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vectorfont}[1]{\boldsymbol{#1}}
{\boldsymbol{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\setfont}[1]{\mathbb{#1}}
{\mathbb{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\matrixfont}[1]{\mathbf{#1}}
{\mathbf{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\graphfont}[1]{\mathcal{#1}}
{\mathcal{#1}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\vz}{\vectorfont z}
{\vectorfont z}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vx}{\vectorfont x}
{\vectorfont x}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vy}{\vectorfont y}
{\vectorfont y}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vz}{\vectorfont z}
{\vectorfont z}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vphi}{\vectorfont{\phi}}
{\vectorfont{\phi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\valpha}{\vectorfont{\alpha}}
{\vectorfont{\alpha}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vpsi}{\vectorfont{\psi}}
{\vectorfont{\psi}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vf}{\vectorfont f}
{\vectorfont f}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vtheta}{\vectorfont{\theta}}
{\vectorfont{\theta}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vgamma}{\vectorfont{\gamma}}
{\vectorfont{\gamma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vepsilon}{\vectorfont{\epsilon}}
{\vectorfont{\epsilon}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vb}{\vectorfont b}
{\vectorfont b}
\end_inset


\begin_inset FormulaMacro
\newcommand{\softplus}{\textrm{softplus}}
{\textrm{softplus}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\vw}{\vectorfont w}
{\vectorfont w}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vm}{\vectorfont m}
{\vectorfont m}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vmu}{\vectorfont{\mu}}
{\vectorfont{\mu}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vr}{\vectorfont r}
{\vectorfont r}
\end_inset


\begin_inset FormulaMacro
\newcommand{\va}{\vectorfont a}
{\vectorfont a}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vh}{\vectorfont h}
{\vectorfont h}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vv}{\vectorfont v}
{\vectorfont v}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mY}{\matrixfont Y}
{\matrixfont Y}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mI}{\matrixfont I}
{\matrixfont I}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\mSigma}{\matrixfont{\Sigma}}
{\matrixfont{\Sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mPhi}{\matrixfont{\Phi}}
{\matrixfont{\Phi}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\mPsi}{\matrixfont{\Psi}}
{\matrixfont{\Psi}}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\mW}{\matrixfont W}
{\matrixfont W}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\gH}{\graphfont H}
{\graphfont H}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gN}{\graphfont N}
{\graphfont N}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gD}{\graphfont D}
{\graphfont D}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gU}{\graphfont U}
{\graphfont U}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gF}{\graphfont F}
{\graphfont F}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gG}{\graphfont G}
{\graphfont G}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gQ}{\graphfont Q}
{\graphfont Q}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gR}{\graphfont R}
{\graphfont R}
\end_inset


\begin_inset FormulaMacro
\newcommand{\gX}{\graphfont X}
{\graphfont X}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\sR}{\setfont R}
{\setfont R}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
{\mathbb{E}{}_{#1}\left[#2\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatorname*{\arg\min}}
{\textrm{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Dx}{\Delta_{\vtheta}(\vx)}
{\Delta_{\vtheta}(\vx)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\Dxt}{\Delta_{\vtheta_{t}}(\vx)}
{\Delta_{\vtheta_{t}}(\vx)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\KL}{\mathrm{KL}}
{\mathrm{KL}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\renewcommand{\NN}{\mathrm{NN}}
{\mathrm{NN}}
\end_inset


\end_layout

\begin_layout Chapter
Literature review
\end_layout

\begin_layout Section
Approximate inference and learning 
\end_layout

\begin_layout Subsection
Variational learning
\end_layout

\begin_layout Standard
Consider a generative model described by a directed graph 
\begin_inset Formula $\vZ_{0}\rightarrow\vZ_{2}\rightarrow\cdots\rightarrow\vZ_{L}$
\end_inset

, where 
\begin_inset Formula $\vZ_{0}\sim p_{\theta_{0}}\left(\vz_{0}\right)$
\end_inset

 and each arrow suggests a parametric conditional distribution 
\begin_inset Formula $p_{\theta_{l}}(\vZ_{l}|\vZ_{l-1}),l\in[1\dots,L]$
\end_inset

.
 In addition, denote 
\begin_inset Formula $\vX=\vZ_{L}$
\end_inset

, and 
\begin_inset Formula $\vZ=\left\{ \vZ_{0},\cdots\vZ_{L-1}\right\} $
\end_inset

.
 The goal is to fit the parameters 
\begin_inset Formula $\theta=\{\theta_{l}\}_{l=1}^{L}$
\end_inset

 in order to maximize the likelihood of some observed data 
\begin_inset Formula $\vx=\left\{ \vx_{n}\right\} _{n=1}^{N}\sim\Pi(\vx)$
\end_inset

 i.i.d from some unknown true distribution 
\begin_inset Formula $\Pi(\vx)$
\end_inset

.
 Although the set of conditional distributions implies a normalized joint
 distribution 
\begin_inset Formula $p_{\theta}(\vZ,\vX)=p_{\theta_{0}}\left(\vZ_{0}\right)\prod_{l=1}^{L}p_{\theta_{l}}\left(\vZ_{l}|\vZ_{l-1}\right)$
\end_inset

 , evaluating the likelihood of data 
\begin_inset Formula $\mathcal{L}(\theta):=\log p_{\theta}(\vx)$
\end_inset

 involves the sum 
\begin_inset Formula $\sum_{\vZ}p_{\theta}(\vZ,\vX)$
\end_inset

, which is computationally intractable.
 Instead, we evaluate the free energy
\begin_inset Formula 
\begin{align*}
\mathcal{F}(\theta,q): & =\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX)-\log(q(\vZ|\vX))}\\
 & =\mathcal{L}(\theta)-\KL\left[q(\vZ|\vX)\|p(\vZ\|\vX)\right]\\
 & \le\mathcal{L}(\theta)
\end{align*}

\end_inset

where 
\begin_inset Formula $q(\vZ|\vX)$
\end_inset

 is the variational distribution that can depend on 
\begin_inset Formula $\vX$
\end_inset

.
 It can be shown that the bound is tight if and only if 
\begin_inset Formula $\KL\left[\log q\left(\vZ|\vX\right)\|\log p_{\theta}\left(\vZ|\vX\right)\right]=0$
\end_inset

.
 Thus, maximizing 
\begin_inset Formula $\mathcal{F}(\theta,q)$
\end_inset

 involves an iterative process that in each iteration alternates between
\end_layout

\begin_layout Enumerate
(M-step) maximizing 
\begin_inset Formula $\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX))}$
\end_inset


\end_layout

\begin_layout Enumerate
(E-step) minimizing 
\begin_inset Formula $\KL\left[\log q\left(\vZ|\vX\right)\|\log p_{\theta}\left(\vZ|\vX\right)\right]$
\end_inset


\end_layout

\begin_layout Standard
By defining the joint distribution 
\begin_inset Formula $q_{\phi}(\vZ,\vX):=q_{\phi}(\vZ|\vX)\Pi(\vX)$
\end_inset

, the M-step is also equivalent to
\begin_inset Formula 
\begin{equation}
\min_{\theta}\KL\left[q_{\phi}(\vZ,\vX)\|p_{\theta}(\vZ,\vX)\right]\label{eq:M_step_joint_KL}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Helmholtz machine
\end_layout

\begin_layout Standard
The wake-sleep algorithm by 
\begin_inset CommandInset citation
LatexCommand citet
key "HintonEtAl1995"
literal "true"

\end_inset

, designed for the classical Helmholtz machine proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "DayanEtAl1995"
literal "true"

\end_inset

, performs the M-step by samples from an approximate posterior 
\begin_inset Formula $q_{\phi}(\vZ|\vX)$
\end_inset

, and the posterior samples are obtained from an inferential model using
 fixed inferential parameters 
\begin_inset Formula $\phi$
\end_inset

.
 Because the generative model is Markov, the approximate (and true) posteriors
 factorizes
\begin_inset Formula 
\[
q_{\phi}(\vZ|\vX)=\prod_{l=0}^{L-1}q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})
\]

\end_inset


\end_layout

\begin_layout Standard
Thus, posterior samples can be obtained by a sequence of conditional sampling:
 given an input 
\begin_inset Formula $\vx$
\end_inset

, draws 
\begin_inset Formula $\vZ_{L-1}$
\end_inset

 given 
\begin_inset Formula $\vx$
\end_inset

, and then draws 
\begin_inset Formula $\vZ_{L-2}$
\end_inset

 conditioned on each a posterior sample 
\begin_inset Formula $\vz_{L-1}|\vx$
\end_inset

, and so on.
 Each factor 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})$
\end_inset

 is implemented as a distribution with tractable sampling routine and flexible
 parameterisation, such as an exponential family distribution with parameters
 that are output of neural networks.
 For example, if 
\begin_inset Formula $\vZ$
\end_inset

 is binary, then we may choose conditionally independent posterior 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vz_{l+1})=\prod_{d}q_{\phi_{l,d}}(Z_{l,d}|\vz_{l+1})$
\end_inset

, 
\begin_inset Formula $q_{\phi_{l}}(Z_{l,d}|\vz_{l+1})=\Bern(Z_{l,d}|\NN(\vz_{l+1};\phi_{l,d}))$
\end_inset

, which in addition assumes conditional independence.
 To train 
\begin_inset Formula $\phi_{l}$
\end_inset

, generative samples 
\begin_inset Formula $\left\{ \vz_{l},\vz_{l+1}\right\} $
\end_inset

are drawn from 
\begin_inset Formula $p_{\theta}(\vZ_{l},\vZ_{l+1})$
\end_inset

 by ancestral sampling, and the objective is to maximize the likelihood
 of 
\begin_inset Formula $\vz_{l}$
\end_inset

 given 
\begin_inset Formula $\vz_{l+1}$
\end_inset

, averaged over different 
\begin_inset Formula $\vz_{l+1}$
\end_inset


\begin_inset Formula 
\[
\max_{\phi_{l}}\E{p_{\theta}(\vZ_{l},\vZ_{l+1})}{\log q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})}
\]

\end_inset

When the same procedure is used for all layers, it can be shown that this
 process is equivalent to minimizing the average reverse KL divergence 
\begin_inset Formula 
\[
\min_{\phi}\E{p_{\theta}(\vX)}{\KL\left[p_{\theta}(\vZ|\vX)\|q_{\phi}(\vZ|\vX)\right]}
\]

\end_inset

Or equivalently, by defining the joint distribution 
\begin_inset Formula $q_{\phi}(\vZ,\vX):=q_{\phi}(\vZ|\vX)\Pi(\vX)$
\end_inset


\begin_inset Formula 
\[
\min_{\phi}\KL\left[p_{\theta}(\vZ,\vX)\|q_{\phi}(\vZ,\vX)\right]
\]

\end_inset

This step of using generated sample to train the inferential model is known
 as the 
\begin_inset Quotes eld
\end_inset

sleep phase
\begin_inset Quotes erd
\end_inset

 .
 Note that the objective been minimized here is different from the KL in
 the usual E-step.
\end_layout

\begin_layout Standard
In the 
\begin_inset Quotes eld
\end_inset

wake phase
\begin_inset Quotes erd
\end_inset

, the inferential model takes real observations 
\begin_inset Formula $\vX\sim\Pi(\vx)$
\end_inset

 and produces posterior samples from 
\begin_inset Formula $q_{\phi}(\vZ|\vX)$
\end_inset

.
 These samples are used to perform the usual M-step, which corresponds to
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:M_step_joint_KL"

\end_inset

.
 Therefore, the wake-sleep algorithm does not optimize a consistent divergence,
 although if 
\begin_inset Formula $p_{\theta}(\vZ,\vX)=q_{\phi}(\vZ,\vX)$
\end_inset

, both losses are at global minimum 0.
\end_layout

\begin_layout Standard
Particular instances of Helmholtz machine are biologically attractive.
 When 
\begin_inset Formula $p_{\theta_{l}}(\vZ_{l}|\vZ_{l-1})$
\end_inset

 and 
\begin_inset Formula $q_{\phi_{l}}(\vZ_{l}|\vZ_{l+1})$
\end_inset

 is in the exponential family, conditionally independent, and each factor
 is parameterized by its conditional mean in the form of the canonical link
 function on 
\begin_inset Formula $\vz_{l+1}$
\end_inset

 (a generalized linear model), then the gradient of 
\begin_inset Formula $\E{q(\vZ|\vX)}{\log p_{\theta}(\vZ,\vX))}$
\end_inset

 w.r.t 
\begin_inset Formula $\theta$
\end_inset

 and the gradient of 
\begin_inset Formula $\E{p(\vZ,\vX)}{\log q_{\phi}(\vZ|\vX)}$
\end_inset

 w.r.t 
\begin_inset Formula $\phi$
\end_inset

 only depend on local samples, so learning in the generative and inferential
 model can be done using the biologically plausible learning rule.
 This is the case for the sigmoid belief net 
\begin_inset Formula $p$
\end_inset

 and 
\begin_inset Formula $q$
\end_inset

 in 
\begin_inset CommandInset citation
LatexCommand citep
key "DayanEtAl1995"
literal "true"

\end_inset

 and generalisations in 
\begin_inset CommandInset citation
LatexCommand citep
key "DayanHinton1996"
literal "true"

\end_inset

.
\end_layout

\begin_layout Section
Neural representation of uncertainty
\end_layout

\begin_layout Subsection
Probabilistic population code (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
The probabilistic population code was proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "MaPouget2006Bayesian,BeckEtAl2007"
literal "false"

\end_inset

 as a way for representing a distribution by a group of noisy neurons.
 It considers how neuronal activities 
\begin_inset Formula $\vr:=\left[r_{m}\right]_{m=1}^{M},r_{m}\in\mathcal{R}$
\end_inset


\begin_inset Foot
status open

\begin_layout Plain Layout
not to be confused with the real domain 
\begin_inset Formula $\mathbb{R}$
\end_inset


\end_layout

\end_inset

 could represent a stimulus 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

 (assumed to be scalar for simplicity), and how this representation could
 be used for a variety of tasks.
 The following main claims were made in the initial proposal 
\begin_inset CommandInset citation
LatexCommand citep
key "MaPouget2006Bayesian,BeckEtAl2007"
literal "false"

\end_inset

:
\end_layout

\begin_layout Enumerate
neuronal variability encodes probability distributions over 
\begin_inset Formula $x$
\end_inset

;
\end_layout

\begin_layout Enumerate
probabilistic computations, such as cue combination, can be carried out
 by simple operations in 
\begin_inset Formula $\mathcal{R}$
\end_inset

;
\end_layout

\begin_layout Enumerate
the above properties hold for arbitrary tuning curves.
\end_layout

\begin_layout Standard
I remark each of these claims as I reproduce the PPC theory.
 Given a stimulus 
\begin_inset Formula $x$
\end_inset

, Poisson-like neural population with tuning functions 
\begin_inset Formula $\vphi(x):=\left[\phi_{m}(x)\right]_{m}^{M}$
\end_inset

 encodes 
\begin_inset Formula $x$
\end_inset

 by a likelihood function 
\begin_inset Formula 
\begin{equation}
p(\vr|x)=\frac{1}{Z(x)}\nu(\vr)\exp\left[\vh(x)\cdot\vr\right].\label{eq:PPC_lik}
\end{equation}

\end_inset

A typical example is the Poisson firing neurons for which 
\begin_inset Formula $h_{m}(x)=\log\phi_{m}(x)$
\end_inset

 , also known as the response kernel.
 Under the uniform prior over 
\begin_inset Formula $x$
\end_inset

, this likelihood imples the following unnormalised posterior distribution
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
p(x|\vr)\propto\frac{1}{Z(x)}\exp\left[\vh(x)\cdot\vr\right],\label{eq:PPC_post}
\end{equation}

\end_inset

which belongs to the exponential family with 
\begin_inset Formula $\vh(x)$
\end_inset

 being the sufficient statistics, and 
\begin_inset Formula $\vr$
\end_inset

 is the natural parameter.
 Note that, although the tuning curves may form a rich set of basis functions,
 their logarithm 
\begin_inset Formula $\vh(x)$
\end_inset

 may not.
 An example is that Gaussian-shaped 
\begin_inset Formula $\vphi(x)$
\end_inset

 always implies a Gaussian 
\begin_inset Formula $p(x|\vr)$
\end_inset

.
 Based on the encoding construction 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_lik"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and the decoded posterior 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

, I make the following remarks: 
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Uncertainty encoded by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 
\end_layout

\end_inset

The stimulus 
\begin_inset Formula $x$
\end_inset

 is in fact deterministic with no uncertainty.
 The uncertainty in the posterior 
\begin_inset Formula $p(x|\vr)$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is induced in decoding through the likelihood 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_lik"
plural "false"
caps "false"
noprefix "false"

\end_inset

 while incorporating a uniform prior 
\begin_inset Formula $p(s)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 as log linear codes
\end_layout

\end_inset

We can interpret PPC defined by 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

 as a log linear encoding of distribution with basis functions 
\begin_inset Formula $\vh(x)$
\end_inset

 and linear weight, or natural parameters, 
\begin_inset Formula $\vr$
\end_inset

 and base measure 
\begin_inset Formula $\frac{1}{Z(x)}$
\end_inset

.
 A generic base measure may complicate downsteram computations.
 This can be aleviated with further assumption about the tuning curves.
 For example, for Poisson neurons with that encodes 
\begin_inset Formula $x$
\end_inset

 by 
\begin_inset Formula 
\[
p(x|\vr)\propto p(\vr|x)=\prod_{n=1}^{N}\text{Pois}\left(r_{m}|\phi_{m}(x)\right)\propto\frac{1}{\exp\left(\sum_{m=1}^{M}\phi_{m}(x)\right)}\exp\left(\vr\cdot\vh(\vx)\right),
\]

\end_inset

under the condition that the total firing rate is independent of 
\begin_inset Formula $x$
\end_inset


\begin_inset Formula 
\begin{equation}
\sum_{m=1}^{M}\phi_{m}(x)\approx C(\vphi),\label{eq:PPC_total_firing_rate}
\end{equation}

\end_inset

the inverse base measure 
\begin_inset Formula $Z(x)=\exp\left(\sum_{m=1}^{M}\phi_{m}(x)\right)$
\end_inset

 can be assumed to be a constant for any 
\begin_inset Formula $x$
\end_inset

.
 This is possible if 
\begin_inset Formula $\vphi$
\end_inset

 is densely arranged on 
\begin_inset Formula $\gX$
\end_inset

 and translation-invarant.
 Under the log linear code interpretation, the source of uncertainty is
 simply from the definition and depends on 
\begin_inset Formula $\vh$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 as sub-optimal decoding
\end_layout

\end_inset

We may take 
\begin_inset Formula $p(x|\vr)$
\end_inset

 as Bayesian decoding of what is encoded about 
\begin_inset Formula $x$
\end_inset

 by a particular firing pattern 
\begin_inset Formula $\vr$
\end_inset

.
 Optimal decoding requires that the brain have the exact represents for
 
\begin_inset Formula $p(\vr|s)$
\end_inset

 and of 
\begin_inset Formula $p(s)$
\end_inset

.
 While the former depends on the property of neurons and hence may be known
 by the brain, the latter is usually not flat even for visual orientations
 (oblique effect), not to mension higher dimensional stimuli, such as images.
 Thus, decoding with a flat prior to obtain 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:PPC_post"
plural "false"
caps "false"
noprefix "false"

\end_inset

 implies sub-optimality in the uncertainty represented, meaning if may not
 reflect the real distribution of 
\begin_inset Formula $s$
\end_inset

 in the environment that causes 
\begin_inset Formula $\vr$
\end_inset

.
 One may hope that 
\begin_inset Formula $p(s)$
\end_inset

 can be acquired through experience, but this is largely ignored by the
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature.
 
\begin_inset CommandInset label
LatexCommand label
name "rem:PPC_suboptimal_decoding"

\end_inset


\end_layout

\begin_layout Standard
Under the interpretation of Remark 
\begin_inset CommandInset ref
LatexCommand ref
reference "rem:PPC_suboptimal_decoding"
plural "false"
caps "false"
noprefix "false"

\end_inset

, PPC code is invariant to nuisance variables 
\begin_inset Formula $c$
\end_inset

 that enter the likelihood through an additional term in the exponent
\begin_inset Formula 
\[
p(\vr|x,c)=\frac{1}{Z(x)}\nu(\vr)\exp\left[\vh(x)\cdot\vr+g(\vr,c)\right].
\]

\end_inset

Then decoding by Bayes rule gives 
\begin_inset Formula 
\[
p(x|\vr,c)=\frac{p(\vr|x,c)p(x|c)}{p(\vr|c)}\propto p(x|c)\frac{1}{Z(x)}
\]

\end_inset


\end_layout

\begin_layout Standard
PPC can be used to perform cue combination.
 Consider two neural populations with responses described by 
\begin_inset Formula 
\begin{align*}
p(\vr_{a},\vr_{b}|x) & =\frac{1}{Z_{ab}(x)}\nu_{ab}(\vr_{a},\vr_{b})\exp\left[\vh_{a}(x)\cdot\vr_{a}+\vh_{b}(x)\cdot\vr_{b}\right].
\end{align*}

\end_inset

This likelihood implies a posterior 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
p(x|\vr_{a},\vr_{b})\propto\frac{1}{Z_{ab}(x)}\exp\left[\vh_{a}(x)\cdot\vr_{a}+\vh_{b}(x)\cdot\vr_{b}\right].
\]

\end_inset


\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $\vr_{a}$
\end_inset

 and 
\begin_inset Formula $\vr_{b}$
\end_inset

 may not be independent conditioned on 
\begin_inset Formula $s$
\end_inset

.
 For cue combination, it is desirable for a downstream population to combine
 the messages into activity 
\begin_inset Formula $\vr_{c}$
\end_inset

, such that 
\begin_inset Formula $p(x|\vr_{c})=p(x|\vr_{a},\vr_{b})$
\end_inset

.
 Suppose that this population has response kernel 
\begin_inset Formula $\vh_{c}(s)$
\end_inset

.
 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2007"
literal "false"

\end_inset

 shows that this is possible if 
\begin_inset Formula $\vh_{c}(x)\cdot\vr_{c}=\vh_{a}(x)\cdot\vr_{a}+\vh_{b}(x)\cdot\vr_{b}$
\end_inset

.
 One way to ensure this holds is to construct the response kernels of the
 two upstream populations by
\begin_inset Formula 
\begin{equation}
\vh_{d}(x)=\mW_{d}\vh_{c}(x),\quad d\in\left\{ a,b\right\} \label{eq:PPC_shared_basis}
\end{equation}

\end_inset

This allows cue combinations to be performed easily with
\begin_inset Formula 
\begin{equation}
\vr_{c}=\mW_{a}^{\intercal}\vr_{a}+\mW_{b}^{\intercal}\vr_{b}\label{eq:PPC_cue_combination}
\end{equation}

\end_inset

 with implied decoded posterior 
\begin_inset Formula 
\[
p(x|\vr_{c})\propto\frac{1}{Z_{ab}(x)}\exp\left[\vh_{c}(x)\cdot\vr_{c}\right]
\]

\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 for cue combination
\end_layout

\end_inset

 The shared basis in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_shared_basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 allows PPC to implement cue combination with simple addition of the codes.
 However, it also means that 
\begin_inset Formula $\vh_{c}$
\end_inset

 should in general be more flexible than 
\begin_inset Formula $\vh_{a}$
\end_inset

 and 
\begin_inset Formula $\vh_{b}$
\end_inset

.
 This is against usual concept that response properties of downstream population
 is derived from upstream neurons.
 For example, V4 neurons have wider tuning widths than V1 neurons as they
 pool responses from V1.
 For a cascade of cue combination tasks, higher areas should have response
 kernel that is the most diverse, which may be even more counter-intuitive.
\end_layout

\begin_layout Standard
In addition to cue combination, 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckPouget2011Marginalization"
literal "false"

\end_inset

 proposed how marginalisation could be implemented by PPC.
 The authors considered addition of two random variables: 
\begin_inset Formula $x_{3}=x_{1}+x_{2}$
\end_inset

, and restricted the discussions to all variables being Gaussian distributed,
 
\begin_inset Formula $x_{i}\sim\gN(\mu_{i},\Sigma_{i})$
\end_inset

.
 In such specific case, PPCs 
\begin_inset Formula $\vr_{i}$
\end_inset

 encode the natural parameters of these Gaussians, with corresponding response
 kernel 
\begin_inset Formula $\vh_{d}(x)=[\frac{-x^{2}}{2\vv_{d}};x\vm]^{\intercal}$
\end_inset

 where 
\begin_inset Formula $d\in\left\{ a,b,c\right\} $
\end_inset

 and 
\begin_inset Formula $a_{d,i}\in\sR_{+}$
\end_inset

 and 
\begin_inset Formula $b_{d,i}\in\sR$
\end_inset

 are derived from properties of Gaussian-shaped tuning functions.
 Since means and precisions simply add for addition of Gaussian random variables
, we use the following to find an expression for 
\begin_inset Formula $\vr_{c}$
\end_inset

 
\begin_inset Formula 
\begin{equation}
\frac{\vm_{c}\cdot\vr_{c}}{\vv_{c}\cdot\vr_{c}}=\frac{\vm_{a}\cdot\vr_{a}}{\vv_{a}\cdot\vr_{a}}+\frac{\vm_{b}\cdot\vr_{b}}{\vv_{b}\cdot\vr_{b}}\quad\frac{1}{\vv_{c}\cdot\vr_{c}}=\frac{1}{\vv_{a}\cdot\vr_{a}}+\frac{1}{\vv_{b}\cdot\vr_{b}}.\label{eq:PPC_gaussian_addition}
\end{equation}

\end_inset

This is a underdetermined system, as it only constrains projections of 
\begin_inset Formula $\vr_{c}$
\end_inset

 along 
\begin_inset Formula $\vv_{c}$
\end_inset

 and 
\begin_inset Formula $\vm_{c}$
\end_inset

.
 One solution can be obtained by first finding orthonormal basis 
\begin_inset Formula $\vv_{c}^{\dagger}$
\end_inset

 and 
\begin_inset Formula $\vm_{c}^{\dagger}$
\end_inset

 such that 
\begin_inset Formula $\vv_{c}^{\dagger}\cdot\vv_{c}=\vm_{c}^{\dagger}\cdot\vm_{c}=1;\vv_{c}^{\dagger}\cdot\vm_{c}=\vm_{c}^{\dagger}\cdot\vv_{c}=0$
\end_inset

, and then express 
\begin_inset Formula $\vr_{c}$
\end_inset

 using these basis.
 
\begin_inset Formula 
\begin{equation}
\vr_{c}=\frac{\left[\left(\vm_{a}\cdot\vr_{a}\right)\left(\vv_{b}\cdot\vr_{b}\right)+\left(\vm_{b}\cdot\vr_{b}\right)\left(\vv_{a}\cdot\vr_{a}\right)\right]}{\vv_{a}\cdot\vr_{a}+\vv_{b}\cdot\vr_{b}}\vm_{c}^{\dagger}+\frac{\left(\vv_{a}\cdot\vr_{a}\right)\left(\vv_{b}\cdot\vr_{b}\right)}{\vv_{a}\cdot\vr_{a}+\vv_{b}\cdot\vr_{b}}\vv_{c}^{\dagger}\label{eq:PPC_divisive_subspace}
\end{equation}

\end_inset

which can be written in computationally inefficient but biologically plausible
 form by expanding the dot products 
\begin_inset Formula 
\begin{equation}
r_{c,k}=\frac{\sum_{ij}w_{ijk}r_{a,i}r_{b,j}}{\vv_{a}\cdot\vr_{a}+\vv_{b}\cdot\vr_{b}},\quad w_{ijk}=\left(m_{a,i}v_{b,j}+m_{b,i}v_{a,j}\right)m_{c,k}^{\dagger}+v_{a,i}v_{b,j}v_{c,k}^{\dagger}.\label{eq:PPC_divisive}
\end{equation}

\end_inset

The authors noted that this is similar to divisive normalisation that is
 thought to underlie many cortical computation 
\begin_inset CommandInset citation
LatexCommand citet
key "CarandiniHeeger2012Normalization"
literal "false"

\end_inset

.
 Divisive normalisation is then used as a component for computing nonlinear
 interactions of non-Gaussian variables encoded by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

, with weights 
\begin_inset Formula $\vw$
\end_inset

 trained by supervised learning.
 Across tasks, it was found that a network with normalisation nonliearity
 can form a posterior (by linear decoding) very close to the true posterior
 , and was better than networks without normalisation or with ReLU nonlinearity
 instead.
 In addition, to test whether 
\begin_inset Formula $\vr_{c}$
\end_inset

 is a 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 representation, the authors used a criterion that is how close is the posterior
 readout from 
\begin_inset Formula $\vr_{c}$
\end_inset

 to the true posterior, independent of a nuisance features (contrast).
 Indeed, this criteron holds for 
\begin_inset Formula $\vr_{c}$
\end_inset

 computed in many tasks.
 
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Constrained space of 
\begin_inset Formula $\vr_{c}$
\end_inset


\end_layout

\end_inset

 The simpler expression 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive_subspace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 suggests that 
\begin_inset Formula $\vr_{c}$
\end_inset

 only lives on the plane defined by 
\begin_inset Formula $\vm_{c}^{\dagger}$
\end_inset

 and 
\begin_inset Formula $\vv_{c}^{\dagger}$
\end_inset

, which is a rather constrained space for neural activities.
 Note that 
\begin_inset Formula $\vr_{a}$
\end_inset

 and 
\begin_inset Formula $\vr_{b}$
\end_inset

 does not have such constraints.
 For example, if they are Poisson variables, their covariance matrices are
 full rank for typical tuning curves.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Divisive normalisation for optimal performance
\end_layout

\end_inset

 Although the divisive normalisation is derived the summation of Gaussian
 variables, empirically it seems to be important in many nonlinear problems
 with non-Gaussian variables.
 This finding agrees with the literature that uses local gain control to
 improve performance on a wide range of tasks involving images 
\begin_inset CommandInset citation
LatexCommand citep
key "BalleSimoncelli2016Density,LaparraSimoncelli2017Perceptually,BalleSimoncelli2017End"
literal "false"

\end_inset

.
 However, the true posteriors in the experiments are arguably close to a
 Gaussian distribution, and is symmetric and unimodal.
 A stronger test would be a task where the posterior is skewed or multimodal.
 
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Contrast invariance
\end_layout

\end_inset

 Contrast is the only instance of nuisance variable in all experiments,
 in which case it only scales the tuning curves and thus does not affect
 
\begin_inset Formula $p(x|\vr)$
\end_inset

.
 Other nuisance variables, such as spatial frequency and phase, may affect
 tuning curves in other ways.
 Therefore, using contrast invariance as the criterion for a 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 representation is sufficient only in a rather limited context.
 
\end_layout

\begin_layout Standard
The principled solutions for cue combination 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_cue_combination"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and linear addition 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are exact and easy to implement by two different types of neural circuits.
 The assumptions of shared basis 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_shared_basis"
plural "false"
caps "false"
noprefix "false"

\end_inset

, Gaussian addition 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_gaussian_addition"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and the subspace hypothesis 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive_subspace"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are quite strong, as the uncertain beliefs involved in real-world and biologica
lly relevant situations may be more complicated.
 Although divisive normalisation inspired by 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:PPC_divisive"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is empirically demonstrated to be accurate for some tasks, implementation
 for other problems using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 is analytically challenging to derive from first principles.
 As shown in later work, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 implementations for visual search 
\begin_inset CommandInset citation
LatexCommand citep
key "MaPouget2011Behavior"
literal "false"

\end_inset

, visual categorisation 
\begin_inset CommandInset citation
LatexCommand citep
key "QamarMa2013Trial"
literal "false"

\end_inset

 and causal inference 
\begin_inset CommandInset citation
LatexCommand citep
key "MaRahmati2013Towards"
literal "false"

\end_inset

 all require specific circuitries, and have computations that are intractable
 and/or implausible.
 An overview of these circuitries can be found at 
\begin_inset CommandInset citation
LatexCommand citep
after "Eqs. 5-17"
key "OrhanMa2017Efficient"
literal "false"

\end_inset

.
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
PPC for generic computation
\end_layout

\end_inset

 Since PPC represents probability distributions by their natural parameters,
 it is in general challenging to derive exact or approximate algorithms
 using PPC, and the implementation may not be plausible.
 However, in specific settings, it may be possible to arrive at tractable
 and plausible form of solutions.
 
\begin_inset CommandInset label
LatexCommand label
name "rem:PPC_intractable"

\end_inset


\end_layout

\begin_layout Standard
For completeness, I briefly go through the three tasks and illustrate Remark
 
\begin_inset CommandInset ref
LatexCommand formatted
reference "rem:PPC_intractable"
plural "false"
caps "false"
noprefix "false"

\end_inset

 in each of the cases.
 In visual search 
\begin_inset CommandInset citation
LatexCommand citep
key "MaPouget2011Behavior"
literal "false"

\end_inset

, for each trial, a target appears (
\begin_inset Formula $T=1$
\end_inset

) with probability 0.5 among distractors.
 The target is a single oriented bar with orientation 
\begin_inset Formula $s_{T}\in[0,\pi)$
\end_inset

 appears at random locations of the screen.
 There are also 
\begin_inset Formula $N$
\end_inset

 distractors bars with orientation distributed according to 
\begin_inset Formula $p_{d}(s)$
\end_inset

 and location uniformly distributed on the screen.
 Suppose that each stimulus 
\begin_inset Formula $s$
\end_inset

 generates a neural pattern 
\begin_inset Formula $\vr_{n}$
\end_inset

 with tuning kernel 
\begin_inset Formula $\vh_{n}$
\end_inset

.
 Since this is a decision making task, the desired probability is the Bernoulli
 distribution of whether the target exists or not given all the firing patterns
 
\begin_inset Formula $\left\{ \vr_{n}\right\} _{n=1}^{N}$
\end_inset

.
 The natural parameters, and hence PPC, for this distribution is the log
 probability ratio
\begin_inset Formula 
\[
d=\log\frac{p\left(T=1|\left\{ \vr_{n}\right\} _{n=1}^{N}\right)}{p\left(T=0|\left\{ \vr_{n}\right\} _{n=1}^{N}\right)}.
\]

\end_inset


\end_layout

\begin_layout Standard
It can be shown that the log likelihood ratio for the decision is
\begin_inset Formula 
\begin{equation}
d=\log\frac{1}{N}\sum_{n=1}^{N}e^{d_{n}},\quad d_{n}=\vh_{n}(s_{T})\cdot\vr_{n}-\log\int_{0}^{\pi}\exp\left[\vh_{n}(x_{n})\cdot\vr_{n}\right]dp_{d}(x_{n}).\label{eq:PPC_visual_search}
\end{equation}

\end_inset

Intuitively, the decision variable 
\begin_inset Formula $d$
\end_inset

 is the log of the average probability ratio that the target 
\begin_inset Formula $s_{T}$
\end_inset

 is present at the 
\begin_inset Formula $n$
\end_inset

'the location.
 
\end_layout

\begin_layout Remark
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Integral over stimulus distribution
\end_layout

\end_inset

 If 
\begin_inset Formula $p_{d}(x_{n})$
\end_inset

 is a delta function at 
\begin_inset Formula $x_{D}$
\end_inset

, then 
\begin_inset Formula $d_{n}=\left[\vh_{n}(x_{T})-\vh_{n}(x_{D})\right]\cdot\vr_{n}$
\end_inset

, and 
\begin_inset Formula $d$
\end_inset

 can be implemented easily by neural circuits.
 However, for a generic 
\begin_inset Formula $p_{d}(x_{n})$
\end_inset

, it is not known how neural circuits can compute or approximate the integral
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:PPC_visual_search"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Importantly, it depends on 
\begin_inset Formula $p_{d}(x_{n})$
\end_inset

 that is not represented by or learned through 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

.
 Even assuming 
\begin_inset Formula $p_{d}(x)$
\end_inset

 is uniform (as suggested in Remark 
\begin_inset CommandInset ref
LatexCommand formatted
reference "rem:PPC_suboptimal_decoding"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for decoding) does not help with this computation.
\end_layout

\begin_layout Standard
In visual categorisation, 
\end_layout

\begin_layout Standard
In causal inference 
\begin_inset CommandInset citation
LatexCommand citep
key "MaRahmati2013Towards"
literal "false"

\end_inset

, subjects receive two stimuli 
\begin_inset Formula $x_{1},x_{2}\in\sR^{2}$
\end_inset

 and need to decide whether they have a common cause, a task that resembles
 whether subjects show the ventriloquist effect 
\begin_inset CommandInset citation
LatexCommand citep
key "HowardTempleton1966Human"
literal "false"

\end_inset

.
 In half of the trials, the two stimuli are generated by a common source
 
\begin_inset Formula $x\sim p_{x}(s)$
\end_inset

, producing neural responses 
\begin_inset Formula $\vr_{1},\vr_{2}$
\end_inset

 with likelihood 
\begin_inset Formula 
\[
p(\vr_{1},\vr_{2}|C=1)=\int p_{r}(\vr_{1}|x)p_{r}(\vr_{2}|x)p_{x}(x)ds,
\]

\end_inset

 where 
\begin_inset Formula $C=1$
\end_inset

 indicates common cause, 
\begin_inset Formula $p_{r}$
\end_inset

 is the likelihood of neural response, and 
\begin_inset Formula $p_{x}$
\end_inset

 is the stimulus distribution.
 In the other half of the trials, the two stimuli are generated by two independe
nt causes 
\begin_inset Formula $x_{1},x_{2}\stackrel{iid}{\sim}p_{x}(x)$
\end_inset

 with likelihood 
\begin_inset Formula 
\[
p(\vr_{1},\vr_{2}|C=0)=\left(\int p_{r}(\vr_{1}|x_{1})p_{x}(x_{1})dx_{1}\right)\left(\int p_{r}(\vr_{2}|x_{2})p_{x}(x_{2})dx_{2}\right).
\]

\end_inset

The log odds that is relevant for decision is then 
\begin_inset Formula 
\[
d=\log\frac{p(\vr_{1},\vr_{2}|C=1)}{p(\vr_{1},\vr_{2}|C=0)}
\]

\end_inset

Under the assumptions that the 
\begin_inset Formula $p_{r}(\vr_{1}|x)$
\end_inset

 is Poisson with Gaussian-shaped tuning curves, and that the sum of firing
 rate is a constant for any 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $d$
\end_inset

 can be written in closed form, but the length of the expression is omitted
 due to its length 
\begin_inset CommandInset citation
LatexCommand citep
after "eqn. 12"
key "MaRahmati2013Towards"
literal "false"

\end_inset

.
 It involves a logarithm term that is further approximated by a constant
 that is the trial average.
 The final expression has 
\begin_inset CommandInset citation
LatexCommand citep
after "eqn. 14"
key "MaRahmati2013Towards"
literal "false"

\end_inset

 linear projections of 
\begin_inset Formula $\vr_{1}$
\end_inset

 and 
\begin_inset Formula $\vr_{2}$
\end_inset

, their polynomials up to order 3 and divisions.
 
\end_layout

\begin_layout Standard
Finally, almost all the literature 
\end_layout

\begin_layout Subsubsection
Neural filtering algorithms
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "WilsonFinkel2009"
literal "true"

\end_inset

 modified the line attractor network for head direction cells initially
 proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "Zhang1996"
literal "true"

\end_inset

 to implement the Kalman filter; in the case of constant, noiseless innovation
 (fixed drifting speed) and small observation noise, this network is shown
 to approximate the Kalman filtering equations for the mean and variance
 for 1-dimensional linear Gaussian system; for larger noises, the experiments
 show that it is able to adapt to change points much quicker than the Kalman
 filter.
 The neurons show higher firing rate when the underlying latent variable
 has higher precision, consistent with recordings
\begin_inset CommandInset citation
LatexCommand citet
key "SclarFreeman1982"
literal "true"

\end_inset

.
 In order to implement the dynamics, the speed in the latent variable enters
 through a parameter in the weight matrices connecting all neurons, which
 is not very biologically plausible, although some justifications were provided.
 
\end_layout

\begin_layout Standard
The neural particle filter (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

)
\begin_inset CommandInset citation
LatexCommand citep
key "KutschireiterEtAl2017"
literal "true"

\end_inset

 is a sample-based filtering algorithm for continuous time 
\begin_inset ERT
status open

\begin_layout Plain Layout

gls{SSM}
\end_layout

\end_inset

:
\begin_inset Formula 
\begin{align*}
d\vz_{t} & =\vf(\vz_{t})+\mSigma_{z}^{1/2}d\vomega_{t}^{(z)}\\
d\vx_{t} & =\vg(\vz_{t})+\mSigma_{x}^{1/2}d\vomega_{t}^{(x)}
\end{align*}

\end_inset

where 
\begin_inset Formula $\vomega_{t}^{(\cdot)}$
\end_inset

 is the standard Brownian motion.
 The filtered posterior is represented by a set of unweighted samples 
\begin_inset Formula $\left\{ \vz_{t}^{(s)}\right\} _{s}$
\end_inset

, and the evolution of each particle is motivated by the Kushner equation
 given below, denoting 
\begin_inset Formula $\E{p(\vz_{t}|\vx_{1:t})}{\cdot}$
\end_inset

 by 
\begin_inset Formula $\langle\cdot\rangle$
\end_inset

 and let 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 be an arbitrary function on 
\begin_inset Formula $\vz$
\end_inset

 :
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
d\langle\phi\rangle=\langle\vf\rangle dt+\text{cov}(\phi,\vg)\mSigma_{x}^{-1}\left(d\vx_{t}-\left\langle \vg\right\rangle dt\right)\label{eq:NPF_kushner}
\end{equation}

\end_inset

.
 When 
\begin_inset Formula $\vg$
\end_inset

 is linear and 
\begin_inset Formula $\phi(\vz)=\vz$
\end_inset

, it recovers the SDE governing the posterior mean in continuous-time Kalman
 filtering.
 Although this equation is exact, we still do not know the higher moments
 
\begin_inset Formula $\text{cov}(\phi,\vg)$
\end_inset

 and thus direct implementation is impossible (closure problem).
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 proposes to remedy this issue with two design choices.
 First, each particle 
\begin_inset Formula $\vz_{t}^{(s)}$
\end_inset

 evolves according to 
\begin_inset Formula 
\begin{equation}
d\vz_{t}^{(s)}=\vf(\vz_{t}^{(s)})dt+\mW_{t}\left(d\vx_{t}-\left\langle \vg\right\rangle dt\right)+\mSigma_{x}^{1/2}d\vomega_{t}^{(z)}\label{eq:NPF_sample_evolution}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mW_{t}=\hat{\text{cov}}(\vz,\vg)\mSigma_{x}^{-1}$
\end_inset

 with 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 begin the empirically estimated using all samples.
 We can see 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 is similar to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_kushner"

\end_inset

 when 
\begin_inset Formula $\phi(\vz)=\vz$
\end_inset

 except for the last noise term.
 Therefore, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 tries to avoid the closure problem by employing a population of samples
 that follow the trajectory for the mean, replacing 
\begin_inset Formula $\text{cov}(\vz,\vg)$
\end_inset

 with the sample estimate.
 
\end_layout

\begin_layout Standard
By avoiding the need for particle weights as in common particle filters,
 it is made biologically plausible and a neural circuits implementation
 was proposed.
 By training the network weights using maximum likelihood, the model was
 able to learn the parameters of the observations density.
 The latent dynamics was not considered as joint samples of adjacent time
 steps were not considered.
 The experiments show that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{NPF}
\end_layout

\end_inset

 avoided the 
\begin_inset Quotes eld
\end_inset

curse of dimensionality
\begin_inset Quotes erd
\end_inset

, requires much fewer particles than the conventional bootstrap particle
 filter for higher-dimensional problems.
\end_layout

\begin_layout Standard
It remains a question as to why the 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 worked at all, as it is not obvious why (approximately) following the trajector
y of the mean allows the particles to be distributed correctly.
 Intuitively, the first term on the RHS of 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NPF_sample_evolution"

\end_inset

 is the dynamics contribution, the second term is the correction from observatio
n.
 
\begin_inset Formula $\mW_{t}$
\end_inset

 would be the correct gain if 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 were exact, and the amount of correction is determined by the posterior
 variance.
 If the third noise term is absent and the particles are initially correctly
 distributed, then all the particles would move closer to the mean after
 infinitesimal amount of filtering time, and 
\begin_inset Formula $\hat{\text{cov}}(\vz,\vg)$
\end_inset

 is then an underestimate of the true covariance, resulting in a smaller
 correction for future time.
 Since there is a noise term, 
\begin_inset Formula $\left\{ \vz_{t}^{(s)}\right\} _{s}$
\end_inset

 are pushed away from the mean which increases 
\begin_inset Formula $\text{cov}(\phi,\vg)$
\end_inset

 and hence the correction.
 The correct amount of 
\begin_inset Quotes eld
\end_inset

push
\begin_inset Quotes erd
\end_inset

 is the uncertainty induced by data and likely correlated, reflecting explaining
-away, instead of the independent prior noise.
 Thus, it may be the case that the full distribution is different from the
 true posterior, but the posterior mean estimated from these particles are
 close to the correct value.
 The authors did not show full distribution of the posteriors.
\end_layout

\begin_layout Paragraph
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 implementation of the Kalman filter 
\end_layout

\begin_layout Paragraph
Recurrent exponential family harmonium (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
Hard-wiring the neurons to implement/approximate optimal statistical computation
s, as done in some 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature, 
\begin_inset CommandInset citation
LatexCommand citep
key "BeckEtAl2007,BeckEtAl2011"
literal "true"

\end_inset

, is not biologically plausible, as the brain does not receive such fine-grained
 supervisions to construct the wiring.
 
\end_layout

\begin_layout Standard
Viewing inference and learning as a density estimation operation, 
\begin_inset CommandInset citation
LatexCommand citet
key "MakinEtAl2013"
literal "true"

\end_inset

 proposed to use the exponential family harmonium 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, a restricted Boltzmann machine-like fully-connected graphical model, to
 implement a 
\emph on
model
\emph default
 joint distribution of observation 
\begin_inset Formula $\vX$
\end_inset

 and latent code 
\begin_inset Formula $\vR$
\end_inset

, 
\begin_inset Formula $q(\vx,\vr)$
\end_inset

.
 
\begin_inset Formula 
\begin{align*}
q(\vr|\vx) & =\prod_{i=1}^{\Dim R}\Bern\left[r_{i}|\sigm\left(\mM_{RX}\vx+\vtheta_{R}\right)\right]\\
q(\vx|\vr) & =\prod_{i=1}^{\Dim X}\Pois\left[x_{i}|\exp\left(\mM_{RX}^{\intercal}\vr+\vtheta_{X}\right)\right]
\end{align*}

\end_inset

The choices of 
\begin_inset Formula $\sigm$
\end_inset

 and 
\begin_inset Formula $\exp$
\end_inset

 are the canonical link functions for Bernoulli and Poisson, respectively,
 that allow modulation of the natural parameters through linear transformation
 of the conditioning variable.
 The latent code is a sample drawn from 
\begin_inset Formula $\vR|\vx$
\end_inset

, or just its mean rate.
 In fact, the observation 
\begin_inset Formula $\vX$
\end_inset

 is spike count generated by Poisson neurons with Gaussian tuning functions
 over some unobserved variable 
\begin_inset Formula $\vZ$
\end_inset

, yielding a 
\emph on
generative
\emph default
 joint
\emph on
 
\emph default
distribution 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

.
 Learning entails maximizing the spike count log likelihood 
\begin_inset Formula $\E{p(\vx)}{\log q(\vx)}$
\end_inset

 using the standard contrastive divergence for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, similar to one more commonly known for training restricted Boltzmann machines.
 
\end_layout

\begin_layout Standard
Now it is clear that 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 is a density estimation approach to model 
\begin_inset Formula $p(\vx)$
\end_inset

.
 Different from other density estimation approach to model natural stimuli
 
\begin_inset CommandInset citation
LatexCommand citep
key "Lewicki2002,LewickiOlshausen1999,LewickiSejnowski2000"
literal "true"

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 is a fully connected graph rather than a directed graph, and inference
 produces conditionally independent 
\emph on
code
\emph default
 
\begin_inset Formula $\vr$
\end_inset

 rather than the latent variables 
\begin_inset Formula $\vr$
\end_inset

.
 However, the latent code 
\begin_inset Formula $\vr$
\end_inset

 is some abstract feature to describe 
\begin_inset Formula $\vx$
\end_inset

 and is not associated directly with the latent variables 
\begin_inset Formula $\vZ$
\end_inset

 in the generative model, and thus the uncertainty in the posterior cannot
 be interrogated straightforwardly.
 Indeed, to evaluate the performance of this model by decoding, 
\begin_inset Formula $\vr$
\end_inset

 is first used to evaluate the conditional expected spike count 
\begin_inset Formula $\E{q(\vx|\vr)}{\vx}$
\end_inset

 under the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 model, assuming there is no loss of information; then, the mean and covariance
 of 
\begin_inset Formula $\vz$
\end_inset

 are approximately decoded according to the generative joint 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

, pretending
\begin_inset Formula $\E{q(\vx|\vr)}{\vx}$
\end_inset

 is evaluated under 
\begin_inset Formula $p(\vz,\vx)$
\end_inset

.
 Using this approach, one can in principle decoded posterior distribution
 that are more complicated than a Gaussian using, for example, histograms.
\end_layout

\begin_layout Standard
Later on, the same authors 
\begin_inset CommandInset citation
LatexCommand citet
key "MakinEtAl2015"
literal "true"

\end_inset

 applied the same principle to build a recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 for filtering.
 In this model, the latent code from the previous time step 
\begin_inset Formula $\vR_{t-1}$
\end_inset

 and the spike observation 
\begin_inset Formula $\vX_{t}$
\end_inset

 are concatenated, which is used to form an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 with a latent code distribution 
\begin_inset Formula $\vR_{t}$
\end_inset

.
 The latent code for next time step is then the conditional sample 
\begin_inset Formula $\vr_{t}\sim\vR_{t}|\vr_{t-1},\vx_{t}$
\end_inset

, and the same construction of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 continues, giving the recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

.
 The learning algorithm is the 1-step contrastive divergence performed at
 each time step, which maximises the log likelihood of 
\begin_inset Formula $\left[\vr_{t-1},\vx_{t}\right]$
\end_inset

, treating the previous latent code 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 as an 
\emph on
observation
\emph default
.
 
\end_layout

\begin_layout Standard
Although the learning rule is local and thus biologically plausible, the
 underlying cost function is unclear; as the authors also pointed out, the
 justification for treating 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 as an observation is missing.
 By copying 
\begin_inset Formula $\vr_{t-1}$
\end_inset

 to the next time step, no latent dynamics is explicitly modelled and needs
 to be learned by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 at each time step.
 As the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 also needs to incorporate new evidence, it is conceivable that the class
 of transition dynamics and emission may be restrictive.
 Indeed, the experiments conducted all had linear Gaussian latent dynamics.
\end_layout

\begin_layout Paragraph
PPC Bayes rule and EFH (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

)
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset

 proposed a neural filtering algorithm that combines 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citet
key "WellingEtAl2005"
literal "true"

\end_inset

.
 By extensive use of properties of the exponential family distributions,
 the theory suggests that evidence from observation can be more or less
 exactly incorporated into the belief of the latent, that is, Bayes rule
 can be implemented exactly.
 Here, I refer to this approach as the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule with 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

(
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

).
 Forming the prior for the next time step from the previous belief requires
 knowledge about the transition dynamics, which is not explicitly modeled;
 nonetheless, the mapping of natural parameter of the posterior 
\begin_inset Formula $p(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

 to that of the predictive distribution 
\begin_inset Formula $p(\vz_{t}|\vx_{1:t-1})$
\end_inset

 was modeled as a neural network function.
 Although the neural network needs to be trained by the non-plausible back-propa
gation, and only Gaussian posterior is allowed in practice, the theory itself
 is interesting and is thoroughly reviewed here.
 
\end_layout

\begin_layout Standard
This framework has two parts.
 In the first part, a relationship between sensory input 
\begin_inset Formula $\rvfont Z$
\end_inset

 and Poisson spike count 
\begin_inset Formula $\rvfont X$
\end_inset

 is constructed so that the prior and posterior of 
\begin_inset Formula $\rvfont Z$
\end_inset

 live in the same exponential family; the second part describes how the
 prior and posterior can be encoded by the two different neural populations,
 using natural parameters
\begin_inset Formula $\vtheta_{t}$
\end_inset

, and the belief update on these natural parameters.
 In short, the purpose for conjugacy is to maintain consistency of neural
 representation as PPC for the latent variable.
\end_layout

\begin_layout Subparagraph
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\end_layout

\begin_layout Standard
In first part, a conjugacy between prior and posterior 
\begin_inset Formula $z$
\end_inset

 (consider a scalar latent for simplicity) with Poisson likelihood 
\begin_inset Formula $p(\vx|z)$
\end_inset

 is established.
 Let the prior of sensory input 
\begin_inset Formula $z$
\end_inset

 follow some generic exponential family distribution with sufficient statistics
 
\begin_inset Formula $\vpsi(z)$
\end_inset

 
\begin_inset Formula 
\begin{equation}
p(z)=\frac{1}{Z_{Z}(\vtheta_{Z})}\nu_{Z}(z)\exp\left[\vtheta_{Z}\cdot\vpsi(z)\right]\label{eq:NBR_prior}
\end{equation}

\end_inset

denote the set of exponential family distributions with sufficient statistics
 
\begin_inset Formula $\vpsi$
\end_inset

 to be 
\begin_inset Formula $\sM_{\psi}$
\end_inset

.
 These inputs generate neural responses 
\begin_inset Formula $N$
\end_inset

 that is assumed to be independent Poisson conditioned on a particular 
\begin_inset Formula $Z=z$
\end_inset

 with Gaussian-shaped tuning curves 
\begin_inset Formula 
\[
f_{i}(z)=\gamma\exp\left(-\frac{\left(z-z_{i}^{0}\right)^{2}}{2\sigma^{2}}\right),\ i\in\N_{\Dim X},
\]

\end_inset

where homogeneous gain 
\begin_inset Formula $\gamma$
\end_inset

 and tuning width 
\begin_inset Formula $\sigma$
\end_inset

.
 The likelihood of a given firing pattern 
\begin_inset Formula $\vx$
\end_inset

 is
\begin_inset Formula 
\begin{align}
p(\vx|z) & =\prod_{i=1}^{\Dim X}\exp\left[-f_{i}(z)\right]\frac{f_{i}(z)^{x_{i}}}{x_{i}!}\nonumber \\
 & =\frac{1}{\exp\left(\sum_{i=1}^{\Dim X}f_{i}(z)\right)\prod_{i=1}^{\Dim X}(x_{i}!)}\exp\left[\sum_{i=1}^{\Dim X}\log f_{i}(z)x_{i}\right]\nonumber \\
 & =\frac{1}{\exp\left(\Phi_{X}(z)\right)}\nu_{X}(\vx)\exp\left[\vpsi(z)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right]\label{eq:NBR_poisson}
\end{align}

\end_inset

where 
\begin_inset Formula $\vpsi(z)$
\end_inset

, 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 are related to the tuning functions 
\begin_inset Formula $f_{i}$
\end_inset


\begin_inset Formula 
\begin{align*}
\nu_{X}(\vx) & =1/\prod_{i=1}^{\Dim X}x_{i}!\\
\vpsi(z) & =\left[z;z^{2}\right]\\
\mM_{ZX} & =\left[\frac{z^{0\intercal}}{\sigma^{2}};-\frac{\mathbf{\mathbf{1}}^{\intercal}}{2\sigma^{2}}\right]\\
\vtheta_{X} & =\left[\log\gamma-\frac{\left(z^{0}\right)^{2}}{2\sigma^{2}}\right],
\end{align*}

\end_inset

and 
\begin_inset Formula $\Phi_{X}(z)$
\end_inset

 is the log normaliser of 
\begin_inset Formula $p(\vx|z)$
\end_inset


\begin_inset Formula 
\begin{equation}
\Phi_{X}(z)=\sum_{i=1}^{\Dim X}f_{i}(z)=\log\sum_{\vx}\nu_{X}(\vx)\exp\left(\vpsi(z)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right)\label{eq:NBR_poisson_logZ}
\end{equation}

\end_inset

where 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 denotes the the weights for 
\emph on
interaction
\emph default
 of 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

 .
 Together with 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_prior"

\end_inset

, 
\begin_inset Formula $p(\vx|z)$
\end_inset

 defines a joint distribution between 
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $X$
\end_inset

.
 
\end_layout

\begin_layout Standard
Now we would like the posterior 
\begin_inset Formula $p(z|\vx)$
\end_inset

 to be conjugate, meaning that it is also in 
\begin_inset Formula $\sM_{\psi}$
\end_inset

.
 The author proposes a form of density function of
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $\vN$
\end_inset

 that is an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 permitting Poisson 
\begin_inset Formula $p(\vx|z)$
\end_inset

 
\begin_inset Formula 
\[
kp_{0}(z,\vx)\propto\nu_{Z}(z)\nu_{X}(\vx)\exp\left[\vtheta_{Z}\cdot\vpsi(z)+\vpsi(z)\cdot\mM_{ZX}\cdot\vx+\vtheta_{X}\cdot\vx\right]
\]

\end_inset

which captures the relationship between the observation and neural spiking.
 Using 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_poisson_logZ"

\end_inset

, we can sum out 
\begin_inset Formula $\vx$
\end_inset

 and obtain (unnormalised) 
\begin_inset Formula $p_{0}(z)$
\end_inset

,
\begin_inset Formula 
\begin{align}
p_{0}(z) & \propto\nu_{Z}(z)\exp\left[\vtheta_{Z}\cdot\vpsi(z)+\Phi_{X}(z)\right]\label{eq:NBR_true_prior}\\
p_{0}(\vx) & \propto\nu_{X}(\vx)\exp\left[\vtheta_{X}\cdot\vx+\log Z_{Z}(\vtheta_{Z}+\mM_{ZX}\cdot\vx)\right].\label{eq:NBR_true_lik}
\end{align}

\end_inset

The implied posterior distribution is
\begin_inset Formula 
\begin{equation}
p_{0}(z|\vx)\propto\nu_{Z}(z)\exp\left(\vpsi(z)\cdot\left[\mM_{ZX}\cdot\vx+\vtheta_{Z}\right]\right)\label{eq:NBR_true_posterior}
\end{equation}

\end_inset

and 
\begin_inset Formula $p_{0}(z|\vx)\in\sM$
\end_inset

.
 Thus, 
\begin_inset Formula $p_{0}(z)$
\end_inset

 is almost in 
\begin_inset Formula $\sM_{\psi}$
\end_inset

 but off by 
\begin_inset Formula $\Phi_{X}(z)=\sum_{i=1}^{\Dim X}f_{i}(z)$
\end_inset

 in the log density.
 If one assumes that this sum, which is the total firing rate for any given
 
\begin_inset Formula $z$
\end_inset

, is a constant, 
\begin_inset Formula 
\begin{equation}
\sum_{i=1}^{\Dim X}f_{i}(z)=\lambda\label{eq:NBR_const_total_rate}
\end{equation}

\end_inset

then 
\begin_inset Formula $p_{0}\left(z,\vx\right)=p\left(z,\vx\right)$
\end_inset

, and we obtain a model that has the following properties:
\end_layout

\begin_layout Enumerate
the prior 
\begin_inset Formula $p(z)\in\sM_{\psi}$
\end_inset


\end_layout

\begin_layout Enumerate
the likelihood 
\begin_inset Formula $p(\vx|z)$
\end_inset

 is independent Poisson with mean rate depending on Gaussian tuning functions
 
\begin_inset Formula $f_{i}$
\end_inset


\end_layout

\begin_layout Enumerate
the posterior 
\begin_inset Formula $p(z|\vx)\in\sM_{\psi}$
\end_inset


\end_layout

\begin_layout Standard
Given these properties, the next question is how to encode these distributions
 into neural populations.
 Following PPC, and assuming that 
\begin_inset Formula $\sum_{i=1}^{\Dim X}f_{i}(z)$
\end_inset

 is a constant, two separate populations with rates 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 encode the prior 
\begin_inset Formula $p_{0}(z)$
\end_inset

 and posterior 
\begin_inset Formula $p_{0}(z|\vx)$
\end_inset

, respectively, about 
\begin_inset Formula $z$
\end_inset

.
 
\begin_inset Formula 
\begin{align}
q(z) & \propto\exp\left[\vpsi(z)\cdot\mW_{Z}\cdot\vr_{Z}\right]\label{eq:NBR_ppc_prior}\\
q(z|\vx) & \propto\exp\left[\vpsi(z)\cdot\mW_{Z|X}\cdot\vr_{Z|X}\right]\label{eq:NBR_ppc_posterior}
\end{align}

\end_inset

This encoding allows more neurons than parameters to encode a distribution,
 and thus allows noise in the representation.
 For exact Bayes rule, it is desirable for 
\begin_inset Formula $q(z|\vr_{X})=p(z)$
\end_inset

, 
\begin_inset Formula $q(z|\vr_{Z|X})=p(z|\vx)$
\end_inset

 and 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 to be a function of 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

.
 By matching the natural parameters between 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_prior"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior"

\end_inset

 (taking into account 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

), 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_posterior"

\end_inset

and 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_posterior"

\end_inset

, we have
\begin_inset Formula 
\begin{align}
\mW_{Z}\cdot\vr_{Z} & =\vtheta_{Z}\label{eq:NBR_ppc_prior_np}\\
\mW_{Z|X}\cdot\vr_{Z|X} & =\vtheta_{Z}+\mM_{ZX}\cdot\vx
\end{align}

\end_inset

which gives 
\begin_inset Formula 
\[
\mW_{Z|X}\cdot\vr_{Z|X}=\mW_{Z}\cdot\vr_{Z}+\mM_{ZX}\cdot\vx.
\]

\end_inset

To express 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 in terms of 
\begin_inset Formula $\vr_{X}$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

, the author further assumes (akin to 
\begin_inset CommandInset citation
LatexCommand citep
key "BeckEtAl2007"
literal "true"

\end_inset

) that 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\mW_{Z}$
\end_inset

 are related to 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 through linear recoding
\begin_inset Formula 
\begin{align}
\mM_{ZX} & =\mW_{Z|X}\mfont V_{X}\nonumber \\
\mW_{Z} & =\mW_{Z|X}\mfont V{}_{Z}\label{eq:NBR_W_map}
\end{align}

\end_inset

Finally, the encoding for the posterior 
\begin_inset Formula $\vr_{Z|X}$
\end_inset

 can be expressed in terms of prior encoding 
\begin_inset Formula $\vr_{X}$
\end_inset

 and the number of spike
\begin_inset Formula $\vx$
\end_inset


\begin_inset Formula 
\[
\vr_{Z|X}=\mfont V_{X}\vx+\mfont V{}_{Z}\vr_{Z}
\]

\end_inset

which is the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\begin_inset Foot
status open

\begin_layout Plain Layout
referred to as neural Bayes rule by 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset


\end_layout

\end_inset

.
 If the same encoding is used for prior and posterior, meaning 
\begin_inset Formula $\mW_{Z}=\mW_{Z|X}$
\end_inset

, then 
\begin_inset Formula $\mfont V{}_{Z}$
\end_inset

 is the identity.
 Compared to 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 framework proposed by 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2007"
literal "true"

\end_inset

, the present formulation differs in that
\end_layout

\begin_layout Enumerate
the prior 
\begin_inset Formula $p(z)$
\end_inset

 is explicitly represented by a neural population
\end_layout

\begin_layout Enumerate
\begin_inset Formula $z$
\end_inset

 and 
\begin_inset Formula $\vx$
\end_inset

 are connected by a joint distribution in the form of an 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We note here that the observations 
\begin_inset Formula $\vx$
\end_inset

 is implied by the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 joint and hence lives in the exponential family, which can be restrictive.
 Also, the assumption for constant firing rate 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

 may not hold for tuning curves that are asymmetric (e.g.
 sigmoidal), and holds as a result of using symmetric tuning functions and
 Poisson neurons; indeed, as with previous work on 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

, Gaussian tuning curves 
\begin_inset Formula $f_{i}$
\end_inset

 are used in all experiments, which means that the prior and posteriors
 are all Gaussian, and 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 in 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_poisson"

\end_inset

 are determined by parameters in 
\begin_inset Formula $f_{i}$
\end_inset

.
 While it may be the case that other choices of tuning curve allows more
 flexible prior and posterior distributions, this is largely unexplored
 in the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 literature.
 
\end_layout

\begin_layout Subparagraph
Forward inference and learning using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule
\end_layout

\begin_layout Standard
In filtering, we would like to recursively infer the distribution of latent
 state 
\begin_inset Formula $\vz_{t}$
\end_inset

 given all Poisson observation 
\begin_inset Formula $\vx_{1:t}$
\end_inset

, whose belief is represented by rate 
\begin_inset Formula $\vr_{t|t}:=\vr_{Z_{t}|X_{1:t}}$
\end_inset

.
 The unknown latent dynamics should also be updated to incoming statistics.
\end_layout

\begin_layout Standard
For inference, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule facilitates evidence incorporation that updates 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

 to 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t})$
\end_inset

 at each time 
\begin_inset Formula $t$
\end_inset

, but the step from the previous posterior 
\begin_inset Formula $q(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

 to the predictive distribution 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

 is missing; that is, in terms of PPC, how to map from 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 to 
\begin_inset Formula $\vr_{t|t-1}$
\end_inset

.
 If the true latent process is Markovian, then there exists a mapping from
 
\begin_inset Formula $q(\vz_{t-1}|\vx_{1:t-1})$
\end_inset

 to 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

, which is a multiplication by the true transition dynamics 
\begin_inset Formula $p(\vz_{t}|\vz_{t-1})$
\end_inset

 followed by marginalisation of 
\begin_inset Formula $\vz_{t-1}$
\end_inset

.
 It may be possible to realise these operations in the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 code, so 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset

 proposes to train a generic neural network 
\begin_inset Formula $\phi(\cdot;\mW_{\phi})$
\end_inset

, parameterised by 
\begin_inset Formula $\mW_{\phi}$
\end_inset

, to perform approximate this mapping.
 
\begin_inset Formula $\mW_{\phi}$
\end_inset

 can then be seen as a generative parameter that approximates the unknown
 true dynamics, which may be nonlinear and non-Gaussian.
 The predictive distribution still needs to be well approximated by a Gaussian
 as required by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule.
\end_layout

\begin_layout Standard
For learning, first note that some parameters are restricted.
 Given the Gaussian tuning curve restrictions, 
\begin_inset Formula $\mM_{ZX}$
\end_inset

 and 
\begin_inset Formula $\vtheta_{X}$
\end_inset

 are determined by tuning parameters of 
\begin_inset Formula $f_{i}$
\end_inset

.
 The authors set 
\begin_inset Formula $\mW_{Z}=\mW_{Z|X}$
\end_inset

, so
\begin_inset Formula $\mfont V{}_{Z}$
\end_inset

 is the identity matrix.
 So the only free parameters are 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 and 
\begin_inset Formula $\mW_{\phi}$
\end_inset

.
 In the experiment of 
\begin_inset CommandInset citation
LatexCommand citet
key "Sokoloski2017"
literal "true"

\end_inset

, the author fixes 
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 to one of the two choices:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mW_{Z|X}=\mM_{ZX}$
\end_inset

 (naive code), implying projections along the centres of the Gaussian tuning
 functions; 
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\mW_{Z|X}$
\end_inset

 with rows that are orthonormal to each other and also to the 1-vector (orthonor
mal code), as laid out in 
\begin_inset CommandInset citation
LatexCommand citet
key "BeckEtAl2011"
literal "true"

\end_inset


\end_layout

\begin_layout Standard
The objective for online learning is the conditional likelihood 
\begin_inset Formula $q(\vx_{t}|\vx_{1:t-1})$
\end_inset

, which is similar to 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior_np"

\end_inset

 but with prior natural parameter 
\begin_inset Formula $\vtheta_{Z}$
\end_inset

 encoded by 
\begin_inset Formula $\vr_{t|t-1}=\phi(\vr_{t-1|t-1};\mW_{\phi})=:\phi_{t-1}(\mW_{\phi})$
\end_inset

 using 
\begin_inset Formula $\mW_{Z}$
\end_inset

.
 Substituting this into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_ppc_prior_np"

\end_inset

 and then into 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_true_lik"

\end_inset

, and adding the time indices, the likelihood is 
\begin_inset Formula 
\begin{align*}
q(\vx_{t}|\vx_{1:t-1};\mW_{\phi}) & \propto\nu(\vx_{t})\exp\left[\vtheta_{X}\cdot\vx_{t}+Z_{Z}(\mW_{Z}\cdot\vr_{t|t-1}+\mM_{ZX}\cdot\vx_{t})\right]
\end{align*}

\end_inset

Using contrastive divergence-like approach for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

, and the fact that 
\begin_inset Formula $\nabla Z_{Z}(\theta)=\E{p(Z)}{\vpsi(\vz)}$
\end_inset

 when 
\begin_inset Formula $\theta$
\end_inset

 is the natural parameter of 
\begin_inset Formula $p(Z)$
\end_inset

, it can be shown that the gradient for 
\begin_inset Formula $\mW_{\phi}$
\end_inset

 is
\begin_inset Formula 
\begin{equation}
\nabla_{\mW_{\phi}}\log q(\vx_{t}|\vx_{1:t-1};\mW_{\phi})=\left[\E{q(\vz_{t}|x_{1:t})}{\vpsi(\vz_{t})}-\E{q(\vz_{t}|x_{1:t-1})}{\vpsi(\vz_{t})}\right]\cdot\mW_{Z}\cdot\nabla_{\mW_{\phi}}\vr_{t|t-1}\label{eq:NBR_learning}
\end{equation}

\end_inset


\begin_inset Formula $\nabla_{\mW_{\phi}}\vr_{t|t-1}$
\end_inset

 has long-range dependencies as 
\begin_inset Formula $\phi(\cdot)$
\end_inset

 is applied recursively.
 However, if inference is optimal, 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 contains all stimulus information, since the generative process is Markov.
 Thus, the author uses the approximation 
\begin_inset Formula $\nabla_{\mW_{\phi}}\vr_{t|t-1}\approx\nabla_{\mW_{\phi}}\phi(\vr_{t-1|t-1};\mW_{\phi})$
\end_inset

 and treats 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 as a constant, which avoids back-propagation through time.
 The first expectation can be computed exactly as it is Gaussian.
 The two expectations can be evaluated either by 
\end_layout

\begin_layout Enumerate
Gibbs sampling from 
\begin_inset Formula $p_{0}(\vz_{t},\vx_{t}|\vx_{1:t-1})$
\end_inset


\end_layout

\begin_layout Enumerate
direct computation of mean parameters from 
\begin_inset Formula $\vr_{t|t-1}$
\end_inset

 assuming 
\begin_inset Formula $q(\vz_{t}|\vx_{1:t-1})$
\end_inset

 is Gaussian.
\end_layout

\begin_layout Standard
Overall, this framework can approximate unknown latent dynamics in the latent
 space, and the conditioning can be done exactly using 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

 Bayes rule.
 However, realisation of the generic 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{EFH}
\end_layout

\end_inset

 theory is restricted to Gaussian prior and posteriors by the use of Gaussian-sh
aped tuning curves, which restricts the range of possible transition dynamics
 that can be well approximated.
 Interestingly, it is reported that when treating 
\begin_inset Formula $\vr_{t-1|t-1}$
\end_inset

 as a constant in order to avoid backpropagation through time, the model
 cannot discover second-order latent variables, such as velocity when observing
 the position of an oscillatory signal.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 is related to the recurrent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "MakinEtAl2015"
literal "true"

\end_inset

 but are fundamentally different.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 takes a density density estimation approach and treats the latent code
 
\begin_inset Formula $\vr_{t}$
\end_inset

 as features, but not an explicit encoding of the latent variables 
\begin_inset Formula $\vz_{t}$
\end_inset

 as in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 which encodes natural parameter of the posterior, a 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 allows arbitrary transition dynamics to be learned by back propagation;
 whereas 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 does not explicitly learn any transition dynamics, but the overall learning
 uses the more biologically plausible contrastive divergence rule.
 
\end_layout

\begin_layout Enumerate
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 maximizes the incremental log likelihood of 
\begin_inset Formula $\log p(\vx_{t}|\vx_{1:t-1})$
\end_inset

, whereas 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 does not optimize any principled cost function.
 
\end_layout

\begin_layout Standard
The only part common to both approaches is the choice of likelihood via
 Gaussian tuning curves.
 This choice may be important for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{PPC-BR-EFH}
\end_layout

\end_inset

 to admit the approximation 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:NBR_const_total_rate"

\end_inset

, but less so for 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

, as the density estimation formulation is agnostic to the generative model,
 as long as the support of observation distribution in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{rEFH}
\end_layout

\end_inset

 include the data.
\end_layout

\begin_layout Paragraph
General machine learning algorithms
\end_layout

\begin_layout Standard
\noindent
\begin_inset FormulaMacro
\newcommand{\w}{\mathbf{w}}
{\mathbf{w}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\x}{\mathbf{x}}
{\mathbf{x}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\y}{\mathbf{y}}
{\mathbf{y}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\z}{\mathbf{z}}
{\mathbf{z}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "MarinoEtAl2018"
literal "true"

\end_inset

 studied a more general non-Markovian state-space model the joint distribution
 factorises as 
\begin_inset Formula $p(\x_{\le T},\z_{\le T}):=\prod_{t=1}^{T}p\left(\x_{t},\z_{t}|\x_{<t},\z_{<t}\right)=\prod_{t=1}^{T}p\left(\x_{t}|\x_{<t},\z_{\le t}\right)p(\z_{t}|\x_{<t},\z_{<t})$
\end_inset

.
 Though it complicates full inference, the authors realise that the free
 energy, when factorised over time, can be optimized for each time step
 in the forward direction, permitting online filtering and learning.
 In addition, at each time step, filtering involves an inner loop of amortized
 inference at each time step 
\begin_inset Formula $t$
\end_inset

 where gradient updates were performed on the free energy term at time 
\begin_inset Formula $t$
\end_inset

, taking the posterior at 
\begin_inset Formula $t-1$
\end_inset

 as the prior.
 
\end_layout

\begin_layout Standard
The Kalman VAE 
\begin_inset CommandInset citation
LatexCommand citet
key "FraccaroEtAl2017"
literal "true"

\end_inset

 incorporates a standard VAE into a chain of Linear Gaussian state space
 model (
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

) in which the linear matrices evolve over time.
 In the generative model, an intermediate sequence is first generated from
 the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
gls{LGSSM}
\end_layout

\end_inset

, using potentially time-dependent transition matrices.
 These intermediate latents are then fed into a nonlinear neural network
 to obtain observations.
 In order to allow for nonlinear dynamics, the linear transition matrices
 were functions of time and intermediate outputs.
 More specifically, let the intermediate latent at time 
\begin_inset Formula $t$
\end_inset

 be 
\begin_inset Formula $a_{t}$
\end_inset

, the dynamics matrix in the latent 
\begin_inset Formula $A=\sum_{k=1}^{K}\alpha_{t}^{(k)}\left(a_{0:t-1}\right)A^{\left(k\right)}$
\end_inset

, where 
\begin_inset Formula $\alpha_{t}^{(k)}$
\end_inset

 is a series of coefficients generated by recurrent neural network.
 These coefficients are used to linearly interpolate between 
\begin_inset Formula $K$
\end_inset

 linear modes (
\begin_inset Formula $\alpha_{t}^{(k)}\in[0,1]$
\end_inset

,
\begin_inset Formula $\sum_{k=1}^{K}\alpha_{t}^{(k)}=1$
\end_inset

).
 The control and observation matrices are constructed in the same fashion.
 Thus, these dynamics are functions of the samples generated from reparameterize
d Gaussian distributions, enabling learning by backpropagation.
 During inference, the message sent from each observation is a Gaussian
 distribution produced from a standard VAE.
 Filtering and smoothing are then analytically tractable, taking samples
 from the observation-only posterior as the usual observation for the LGSSM.
 Thus, although the latent variables follow linear dynamics, the linear
 matrices change over time and can depend on the observation history, giving
 the generative model the capacity of producing complex dynamics while maintaini
ng inferential tractability, provided that the observations were encoded
 into a Gaussian distribution.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset nomenclature
LatexCommand nomenclature
symbol "LGSSM"
description " "
literal "true"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printglossaries
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "ref"
options "plainnat"

\end_inset


\end_layout

\end_body
\end_document
